{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Framework Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipdb\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    #Activation Functions\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    def d_tanh(self, x):\n",
    "        return 1 - np.square(np.tanh(x))\n",
    "    def sigmoid(self, x):\n",
    "        return expit(x)\n",
    "    def d_sigmoid(self, x):\n",
    "        return (1 - self.sigmoid(x)) * self.sigmoid(x)\n",
    "    def ReLu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    def d_ReLu(self, Z):\n",
    "        return Z > 0\n",
    "    \n",
    "    #For output layer, useful for multiclass classification\n",
    "    def softmax(self, Z):\n",
    "        return np.exp(Z) / sum(np.exp(Z))\n",
    "    def d_softmax(self, Z):\n",
    "        pass\n",
    "    \n",
    "    activationFunctions = {\n",
    "        'tanh': (tanh, d_tanh),\n",
    "        'sigmoid': (sigmoid, d_sigmoid),\n",
    "        'reLu': (ReLu, d_ReLu),\n",
    "        'softmax': (softmax, d_softmax)\n",
    "    }\n",
    "    \n",
    "    #Input -> num of neurons in prev layer, Neurons --> num neurons in cur layer, Activation -> activation fxn to use\n",
    "    def __init__(self, inputs, neurons, activation):\n",
    "        self.neurons = neurons\n",
    "        self.W = np.random.rand(neurons, inputs) - 0.5\n",
    "        self.b = np.random.rand(neurons, 1) - 0.5\n",
    "        self.Z = None\n",
    "        self.A_prev = None\n",
    "        self.act, self.d_act = self.activationFunctions.get(activation)\n",
    "        \n",
    "    def initializeWeights(self, inputs, neurons):\n",
    "        self.W = np.random.rand(neurons, inputs) - 0.5\n",
    "        \n",
    "    def getNeuronCount(self):\n",
    "        return self.neurons\n",
    "    \n",
    "    def getWeights(self):\n",
    "        return self.W\n",
    "    \n",
    "    def getBias(self):\n",
    "        return self.b\n",
    "    \n",
    "    def setWeight(self, weight):\n",
    "        self.W = weight\n",
    "        \n",
    "    def setBias(self, bias):\n",
    "        self.b = bias\n",
    "        \n",
    "    def feedForward(self, A_prev):\n",
    "        #ipdb.set_trace()\n",
    "        self.A_prev = A_prev\n",
    "        self.Z = self.W.dot(self.A_prev) + self.b\n",
    "        self.A = self.act(self, self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    #All derivatives are wrt to cost\n",
    "    #Expects dA of cur layer\n",
    "    #Special case where doing multi class classification with mutli class logloss, you can get the dZ wrt dC directly without having to first get dA\n",
    "    def backprop(self, dA, learning_rate, dZ_Special):\n",
    "        \n",
    "        #ipdb.set_trace()\n",
    "        \n",
    "        #elementt by element matrix multip, not a normal dot prod since both matrices have same shape (essentialyl scalar)\n",
    "        dZ = np.multiply(self.d_act(self, self.Z), dA) if dZ_Special.any() == None else dZ_Special\n",
    "        \n",
    "         # need to normalize weights and divide by number of samples\n",
    "        # because it is actually a sum of weights\n",
    "        dW = 1/dZ.shape[1] * np.dot(dZ, self.A_prev.T)\n",
    "        \n",
    "        # this is to match shape since biases is supposed to be a col vector with 1 col but dZ has m cols\n",
    "        # w/ m being num of samples, we want to take avg of all samples in dZ (i.e on a row by row basis, sum of cols\n",
    "        # and divide by total num of smamples)\n",
    "        db = 1 / dZ.shape[1] * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        \n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "        \n",
    "        self.W = self.W - learning_rate * dW\n",
    "        self.b = self.b - learning_rate * db\n",
    "        return dA_prev\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    #Loss Functions, mse for regression, logloss for classification\n",
    "    def mse(self, a, target):\n",
    "        return np.square(a-target)\n",
    "    \n",
    "    def d_mse(self, a, target):\n",
    "        return 2*(a-target)\n",
    "    \n",
    "    def binary_logloss(self, a, target):\n",
    "        return -(target*np.log(a) + (1-target)*np.log(1-a))\n",
    "    \n",
    "    def d_binary_logloss(self, a, target):\n",
    "        return (a - target)/(a*(1 - a))\n",
    "    \n",
    "    #Source - https://www.kaggle.com/c/predict-closed-questions-on-stack-overflow/discussion/2644\n",
    "    def multi_logloss(self, a, target, eps=1e-15):\n",
    "        predictions = np.clip(a, eps, 1 - eps)\n",
    "\n",
    "        # normalize row sums to 1\n",
    "        predictions /= predictions.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        return -np.sum(target * np.log(predictions))/predictions.shape[0]\n",
    "    \n",
    "    def d_multi_logloss(self, a, target):\n",
    "        return np.zeros(a.shape) # kinda just a placeholder\n",
    "    \n",
    "    lossFunctions = {\n",
    "        'mse': (mse, d_mse),\n",
    "        'binary_logloss': (binary_logloss, d_binary_logloss),\n",
    "        'multi_logloss': (multi_logloss, d_multi_logloss)\n",
    "    }\n",
    "        \n",
    "    #LossFunction is either mse of logloss\n",
    "    def __init__(self, lossFunction):\n",
    "        self.layers = []\n",
    "        self.learning_rate = 0.1\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 10\n",
    "        self.classification = False if lossFunction == 'mse' else True\n",
    "        self.lossFunction = lossFunction\n",
    "        self.loss, self.d_loss = self.lossFunctions.get(lossFunction)\n",
    "    \n",
    "    #Units is 1-n and activationFunction is 'ReLu', 'sigmoid', 'tanh', or 'softmax'\n",
    "    def addLayer(self, units, activationFunction):\n",
    "        prevLayerNeuronCount = self.layers[-1].getNeuronCount() if len(self.layers) > 0 else 0\n",
    "        self.layers.append(Layer(prevLayerNeuronCount, units, activationFunction))\n",
    "        \n",
    "    def getNumBatches(self, num_samples, batch_size):\n",
    "        if (num_samples == batch_size):\n",
    "            return 1\n",
    "        elif (num_samples > batch_size):\n",
    "            if (num_samples % batch_size == 0):\n",
    "                return num_samples // batch_size\n",
    "            else:\n",
    "                return (num_samples // batch_size) + 1\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def oneHot(self, x):\n",
    "        one_hot_X = np.zeros((x.max() + 1, x.size)) #making a matrix of 10 x m\n",
    "        one_hot_X[x, np.arange(x.size)] = 1 #going through all cols and setting the row w/ index corresponding to the y to 1, its very easy to iterate over numpy arays like this apparently\n",
    "        return one_hot_X\n",
    "    \n",
    "    #Convert one hot encoded 2d array to original array of 1d\n",
    "    def rev_one_hot(self, target):\n",
    "        rev_one_hot = np.argmax(target, 0)\n",
    "        return rev_one_hot\n",
    "    #Compare two 1d arrays\n",
    "    def get_accuracy(self, target, Y, accuracy_buffer):\n",
    "        #ipdb.set_trace()\n",
    "        return np.sum(abs(target-Y)<accuracy_buffer) / Y.size\n",
    "    \n",
    "    def get_layer_biases(self):\n",
    "        biases = []\n",
    "        for layer in self.layers:\n",
    "            biases.append(layer.getBias())\n",
    "        return biases\n",
    "    \n",
    "    def get_layer_weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights.append(layer.getWeights())\n",
    "        return weights\n",
    "\n",
    "    # Pass weights array, num of elemtns in weights array has to match up w/ layers and num of neurons in layers\n",
    "    def set_weights(self, weights):\n",
    "        if (len(weights) != len(self.layers)):\n",
    "            raise ValueError(\"Num of layers and num of weihts must match\")\n",
    "        for count, weight in enumerate(weights):\n",
    "                if (weight.shape[0] != self.layers[count].getNeuronCount()):\n",
    "                    raise ValueError(\"Num of rows in weights at index \" + count + \" does not match num of neurons in layer \" + count)\n",
    "        for count, weight in enumerate(weights):\n",
    "            self.layers[count].setWeight(weight)\n",
    "            \n",
    "    def set_biases(self, biases):\n",
    "        if (len(biases) != len(self.layers)):\n",
    "            raise ValueError(\"Num of layers and num of biases must match\")\n",
    "        for count, bias in enumerate(biases):\n",
    "            if (bias.shape[0] != self.layers[count].getNeuronCount()):\n",
    "                raise ValueError(\"Num of rows in biases at index \" + count + \" does not match num of neurons in layer \" + count)\n",
    "        for count, bias in enumerate(biases):\n",
    "            self.layers[count].setBias(bias)\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, epochs = None, batch_size = None, learning_rate = None, accuracy_buffer = 0.1):\n",
    "        self.learning_rate = learning_rate if learning_rate != None else self.learning_rate\n",
    "        self.epochs = epochs if epochs != None else self.epochs\n",
    "        self.batch_size = batch_size if batch_size != None else self.batch_size\n",
    "        \n",
    "        #Need at min one layer\n",
    "        if (len(self.layers) == 0):\n",
    "            raise ValueError('No layers have been added. Need at least one layer. Please add a layer') \n",
    "        \n",
    "        #multi class classificaiton problem need y to be one hot encoded and must use multi log loss\n",
    "        multiClassProblem = self.classification and (y.max() - y.min() > 1)\n",
    "        if (multiClassProblem):\n",
    "            y = self.oneHot(y)\n",
    "            if (self.lossFunction != 'multi_logloss'):\n",
    "                raise ValueError('Loss Function Must be multi_logloss for multi class classification')\n",
    "        \n",
    "        epoch_costs = []\n",
    "        batches_cost_sum = 0\n",
    "        num_batches = self.getNumBatches(X.shape[1], self.batch_size)\n",
    "        \n",
    "        #Initializing weights of the first layer \n",
    "        #Need to do it right now because shape of input isnt known until now\n",
    "        self.layers[0].initializeWeights(X.shape[0], self.layers[0].getNeuronCount())\n",
    "        \n",
    "        ###-----Epoch iterations, training occurs here-----###\n",
    "        for epoch in range(self.epochs):\n",
    "            batches_cost_sum = 0\n",
    "            for batch in range(num_batches):\n",
    "                \n",
    "                ###-----Obtaining appropriate batch data-----###\n",
    "                A = X[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
    "                \n",
    "                if (multiClassProblem): \n",
    "                    y_curBatch = y[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
    "                else:\n",
    "                    y_curBatch = y[batch*self.batch_size:(batch+1)*self.batch_size]\n",
    "            \n",
    "                ###-----Performing forward prop and backprop-----###\n",
    "                #ipdb.set_trace()\n",
    "                for layer in self.layers:\n",
    "                    A = layer.feedForward(A)\n",
    "                batches_cost_sum += 1/self.batch_size * np.sum(self.loss(self, A, y_curBatch))\n",
    "                \n",
    "                #For multi class classiifcaiton problems (class > 2) and using softmax, deriv of softmax w.r.t to Zfinal is just actual - pred\n",
    "                dZ_Special = A - y_curBatch if multiClassProblem else np.array([None])\n",
    "                \n",
    "                #After the final output layer dA is found like this since A is just the output\n",
    "                dA = self.d_loss(self, A, y_curBatch)\n",
    "                \n",
    "                #Only final layer does the special dZ matter and only if multi class\n",
    "                for layer in reversed(self.layers):\n",
    "                    if (layer == self.layers[-1]):\n",
    "                        dA = layer.backprop(dA, self.learning_rate, dZ_Special)\n",
    "                    else:\n",
    "                        dA = layer.backprop(dA, self.learning_rate, np.array([None]))\n",
    "                \n",
    "                ###-----Logging Metrics-----###\n",
    "                if (epoch % 10 == 0 and batch == num_batches-1):\n",
    "                    print(\"-----Epoch: \", epoch, \"-----\")\n",
    "                    if (multiClassProblem):\n",
    "                        A = self.rev_one_hot(A)\n",
    "                        y_curBatch = self.rev_one_hot(y_curBatch)\n",
    "                    print(\"Accuracy:\", self.get_accuracy(A, y_curBatch, accuracy_buffer))\n",
    "                    print(\"Cost:\", batches_cost_sum)\n",
    "            epoch_costs.append(batches_cost_sum) \n",
    "        return epoch_costs, self.get_layer_weights(), self.get_layer_biases()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.feedForward(A)\n",
    "        return A\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are three exampels of the neural net in use. In the first example, the net is trained to learn the XOR function. In the second example, the neural net learns the relationshp between petal length and flower height in the flower problem using a general perceptron. In the final example, the MNIST data set is trainedi in various ways to achieve differing levels of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  0 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.28575244552498663\n",
      "-----Epoch:  10 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2807814346582089\n",
      "-----Epoch:  20 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.27860517211026387\n",
      "-----Epoch:  30 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2775317716957579\n",
      "-----Epoch:  40 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.27690237039374804\n",
      "-----Epoch:  50 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.27645377566283136\n",
      "-----Epoch:  60 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2760781753347831\n",
      "-----Epoch:  70 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.27572966248007835\n",
      "-----Epoch:  80 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.275387191812383\n",
      "-----Epoch:  90 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2750397415046734\n",
      "-----Epoch:  100 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2746803241295198\n",
      "-----Epoch:  110 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2743035465042742\n",
      "-----Epoch:  120 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2739045977201329\n",
      "-----Epoch:  130 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2734788144191219\n",
      "-----Epoch:  140 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.27302148229400475\n",
      "-----Epoch:  150 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2725277379498445\n",
      "-----Epoch:  160 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2719925178718319\n",
      "-----Epoch:  170 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2714105345821701\n",
      "-----Epoch:  180 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2707762736320167\n",
      "-----Epoch:  190 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2700840106818075\n",
      "-----Epoch:  200 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.26932785025619277\n",
      "-----Epoch:  210 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2685017886173676\n",
      "-----Epoch:  220 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.26759980321851917\n",
      "-----Epoch:  230 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2666159705493213\n",
      "-----Epoch:  240 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.26554461287094117\n",
      "-----Epoch:  250 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.26438047233313294\n",
      "-----Epoch:  260 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.26311890829887674\n",
      "-----Epoch:  270 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2617561105143032\n",
      "-----Epoch:  280 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.260289317347815\n",
      "-----Epoch:  290 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2587170251391629\n",
      "-----Epoch:  300 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2570391723305948\n",
      "-----Epoch:  310 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2552572811208124\n",
      "-----Epoch:  320 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2533745404152003\n",
      "-----Epoch:  330 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.25139581711918807\n",
      "-----Epoch:  340 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.249327588232039\n",
      "-----Epoch:  350 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.24717779320597177\n",
      "-----Epoch:  360 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.24495561371695707\n",
      "-----Epoch:  370 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.24267119520610692\n",
      "-----Epoch:  380 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.24033533017149067\n",
      "-----Epoch:  390 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2379591263643409\n",
      "-----Epoch:  400 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.23555368336700833\n",
      "-----Epoch:  410 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2331297986167459\n",
      "-----Epoch:  420 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.23069771935919026\n",
      "-----Epoch:  430 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.22826695113108914\n",
      "-----Epoch:  440 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.22584612713292165\n",
      "-----Epoch:  450 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2234429371048901\n",
      "-----Epoch:  460 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2210641096781144\n",
      "-----Epoch:  470 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.21871543896616072\n",
      "-----Epoch:  480 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2164018444526794\n",
      "-----Epoch:  490 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.21412745287652252\n",
      "-----Epoch:  500 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.21189569154838375\n",
      "-----Epoch:  510 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.20970938403302697\n",
      "-----Epoch:  520 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.20757084108237428\n",
      "-----Epoch:  530 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.20548194182826665\n",
      "-----Epoch:  540 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2034442023138472\n",
      "-----Epoch:  550 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.2014588302904733\n",
      "-----Epoch:  560 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.19952676671907987\n",
      "-----Epoch:  570 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.1976487155282144\n",
      "-----Epoch:  580 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.19582516387907614\n",
      "-----Epoch:  590 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.19405639549493955\n",
      "-----Epoch:  600 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.19234249958531988\n",
      "-----Epoch:  610 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.19068337761385357\n",
      "-----Epoch:  620 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.18907874971351513\n",
      "-----Epoch:  630 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.18752816203232267\n",
      "-----Epoch:  640 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.18603099577382826\n",
      "-----Epoch:  650 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.18458647823694507\n",
      "-----Epoch:  660 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.18319369579432587\n",
      "-----Epoch:  670 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.1818516084916958\n",
      "-----Epoch:  680 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.18055906579916076\n",
      "-----Epoch:  690 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.17931482298471269\n",
      "-----Epoch:  700 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.1781175575889583\n",
      "-----Epoch:  710 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.1769658855361548\n",
      "-----Epoch:  720 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.17585837649956626\n",
      "-----Epoch:  730 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.17479356823249034\n",
      "-----Epoch:  740 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.17376997966809637\n",
      "-----Epoch:  750 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.1727861226738672\n",
      "-----Epoch:  760 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.17184051241592388\n",
      "-----Epoch:  770 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.17093167634353043\n",
      "-----Epoch:  780 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.17005816184508404\n",
      "-----Epoch:  790 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.1692185426554386\n",
      "-----Epoch:  800 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.16841142411248572\n",
      "-----Epoch:  810 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.16763544737064132\n",
      "-----Epoch:  820 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.16688929268224084\n",
      "-----Epoch:  830 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.16617168185654707\n",
      "-----Epoch:  840 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.1654813800015527\n",
      "-----Epoch:  850 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.16481719664714767\n",
      "-----Epoch:  860 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.16417798634038128\n",
      "-----Epoch:  870 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.16356264879513693\n",
      "-----Epoch:  880 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1629701286699983\n",
      "-----Epoch:  890 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.16239941503975852\n",
      "-----Epoch:  900 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.16184954061810278\n",
      "-----Epoch:  910 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.16131958078161876\n",
      "-----Epoch:  920 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.16080865243852138\n",
      "-----Epoch:  930 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1603159127793494\n",
      "-----Epoch:  940 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1598405579413963\n",
      "-----Epoch:  950 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.159381821613753\n",
      "-----Epoch:  960 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15893897360553474\n",
      "-----Epoch:  970 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15851131839608967\n",
      "-----Epoch:  980 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15809819368270603\n",
      "-----Epoch:  990 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15769896893848748\n",
      "-----Epoch:  1000 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15731304399062562\n",
      "-----Epoch:  1010 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15693984762719781\n",
      "-----Epoch:  1020 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15657883623883984\n",
      "-----Epoch:  1030 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15622949250012658\n",
      "-----Epoch:  1040 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1558913240942264\n",
      "-----Epoch:  1050 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15556386248332385\n",
      "-----Epoch:  1060 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15524666172642249\n",
      "-----Epoch:  1070 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15493929734540443\n",
      "-----Epoch:  1080 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15464136523962432\n",
      "-----Epoch:  1090 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15435248064882795\n",
      "-----Epoch:  1100 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15407227716379462\n",
      "-----Epoch:  1110 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1538004057837927\n",
      "-----Epoch:  1120 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1535365340196968\n",
      "-----Epoch:  1130 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15328034504143354\n",
      "-----Epoch:  1140 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15303153686828397\n",
      "-----Epoch:  1150 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15278982160048213\n",
      "-----Epoch:  1160 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15255492469048174\n",
      "-----Epoch:  1170 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15232658425223183\n",
      "-----Epoch:  1180 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15210455040679033\n",
      "-----Epoch:  1190 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15188858466260755\n",
      "-----Epoch:  1200 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15167845932883522\n",
      "-----Epoch:  1210 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15147395696004523\n",
      "-----Epoch:  1220 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15127486983078278\n",
      "-----Epoch:  1230 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1510809994384269\n",
      "-----Epoch:  1240 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15089215603288025\n",
      "-----Epoch:  1250 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1507081581716666\n",
      "-----Epoch:  1260 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15052883229907107\n",
      "-----Epoch:  1270 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15035401234801463\n",
      "-----Epoch:  1280 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15018353936341572\n",
      "-----Epoch:  1290 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.15001726114584701\n",
      "-----Epoch:  1300 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14985503191435495\n",
      "-----Epoch:  1310 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14969671198736378\n",
      "-----Epoch:  1320 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1495421674806429\n",
      "-----Epoch:  1330 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14939127002136665\n",
      "-----Epoch:  1340 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14924389647734887\n",
      "-----Epoch:  1350 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14909992870058178\n",
      "-----Epoch:  1360 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14895925328425694\n",
      "-----Epoch:  1370 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14882176133249048\n",
      "-----Epoch:  1380 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1486873482420162\n",
      "-----Epoch:  1390 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14855591349515287\n",
      "-----Epoch:  1400 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1484273604633873\n",
      "-----Epoch:  1410 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14830159622095482\n",
      "-----Epoch:  1420 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1481785313678288\n",
      "-----Epoch:  1430 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14805807986156633\n",
      "-----Epoch:  1440 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14794015885748643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  1450 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14782468855668382\n",
      "-----Epoch:  1460 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14771159206141143\n",
      "-----Epoch:  1470 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1476007952373863\n",
      "-----Epoch:  1480 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14749222658259867\n",
      "-----Epoch:  1490 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14738581710222487\n",
      "-----Epoch:  1500 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14728150018926517\n",
      "-----Epoch:  1510 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14717921151054547\n",
      "-----Epoch:  1520 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14707888889773968\n",
      "-----Epoch:  1530 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14698047224308397\n",
      "-----Epoch:  1540 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14688390339947005\n",
      "-----Epoch:  1550 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1467891260846154\n",
      "-----Epoch:  1560 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14669608578902146\n",
      "-----Epoch:  1570 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14660472968744057\n",
      "-----Epoch:  1580 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14651500655357988\n",
      "-----Epoch:  1590 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1464268666777805\n",
      "-----Epoch:  1600 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14634026178741377\n",
      "-----Epoch:  1610 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1462551449697422\n",
      "-----Epoch:  1620 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14617147059699637\n",
      "-----Epoch:  1630 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1460891942534199\n",
      "-----Epoch:  1640 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14600827266403507\n",
      "-----Epoch:  1650 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14592866362488027\n",
      "-----Epoch:  1660 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14585032593446695\n",
      "-----Epoch:  1670 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14577321932619822\n",
      "-----Epoch:  1680 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14569730440148448\n",
      "-----Epoch:  1690 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1456225425632808\n",
      "-----Epoch:  1700 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14554889594975823\n",
      "-----Epoch:  1710 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14547632736780833\n",
      "-----Epoch:  1720 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14540480022605692\n",
      "-----Epoch:  1730 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14533427846704677\n",
      "-----Epoch:  1740 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14526472649821623\n",
      "-----Epoch:  1750 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14519610912127603\n",
      "-----Epoch:  1760 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14512839145954395\n",
      "-----Epoch:  1770 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14506153888275872\n",
      "-----Epoch:  1780 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14499551692884133\n",
      "-----Epoch:  1790 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14493029122201553\n",
      "-----Epoch:  1800 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14486582738662893\n",
      "-----Epoch:  1810 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1448020909559383\n",
      "-----Epoch:  1820 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14473904727502776\n",
      "-----Epoch:  1830 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14467666139692054\n",
      "-----Epoch:  1840 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1446148979708176\n",
      "-----Epoch:  1850 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14455372112124612\n",
      "-----Epoch:  1860 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14449309431672486\n",
      "-----Epoch:  1870 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1444329802263494\n",
      "-----Epoch:  1880 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1443733405624517\n",
      "-----Epoch:  1890 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1443141359072044\n",
      "-----Epoch:  1900 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.144255325520696\n",
      "-----Epoch:  1910 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14419686712759575\n",
      "-----Epoch:  1920 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14413871667904452\n",
      "-----Epoch:  1930 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14408082808582395\n",
      "-----Epoch:  1940 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1440231529181635\n",
      "-----Epoch:  1950 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14396564006670098\n",
      "-----Epoch:  1960 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14390823535810202\n",
      "-----Epoch:  1970 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14385088111760755\n",
      "-----Epoch:  1980 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14379351566928178\n",
      "-----Epoch:  1990 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14373607276289255\n",
      "-----Epoch:  2000 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14367848091410043\n",
      "-----Epoch:  2010 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14362066264184523\n",
      "-----Epoch:  2020 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14356253358336\n",
      "-----Epoch:  2030 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14350400146292946\n",
      "-----Epoch:  2040 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14344496488510614\n",
      "-----Epoch:  2050 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14338531191628104\n",
      "-----Epoch:  2060 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14332491840987058\n",
      "-----Epoch:  2070 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14326364601935904\n",
      "-----Epoch:  2080 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14320133982929284\n",
      "-----Epoch:  2090 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14313782551603718\n",
      "-----Epoch:  2100 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14307290592630723\n",
      "-----Epoch:  2110 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1430063569302698\n",
      "-----Epoch:  2120 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14293792236474517\n",
      "-----Epoch:  2130 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14286730782702173\n",
      "-----Epoch:  2140 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.142794173005794\n",
      "-----Epoch:  2150 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14271812213524224\n",
      "-----Epoch:  2160 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1426386920204369\n",
      "-----Epoch:  2170 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14255533689115582\n",
      "-----Epoch:  2180 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14246740907321845\n",
      "-----Epoch:  2190 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14237413408604072\n",
      "-----Epoch:  2200 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14227457822801753\n",
      "-----Epoch:  2210 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14216760591344785\n",
      "-----Epoch:  2220 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14205182284345813\n",
      "-----Epoch:  2230 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1419254993161371\n",
      "-----Epoch:  2240 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14178646526040628\n",
      "-----Epoch:  2250 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14163196433473532\n",
      "-----Epoch:  2260 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14145844767915863\n",
      "-----Epoch:  2270 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1412612769289965\n",
      "-----Epoch:  2280 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14103428782731317\n",
      "-----Epoch:  2290 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14076913461228588\n",
      "-----Epoch:  2300 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14045428082453512\n",
      "-----Epoch:  2310 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.14007340420296358\n",
      "-----Epoch:  2320 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.13960280275869294\n",
      "-----Epoch:  2330 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.13900704906334443\n",
      "-----Epoch:  2340 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1382314927433579\n",
      "-----Epoch:  2350 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.13718900285313793\n",
      "-----Epoch:  2360 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1357363094918191\n",
      "-----Epoch:  2370 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.13363325743625132\n",
      "-----Epoch:  2380 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.13048421614054\n",
      "-----Epoch:  2390 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.12571294349945564\n",
      "-----Epoch:  2400 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.11880013008131068\n",
      "-----Epoch:  2410 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1100421862053178\n",
      "-----Epoch:  2420 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.10084613468959472\n",
      "-----Epoch:  2430 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.09241871281431198\n",
      "-----Epoch:  2440 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.08503998272091895\n",
      "-----Epoch:  2450 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.07858620075342304\n",
      "-----Epoch:  2460 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.07290066349284369\n",
      "-----Epoch:  2470 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.06786061891810927\n",
      "-----Epoch:  2480 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.06337151409593526\n",
      "-----Epoch:  2490 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.05935803424345468\n",
      "-----Epoch:  2500 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.05575846327554576\n",
      "-----Epoch:  2510 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.05252113852701486\n",
      "-----Epoch:  2520 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.04960210988498637\n",
      "-----Epoch:  2530 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.0469635500129427\n",
      "-----Epoch:  2540 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.04457264557391874\n",
      "-----Epoch:  2550 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.04240079392447285\n",
      "-----Epoch:  2560 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.04042299266064221\n",
      "-----Epoch:  2570 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.03861735281814245\n",
      "-----Epoch:  2580 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.03696469488315795\n",
      "-----Epoch:  2590 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.035448204081590655\n",
      "-----Epoch:  2600 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.03405313136087018\n",
      "-----Epoch:  2610 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.03276653189801981\n",
      "-----Epoch:  2620 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.0315770357911802\n",
      "-----Epoch:  2630 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.030474647018549197\n",
      "-----Epoch:  2640 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.029450567472524964\n",
      "-----Epoch:  2650 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.028497043274086222\n",
      "-----Epoch:  2660 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.027607230833657265\n",
      "-----Epoch:  2670 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.026775080338176185\n",
      "-----Epoch:  2680 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.025995234545272042\n",
      "-----Epoch:  2690 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.025262940964606042\n",
      "-----Epoch:  2700 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.024573975703152777\n",
      "-----Epoch:  2710 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.02392457744168552\n",
      "-----Epoch:  2720 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.02331139018996718\n",
      "-----Epoch:  2730 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.02273141363514233\n",
      "-----Epoch:  2740 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.02218196004982941\n",
      "-----Epoch:  2750 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.02166061686279845\n",
      "-----Epoch:  2760 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.021165214116093206\n",
      "-----Epoch:  2770 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.02069379613879463\n",
      "-----Epoch:  2780 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.020244596860429925\n",
      "-----Epoch:  2790 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.01981601826758931\n",
      "-----Epoch:  2800 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.01940661157693984\n",
      "-----Epoch:  2810 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.019015060757818475\n",
      "-----Epoch:  2820 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.018640168089151567\n",
      "-----Epoch:  2830 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.018280841479702964\n",
      "-----Epoch:  2840 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.017936083318583436\n",
      "-----Epoch:  2850 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.017604980655450547\n",
      "-----Epoch:  2860 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.017286696537654966\n",
      "-----Epoch:  2870 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.016980462355420525\n",
      "-----Epoch:  2880 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.01668557106655757\n",
      "-----Epoch:  2890 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.01640137118970397\n",
      "-----Epoch:  2900 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.016127261470087206\n",
      "-----Epoch:  2910 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.01586268613467291\n",
      "-----Epoch:  2920 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.015607130664621641\n",
      "-----Epoch:  2930 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.01536011802247784\n",
      "-----Epoch:  2940 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.015121205279693664\n",
      "-----Epoch:  2950 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.014889980597136077\n",
      "-----Epoch:  2960 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.014666060517300862\n",
      "-----Epoch:  2970 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.014449087532204334\n",
      "-----Epoch:  2980 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.014238727895458709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  2990 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.01403466965096257\n",
      "-----Epoch:  3000 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.01383662085404096\n",
      "-----Epoch:  3010 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.013644307963821031\n",
      "-----Epoch:  3020 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.013457474388195148\n",
      "-----Epoch:  3030 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.013275879164955038\n",
      "-----Epoch:  3040 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.01309929576462453\n",
      "-----Epoch:  3050 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.012927511002215036\n",
      "-----Epoch:  3060 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.012760324046608543\n",
      "-----Epoch:  3070 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.012597545517569326\n",
      "-----Epoch:  3080 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.012438996661520214\n",
      "-----Epoch:  3090 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.012284508598213742\n",
      "-----Epoch:  3100 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.012133921631302922\n",
      "-----Epoch:  3110 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.011987084616584785\n",
      "-----Epoch:  3120 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.011843854382365487\n",
      "-----Epoch:  3130 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.011704095196993265\n",
      "-----Epoch:  3140 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.011567678279130597\n",
      "-----Epoch:  3150 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.011434481346803652\n",
      "-----Epoch:  3160 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.011304388201677574\n",
      "-----Epoch:  3170 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.011177288345371825\n",
      "-----Epoch:  3180 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.011053076624953521\n",
      "-----Epoch:  3190 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.010931652905033777\n",
      "-----Epoch:  3200 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.010812921764148761\n",
      "-----Epoch:  3210 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.010696792213334269\n",
      "-----Epoch:  3220 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.010583177435006062\n",
      "-----Epoch:  3230 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.010471994540440411\n",
      "-----Epoch:  3240 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.010363164344309533\n",
      "-----Epoch:  3250 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.010256611154873935\n",
      "-----Epoch:  3260 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.010152262578562334\n",
      "-----Epoch:  3270 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.010050049337786787\n",
      "-----Epoch:  3280 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.009949905100945556\n",
      "-----Epoch:  3290 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.009851766323660303\n",
      "-----Epoch:  3300 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00975557210037898\n",
      "-----Epoch:  3310 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.009661264025552571\n",
      "-----Epoch:  3320 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.009568786063661862\n",
      "-----Epoch:  3330 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.009478084427433723\n",
      "-----Epoch:  3340 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.009389107463642042\n",
      "-----Epoch:  3350 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.009301805545939323\n",
      "-----Epoch:  3360 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00921613097421184\n",
      "-----Epoch:  3370 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.009132037879991811\n",
      "-----Epoch:  3380 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00904948213749962\n",
      "-----Epoch:  3390 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00896842127992224\n",
      "-----Epoch:  3400 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.008888814420566183\n",
      "-----Epoch:  3410 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.008810622178552092\n",
      "-----Epoch:  3420 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00873380660874335\n",
      "-----Epoch:  3430 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.008658331135625414\n",
      "-----Epoch:  3440 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.008584160490874723\n",
      "-----Epoch:  3450 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.008511260654374622\n",
      "-----Epoch:  3460 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00843959879845523\n",
      "-----Epoch:  3470 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.008369143235150084\n",
      "-----Epoch:  3480 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.008299863366277785\n",
      "-----Epoch:  3490 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.008231729636170743\n",
      "-----Epoch:  3500 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.008164713486886358\n",
      "-----Epoch:  3510 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.008098787315747256\n",
      "-----Epoch:  3520 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.008033924435068385\n",
      "-----Epoch:  3530 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007970099033938373\n",
      "-----Epoch:  3540 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00790728614193265\n",
      "-----Epoch:  3550 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007845461594642748\n",
      "-----Epoch:  3560 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0077846020009157794\n",
      "-----Epoch:  3570 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007724684711704064\n",
      "-----Epoch:  3580 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007665687790431872\n",
      "-----Epoch:  3590 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007607589984792475\n",
      "-----Epoch:  3600 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007550370699895026\n",
      "-----Epoch:  3610 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007494009972684221\n",
      "-----Epoch:  3620 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007438488447563356\n",
      "-----Epoch:  3630 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007383787353152827\n",
      "-----Epoch:  3640 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007329888480123107\n",
      "-----Epoch:  3650 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007276774160043395\n",
      "-----Epoch:  3660 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007224427245191374\n",
      "-----Epoch:  3670 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007172831089272653\n",
      "-----Epoch:  3680 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007121969529002219\n",
      "-----Epoch:  3690 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007071826866502207\n",
      "-----Epoch:  3700 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.007022387852473836\n",
      "-----Epoch:  3710 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006973637670103426\n",
      "-----Epoch:  3720 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006925561919664984\n",
      "-----Epoch:  3730 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006878146603783957\n",
      "-----Epoch:  3740 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006831378113328897\n",
      "-----Epoch:  3750 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006785243213899602\n",
      "-----Epoch:  3760 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006739729032882256\n",
      "-----Epoch:  3770 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006694823047043537\n",
      "-----Epoch:  3780 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0066505130706375265\n",
      "-----Epoch:  3790 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006606787244000461\n",
      "-----Epoch:  3800 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006563634022609977\n",
      "-----Epoch:  3810 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006521042166586522\n",
      "-----Epoch:  3820 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006479000730616129\n",
      "-----Epoch:  3830 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006437499054274844\n",
      "-----Epoch:  3840 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006396526752735613\n",
      "-----Epoch:  3850 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006356073707840435\n",
      "-----Epoch:  3860 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006316130059520825\n",
      "-----Epoch:  3870 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006276686197550194\n",
      "-----Epoch:  3880 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00623773275361391\n",
      "-----Epoch:  3890 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006199260593682101\n",
      "-----Epoch:  3900 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006161260810671993\n",
      "-----Epoch:  3910 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006123724717386603\n",
      "-----Epoch:  3920 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006086643839717937\n",
      "-----Epoch:  3930 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0060500099101030765\n",
      "-----Epoch:  3940 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.006013814861221661\n",
      "-----Epoch:  3950 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005978050819925001\n",
      "-----Epoch:  3960 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005942710101386509\n",
      "-----Epoch:  3970 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005907785203463974\n",
      "-----Epoch:  3980 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0058732688012648965\n",
      "-----Epoch:  3990 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0058391537419061335\n",
      "-----Epoch:  4000 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005805433039459762\n",
      "-----Epoch:  4010 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005772099870077842\n",
      "-----Epoch:  4020 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005739147567287757\n",
      "-----Epoch:  4030 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005706569617452139\n",
      "-----Epoch:  4040 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005674359655385933\n",
      "-----Epoch:  4050 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0056425114601248005\n",
      "-----Epoch:  4060 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005611018950838194\n",
      "-----Epoch:  4070 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0055798761828817975\n",
      "-----Epoch:  4080 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00554907734398339\n",
      "-----Epoch:  4090 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005518616750557235\n",
      "-----Epoch:  4100 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005488488844141676\n",
      "-----Epoch:  4110 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005458688187955182\n",
      "-----Epoch:  4120 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005429209463566243\n",
      "-----Epoch:  4130 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005400047467672734\n",
      "-----Epoch:  4140 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005371197108986613\n",
      "-----Epoch:  4150 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0053426534052197945\n",
      "-----Epoch:  4160 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005314411480167435\n",
      "-----Epoch:  4170 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005286466560884807\n",
      "-----Epoch:  4180 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0052588139749547465\n",
      "-----Epoch:  4190 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00523144914784141\n",
      "-----Epoch:  4200 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005204367600328111\n",
      "-----Epoch:  4210 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005177564946035479\n",
      "-----Epoch:  4220 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005151036889017126\n",
      "-----Epoch:  4230 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005124779221430184\n",
      "-----Epoch:  4240 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005098787821277839\n",
      "-----Epoch:  4250 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005073058650221257\n",
      "-----Epoch:  4260 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005047587751458394\n",
      "-----Epoch:  4270 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.005022371247667561\n",
      "-----Epoch:  4280 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004997405339013041\n",
      "-----Epoch:  4290 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0049726863012106775\n",
      "-----Epoch:  4300 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004948210483651772\n",
      "-----Epoch:  4310 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004923974307582454\n",
      "-----Epoch:  4320 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004899974264337389\n",
      "-----Epoch:  4330 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004876206913625322\n",
      "-----Epoch:  4340 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004852668881865138\n",
      "-----Epoch:  4350 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004829356860570239\n",
      "-----Epoch:  4360 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004806267604780296\n",
      "-----Epoch:  4370 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0047833979315377055\n",
      "-----Epoch:  4380 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004760744718408556\n",
      "-----Epoch:  4390 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0047383049020454\n",
      "-----Epoch:  4400 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004716075476791256\n",
      "-----Epoch:  4410 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004694053493322906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  4420 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004672236057332791\n",
      "-----Epoch:  4430 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0046506203282476715\n",
      "-----Epoch:  4440 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004629203517983009\n",
      "-----Epoch:  4450 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004607982889732336\n",
      "-----Epoch:  4460 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00458695575678963\n",
      "-----Epoch:  4470 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004566119481404551\n",
      "-----Epoch:  4480 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004545471473668728\n",
      "-----Epoch:  4490 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004525009190432603\n",
      "-----Epoch:  4500 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004504730134251647\n",
      "-----Epoch:  4510 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00448463185236106\n",
      "-----Epoch:  4520 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004464711935677901\n",
      "-----Epoch:  4530 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00444496801783027\n",
      "-----Epoch:  4540 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004425397774211965\n",
      "-----Epoch:  4550 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004405998921062457\n",
      "-----Epoch:  4560 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004386769214571271\n",
      "-----Epoch:  4570 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004367706450005492\n",
      "-----Epoch:  4580 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004348808460860617\n",
      "-----Epoch:  4590 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0043300731180331235\n",
      "-----Epoch:  4600 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0043114983290147724\n",
      "-----Epoch:  4610 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004293082037107617\n",
      "-----Epoch:  4620 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004274822220659102\n",
      "-----Epoch:  4630 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0042567168923169875\n",
      "-----Epoch:  4640 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0042387640983029195\n",
      "-----Epoch:  4650 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004220961917704858\n",
      "-----Epoch:  4660 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004203308461787014\n",
      "-----Epoch:  4670 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004185801873317301\n",
      "-----Epoch:  4680 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004168440325911629\n",
      "-----Epoch:  4690 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004151222023394667\n",
      "-----Epoch:  4700 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004134145199176223\n",
      "-----Epoch:  4710 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004117208115643248\n",
      "-----Epoch:  4720 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004100409063566868\n",
      "-----Epoch:  4730 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004083746361523678\n",
      "-----Epoch:  4740 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004067218355331462\n",
      "-----Epoch:  4750 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004050823417498553\n",
      "-----Epoch:  4760 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004034559946686399\n",
      "-----Epoch:  4770 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004018426367185394\n",
      "-----Epoch:  4780 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.004002421128402839\n",
      "-----Epoch:  4790 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003986542704363684\n",
      "-----Epoch:  4800 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003970789593222774\n",
      "-----Epoch:  4810 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0039551603167888435\n",
      "-----Epoch:  4820 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003939653420059723\n",
      "-----Epoch:  4830 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003924267470768548\n",
      "-----Epoch:  4840 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0039090010589404615\n",
      "-----Epoch:  4850 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003893852796459907\n",
      "-----Epoch:  4860 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003878821316647642\n",
      "-----Epoch:  4870 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0038639052738479607\n",
      "-----Epoch:  4880 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0038491033430252406\n",
      "-----Epoch:  4890 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0038344142193695884\n",
      "-----Epoch:  4900 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003819836617911838\n",
      "-----Epoch:  4910 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0038053692731471905\n",
      "-----Epoch:  4920 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0037910109386673748\n",
      "-----Epoch:  4930 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0037767603868012357\n",
      "-----Epoch:  4940 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003762616408263217\n",
      "-----Epoch:  4950 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003748577811810102\n",
      "-----Epoch:  4960 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003734643423905037\n",
      "-----Epoch:  4970 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003720812088389341\n",
      "-----Epoch:  4980 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003707082666161463\n",
      "-----Epoch:  4990 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0036934540348630544\n",
      "-----Epoch:  5000 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0036799250885720548\n",
      "-----Epoch:  5010 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0036664947375023606\n",
      "-----Epoch:  5020 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003653161907710259\n",
      "-----Epoch:  5030 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0036399255408070784\n",
      "-----Epoch:  5040 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0036267845936781592\n",
      "-----Epoch:  5050 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003613738038208039\n",
      "-----Epoch:  5060 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0036007848610111965\n",
      "-----Epoch:  5070 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0035879240631690717\n",
      "-----Epoch:  5080 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0035751546599723134\n",
      "-----Epoch:  5090 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0035624756806686784\n",
      "-----Epoch:  5100 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003549886168216344\n",
      "-----Epoch:  5110 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003537385179042363\n",
      "-----Epoch:  5120 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0035249717828062218\n",
      "-----Epoch:  5130 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0035126450621683646\n",
      "-----Epoch:  5140 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0035004041125637183\n",
      "-----Epoch:  5150 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003488248041979573\n",
      "-----Epoch:  5160 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00347617597073859\n",
      "-----Epoch:  5170 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003464187031285858\n",
      "-----Epoch:  5180 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003452280367980665\n",
      "-----Epoch:  5190 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0034404551368922955\n",
      "-----Epoch:  5200 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0034287105056002272\n",
      "-----Epoch:  5210 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0034170456529984366\n",
      "-----Epoch:  5220 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0034054597691033786\n",
      "-----Epoch:  5230 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003393952054866199\n",
      "-----Epoch:  5240 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0033825217219886207\n",
      "-----Epoch:  5250 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003371167992742565\n",
      "-----Epoch:  5260 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0033598900997933085\n",
      "-----Epoch:  5270 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0033486872860263688\n",
      "-----Epoch:  5280 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0033375588043775095\n",
      "-----Epoch:  5290 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003326503917666693\n",
      "-----Epoch:  5300 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0033155218984347065\n",
      "-----Epoch:  5310 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003304612028783334\n",
      "-----Epoch:  5320 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0032937736002187495\n",
      "-----Epoch:  5330 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0032830059134977756\n",
      "-----Epoch:  5340 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0032723082784772333\n",
      "-----Epoch:  5350 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003261680013966384\n",
      "-----Epoch:  5360 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003251120447581891\n",
      "-----Epoch:  5370 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003240628915605968\n",
      "-----Epoch:  5380 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0032302047628469965\n",
      "-----Epoch:  5390 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003219847342503024\n",
      "-----Epoch:  5400 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0032095560160276247\n",
      "-----Epoch:  5410 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0031993301529987003\n",
      "-----Epoch:  5420 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0031891691309893773\n",
      "-----Epoch:  5430 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0031790723354417068\n",
      "-----Epoch:  5440 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003169039159542434\n",
      "-----Epoch:  5450 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003159069004101407\n",
      "-----Epoch:  5460 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0031491612774321113\n",
      "-----Epoch:  5470 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0031393153952344504\n",
      "-----Epoch:  5480 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0031295307804797556\n",
      "-----Epoch:  5490 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0031198068632980365\n",
      "-----Epoch:  5500 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003110143080866979\n",
      "-----Epoch:  5510 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003100538877303563\n",
      "-----Epoch:  5520 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0030909937035570913\n",
      "-----Epoch:  5530 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0030815070173046137\n",
      "-----Epoch:  5540 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0030720782828481037\n",
      "-----Epoch:  5550 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003062706971013418\n",
      "-----Epoch:  5560 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00305339255905136\n",
      "-----Epoch:  5570 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0030441345305403465\n",
      "-----Epoch:  5580 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003034932375290818\n",
      "-----Epoch:  5590 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.003025785589251571\n",
      "-----Epoch:  5600 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0030166936744175257\n",
      "-----Epoch:  5610 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0030076561387394167\n",
      "-----Epoch:  5620 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00299867249603481\n",
      "-----Epoch:  5630 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0029897422659010426\n",
      "-----Epoch:  5640 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0029808649736293644\n",
      "-----Epoch:  5650 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002972040150120882\n",
      "-----Epoch:  5660 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0029632673318038144\n",
      "-----Epoch:  5670 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002954546060552438\n",
      "-----Epoch:  5680 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002945875883607058\n",
      "-----Epoch:  5690 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0029372563534957683\n",
      "-----Epoch:  5700 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0029286870279573956\n",
      "-----Epoch:  5710 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002920167469865926\n",
      "-----Epoch:  5720 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002911697247156018\n",
      "-----Epoch:  5730 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002903275932749902\n",
      "-----Epoch:  5740 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002894903104485771\n",
      "-----Epoch:  5750 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0028865783450470876\n",
      "-----Epoch:  5760 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0028783012418931823\n",
      "-----Epoch:  5770 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0028700713871912253\n",
      "-----Epoch:  5780 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0028618883777491483\n",
      "-----Epoch:  5790 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002853751814949719\n",
      "-----Epoch:  5800 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0028456613046860015\n",
      "-----Epoch:  5810 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0028376164572975373\n",
      "-----Epoch:  5820 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002829616887507837\n",
      "-----Epoch:  5830 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002821662214362885\n",
      "-----Epoch:  5840 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002813752061170643\n",
      "-----Epoch:  5850 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002805886055441619\n",
      "-----Epoch:  5860 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0027980638288303394\n",
      "-----Epoch:  5870 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0027902850170779324\n",
      "-----Epoch:  5880 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0027825492599554795\n",
      "-----Epoch:  5890 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002774856201208552\n",
      "-----Epoch:  5900 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002767205488502351\n",
      "-----Epoch:  5910 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0027595967733681642\n",
      "-----Epoch:  5920 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0027520297111501254\n",
      "-----Epoch:  5930 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0027445039609535287\n",
      "-----Epoch:  5940 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0027370191855934853\n",
      "-----Epoch:  5950 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0027295750515445322\n",
      "-----Epoch:  5960 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002722171228891156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  5970 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002714807391279102\n",
      "-----Epoch:  5980 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002707483215867379\n",
      "-----Epoch:  5990 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0027001983832810592\n",
      "-----Epoch:  6000 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002692952577565023\n",
      "-----Epoch:  6010 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0026857454861380775\n",
      "-----Epoch:  6020 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0026785767997482505\n",
      "-----Epoch:  6030 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0026714462124283194\n",
      "-----Epoch:  6040 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002664353421452582\n",
      "-----Epoch:  6050 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002657298127293737\n",
      "-----Epoch:  6060 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002650280033581044\n",
      "-----Epoch:  6070 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002643298847058535\n",
      "-----Epoch:  6080 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002636354277544395\n",
      "-----Epoch:  6090 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0026294460378907597\n",
      "-----Epoch:  6100 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0026225738439440717\n",
      "-----Epoch:  6110 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0026157374145060995\n",
      "-----Epoch:  6120 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002608936471295833\n",
      "-----Epoch:  6130 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002602170738911464\n",
      "-----Epoch:  6140 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002595439944793426\n",
      "-----Epoch:  6150 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0025887438191877533\n",
      "-----Epoch:  6160 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002582082095110026\n",
      "-----Epoch:  6170 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0025754545083099723\n",
      "-----Epoch:  6180 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002568860797236488\n",
      "-----Epoch:  6190 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002562300703003299\n",
      "-----Epoch:  6200 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0025557739693550905\n",
      "-----Epoch:  6210 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0025492803426341386\n",
      "-----Epoch:  6220 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0025428195717475226\n",
      "-----Epoch:  6230 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002536391408134754\n",
      "-----Epoch:  6240 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00252999560573582\n",
      "-----Epoch:  6250 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0025236319209600644\n",
      "-----Epoch:  6260 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0025173001126549783\n",
      "-----Epoch:  6270 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002510999942076072\n",
      "-----Epoch:  6280 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0025047311728565213\n",
      "-----Epoch:  6290 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0024984935709780652\n",
      "-----Epoch:  6300 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0024922869047415766\n",
      "-----Epoch:  6310 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0024861109447384226\n",
      "-----Epoch:  6320 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0024799654638225175\n",
      "-----Epoch:  6330 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002473850237082082\n",
      "-----Epoch:  6340 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0024677650418125363\n",
      "-----Epoch:  6350 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0024617096574893523\n",
      "-----Epoch:  6360 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00245568386574152\n",
      "-----Epoch:  6370 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0024496874503251733\n",
      "-----Epoch:  6380 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0024437201970979924\n",
      "-----Epoch:  6390 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0024377818939934\n",
      "-----Epoch:  6400 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002431872330995821\n",
      "-----Epoch:  6410 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002425991300115702\n",
      "-----Epoch:  6420 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0024201385953651483\n",
      "-----Epoch:  6430 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002414314012734098\n",
      "-----Epoch:  6440 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0024085173501663705\n",
      "-----Epoch:  6450 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0024027484075364843\n",
      "-----Epoch:  6460 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0023970069866266876\n",
      "-----Epoch:  6470 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0023912928911041364\n",
      "-----Epoch:  6480 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0023856059264986645\n",
      "-----Epoch:  6490 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002379945900180657\n",
      "-----Epoch:  6500 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0023743126213393233\n",
      "-----Epoch:  6510 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002368705900961405\n",
      "-----Epoch:  6520 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002363125551809904\n",
      "-----Epoch:  6530 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0023575713884033457\n",
      "-----Epoch:  6540 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00235204322699521\n",
      "-----Epoch:  6550 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002346540885553721\n",
      "-----Epoch:  6560 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0023410641837419473\n",
      "-----Epoch:  6570 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0023356129428978864\n",
      "-----Epoch:  6580 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0023301869860154166\n",
      "-----Epoch:  6590 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00232478613772482\n",
      "-----Epoch:  6600 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002319410224274172\n",
      "-----Epoch:  6610 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0023140590735105417\n",
      "-----Epoch:  6620 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0023087325148617908\n",
      "-----Epoch:  6630 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002303430379318304\n",
      "-----Epoch:  6640 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002298152499415329\n",
      "-----Epoch:  6650 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0022928987092151374\n",
      "-----Epoch:  6660 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0022876688442898525\n",
      "-----Epoch:  6670 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0022824627417043384\n",
      "-----Epoch:  6680 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002277280239999035\n",
      "-----Epoch:  6690 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0022721211791735542\n",
      "-----Epoch:  6700 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002266985400670134\n",
      "-----Epoch:  6710 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002261872747357388\n",
      "-----Epoch:  6720 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002256783063514362\n",
      "-----Epoch:  6730 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0022517161948147585\n",
      "-----Epoch:  6740 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002246671988311224\n",
      "-----Epoch:  6750 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0022416502924201206\n",
      "-----Epoch:  6760 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0022366509569062987\n",
      "-----Epoch:  6770 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0022316738328681655\n",
      "-----Epoch:  6780 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002226718772722903\n",
      "-----Epoch:  6790 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002221785630191889\n",
      "-----Epoch:  6800 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002216874260286324\n",
      "-----Epoch:  6810 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002211984519293072\n",
      "-----Epoch:  6820 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002207116264760698\n",
      "-----Epoch:  6830 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0022022693554854815\n",
      "-----Epoch:  6840 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002197443651497939\n",
      "-----Epoch:  6850 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0021926390140494622\n",
      "-----Epoch:  6860 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002187855305598723\n",
      "-----Epoch:  6870 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0021830923897988514\n",
      "-----Epoch:  6880 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0021783501314843836\n",
      "-----Epoch:  6890 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0021736283966585393\n",
      "-----Epoch:  6900 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0021689270524805388\n",
      "-----Epoch:  6910 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002164245967253244\n",
      "-----Epoch:  6920 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002159585010410865\n",
      "-----Epoch:  6930 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0021549440525067617\n",
      "-----Epoch:  6940 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0021503229652015984\n",
      "-----Epoch:  6950 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0021457216212514333\n",
      "-----Epoch:  6960 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0021411398944961267\n",
      "-----Epoch:  6970 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002136577659847765\n",
      "-----Epoch:  6980 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0021320347932793477\n",
      "-----Epoch:  6990 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002127511171813479\n",
      "-----Epoch:  7000 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0021230066735114426\n",
      "-----Epoch:  7010 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0021185211774621014\n",
      "-----Epoch:  7020 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0021140545637711944\n",
      "-----Epoch:  7030 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002109606713550547\n",
      "-----Epoch:  7040 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0021051775089077755\n",
      "-----Epoch:  7050 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002100766832935621\n",
      "-----Epoch:  7060 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0020963745697018268\n",
      "-----Epoch:  7070 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0020920006042390257\n",
      "-----Epoch:  7080 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002087644822534577\n",
      "-----Epoch:  7090 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002083307111520835\n",
      "-----Epoch:  7100 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0020789873590652394\n",
      "-----Epoch:  7110 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0020746854539607975\n",
      "-----Epoch:  7120 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0020704012859164684\n",
      "-----Epoch:  7130 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0020661347455478087\n",
      "-----Epoch:  7140 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0020618857243675476\n",
      "-----Epoch:  7150 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002057654114776674\n",
      "-----Epoch:  7160 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00205343981005508\n",
      "-----Epoch:  7170 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0020492427043527755\n",
      "-----Epoch:  7180 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00204506269268095\n",
      "-----Epoch:  7190 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0020408996709032783\n",
      "-----Epoch:  7200 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0020367535357272994\n",
      "-----Epoch:  7210 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002032624184695925\n",
      "-----Epoch:  7220 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00202851151617878\n",
      "-----Epoch:  7230 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0020244154293642814\n",
      "-----Epoch:  7240 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0020203358242510167\n",
      "-----Epoch:  7250 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0020162726016398377\n",
      "-----Epoch:  7260 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0020122256631258315\n",
      "-----Epoch:  7270 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002008194911090231\n",
      "-----Epoch:  7280 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.002004180248692877\n",
      "-----Epoch:  7290 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0020001815798640784\n",
      "-----Epoch:  7300 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0019961988092973095\n",
      "-----Epoch:  7310 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001992231842441445\n",
      "-----Epoch:  7320 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001988280585493435\n",
      "-----Epoch:  7330 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00198434494539071\n",
      "-----Epoch:  7340 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001980424829804185\n",
      "-----Epoch:  7350 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0019765201471307983\n",
      "-----Epoch:  7360 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0019726308064864652\n",
      "-----Epoch:  7370 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001968756717699098\n",
      "-----Epoch:  7380 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0019648977913015965\n",
      "-----Epoch:  7390 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0019610539385249586\n",
      "-----Epoch:  7400 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001957225071291521\n",
      "-----Epoch:  7410 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0019534111022082162\n",
      "-----Epoch:  7420 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00194961194455996\n",
      "-----Epoch:  7430 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0019458275123031038\n",
      "-----Epoch:  7440 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0019420577200588538\n",
      "-----Epoch:  7450 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0019383024831069852\n",
      "-----Epoch:  7460 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0019345617173794202\n",
      "-----Epoch:  7470 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0019308353394540093\n",
      "-----Epoch:  7480 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001927123266548337\n",
      "-----Epoch:  7490 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0019234254165136158\n",
      "-----Epoch:  7500 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001919741707828561\n",
      "-----Epoch:  7510 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0019160720595934669\n",
      "-----Epoch:  7520 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001912416391524372\n",
      "-----Epoch:  7530 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001908774623947037\n",
      "-----Epoch:  7540 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0019051466777913043\n",
      "-----Epoch:  7550 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0019015324745853654\n",
      "-----Epoch:  7560 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018979319364500335\n",
      "-----Epoch:  7570 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001894344986093282\n",
      "-----Epoch:  7580 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018907715468045863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  7590 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001887211542449652\n",
      "-----Epoch:  7600 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018836648974647815\n",
      "-----Epoch:  7610 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018801315368517178\n",
      "-----Epoch:  7620 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018766113861723337\n",
      "-----Epoch:  7630 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018731043715433115\n",
      "-----Epoch:  7640 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018696104196311014\n",
      "-----Epoch:  7650 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018661294576468156\n",
      "-----Epoch:  7660 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018626614133410873\n",
      "-----Epoch:  7670 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001859206214999189\n",
      "-----Epoch:  7680 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018557637914359628\n",
      "-----Epoch:  7690 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018523340719911277\n",
      "-----Epoch:  7700 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018489169865242895\n",
      "-----Epoch:  7710 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018455124654102068\n",
      "-----Epoch:  7720 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001842120439534169\n",
      "-----Epoch:  7730 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018387408402871766\n",
      "-----Epoch:  7740 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018353735995614519\n",
      "-----Epoch:  7750 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001832018649745813\n",
      "-----Epoch:  7760 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018286759237210852\n",
      "-----Epoch:  7770 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001825345354855853\n",
      "-----Epoch:  7780 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018220268770017653\n",
      "-----Epoch:  7790 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018187204244893775\n",
      "-----Epoch:  7800 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018154259321236446\n",
      "-----Epoch:  7810 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001812143335179913\n",
      "-----Epoch:  7820 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0018088725693993684\n",
      "-----Epoch:  7830 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001805613570985064\n",
      "-----Epoch:  7840 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001802366276597749\n",
      "-----Epoch:  7850 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017991306233517274\n",
      "-----Epoch:  7860 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001795906548810716\n",
      "-----Epoch:  7870 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017926939909841462\n",
      "-----Epoch:  7880 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017894928883228564\n",
      "-----Epoch:  7890 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017863031797152964\n",
      "-----Epoch:  7900 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017831248044837336\n",
      "-----Epoch:  7910 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017799577023802906\n",
      "-----Epoch:  7920 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017768018135831858\n",
      "-----Epoch:  7930 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017736570786929633\n",
      "-----Epoch:  7940 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001770523438728846\n",
      "-----Epoch:  7950 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017674008351249639\n",
      "-----Epoch:  7960 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017642892097267015\n",
      "-----Epoch:  7970 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00176118850478717\n",
      "-----Epoch:  7980 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017580986629636551\n",
      "-----Epoch:  7990 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017550196273140154\n",
      "-----Epoch:  8000 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017519513412931399\n",
      "-----Epoch:  8010 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017488937487496728\n",
      "-----Epoch:  8020 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017458467939224472\n",
      "-----Epoch:  8030 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017428104214371914\n",
      "-----Epoch:  8040 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017397845763030514\n",
      "-----Epoch:  8050 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017367692039094312\n",
      "-----Epoch:  8060 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017337642500225969\n",
      "-----Epoch:  8070 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017307696607825381\n",
      "-----Epoch:  8080 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017277853826996516\n",
      "-----Epoch:  8090 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017248113626516272\n",
      "-----Epoch:  8100 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017218475478802738\n",
      "-----Epoch:  8110 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017188938859883885\n",
      "-----Epoch:  8120 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017159503249367153\n",
      "-----Epoch:  8130 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017130168130409702\n",
      "-----Epoch:  8140 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017100932989685689\n",
      "-----Epoch:  8150 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017071797317359167\n",
      "-----Epoch:  8160 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017042760607052867\n",
      "-----Epoch:  8170 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0017013822355819415\n",
      "-----Epoch:  8180 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016984982064112764\n",
      "-----Epoch:  8190 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016956239235758246\n",
      "-----Epoch:  8200 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016927593377925557\n",
      "-----Epoch:  8210 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001689904400110013\n",
      "-----Epoch:  8220 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016870590619054813\n",
      "-----Epoch:  8230 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001684223274882348\n",
      "-----Epoch:  8240 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016813969910671718\n",
      "-----Epoch:  8250 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016785801628072191\n",
      "-----Epoch:  8260 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016757727427677397\n",
      "-----Epoch:  8270 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016729746839290777\n",
      "-----Epoch:  8280 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016701859395844146\n",
      "-----Epoch:  8290 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001667406463336906\n",
      "-----Epoch:  8300 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001664636209097234\n",
      "-----Epoch:  8310 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016618751310810585\n",
      "-----Epoch:  8320 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016591231838064228\n",
      "-----Epoch:  8330 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001656380322091387\n",
      "-----Epoch:  8340 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016536465010514444\n",
      "-----Epoch:  8350 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016509216760971419\n",
      "-----Epoch:  8360 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016482058029315902\n",
      "-----Epoch:  8370 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016454988375482272\n",
      "-----Epoch:  8380 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016428007362282735\n",
      "-----Epoch:  8390 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001640111455538398\n",
      "-----Epoch:  8400 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016374309523285282\n",
      "-----Epoch:  8410 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016347591837294385\n",
      "-----Epoch:  8420 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016320961071504747\n",
      "-----Epoch:  8430 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016294416802772748\n",
      "-----Epoch:  8440 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016267958610696705\n",
      "-----Epoch:  8450 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00162415860775929\n",
      "-----Epoch:  8460 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016215298788474141\n",
      "-----Epoch:  8470 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016189096331029663\n",
      "-----Epoch:  8480 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016162978295601177\n",
      "-----Epoch:  8490 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016136944275162775\n",
      "-----Epoch:  8500 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001611099386530017\n",
      "-----Epoch:  8510 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016085126664188648\n",
      "-----Epoch:  8520 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016059342272574083\n",
      "-----Epoch:  8530 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016033640293750774\n",
      "-----Epoch:  8540 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0016008020333541696\n",
      "-----Epoch:  8550 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.00159824820002789\n",
      "-----Epoch:  8560 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015957024904782427\n",
      "-----Epoch:  8570 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001593164866034251\n",
      "-----Epoch:  8580 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015906352882697706\n",
      "-----Epoch:  8590 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015881137190017097\n",
      "-----Epoch:  8600 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001585600120288111\n",
      "-----Epoch:  8610 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015830944544261824\n",
      "-----Epoch:  8620 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001580596683950391\n",
      "-----Epoch:  8630 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015781067716307056\n",
      "-----Epoch:  8640 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015756246804706697\n",
      "-----Epoch:  8650 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015731503737056538\n",
      "-----Epoch:  8660 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015706838148009537\n",
      "-----Epoch:  8670 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015682249674499856\n",
      "-----Epoch:  8680 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015657737955726185\n",
      "-----Epoch:  8690 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001563330263313361\n",
      "-----Epoch:  8700 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015608943350395145\n",
      "-----Epoch:  8710 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015584659753396495\n",
      "-----Epoch:  8720 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015560451490216586\n",
      "-----Epoch:  8730 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015536318211112911\n",
      "-----Epoch:  8740 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015512259568502833\n",
      "-----Epoch:  8750 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015488275216947699\n",
      "-----Epoch:  8760 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015464364813137172\n",
      "-----Epoch:  8770 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015440528015870888\n",
      "-----Epoch:  8780 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015416764486044128\n",
      "-----Epoch:  8790 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015393073886630832\n",
      "-----Epoch:  8800 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015369455882667804\n",
      "-----Epoch:  8810 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015345910141239408\n",
      "-----Epoch:  8820 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015322436331461561\n",
      "-----Epoch:  8830 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015299034124465884\n",
      "-----Epoch:  8840 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015275703193385454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  8850 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015252443213337966\n",
      "-----Epoch:  8860 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001522925386141286\n",
      "-----Epoch:  8870 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015206134816654875\n",
      "-----Epoch:  8880 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015183085760048873\n",
      "-----Epoch:  8890 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015160106374507085\n",
      "-----Epoch:  8900 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015137196344852564\n",
      "-----Epoch:  8910 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015114355357806017\n",
      "-----Epoch:  8920 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015091583101971159\n",
      "-----Epoch:  8930 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015068879267820965\n",
      "-----Epoch:  8940 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015046243547683158\n",
      "-----Epoch:  8950 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015023675635726355\n",
      "-----Epoch:  8960 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0015001175227947327\n",
      "-----Epoch:  8970 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014978742022154822\n",
      "-----Epoch:  8980 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014956375717960053\n",
      "-----Epoch:  8990 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014934076016759512\n",
      "-----Epoch:  9000 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001491184262172348\n",
      "-----Epoch:  9010 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014889675237783166\n",
      "-----Epoch:  9020 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001486757357161689\n",
      "-----Epoch:  9030 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014845537331637782\n",
      "-----Epoch:  9040 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014823566227979738\n",
      "-----Epoch:  9050 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001480165997248648\n",
      "-----Epoch:  9060 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014779818278698037\n",
      "-----Epoch:  9070 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014758040861838615\n",
      "-----Epoch:  9080 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014736327438803292\n",
      "-----Epoch:  9090 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001471467772814699\n",
      "-----Epoch:  9100 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014693091450071866\n",
      "-----Epoch:  9110 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014671568326415831\n",
      "-----Epoch:  9120 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014650108080638939\n",
      "-----Epoch:  9130 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014628710437813123\n",
      "-----Epoch:  9140 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014607375124610833\n",
      "-----Epoch:  9150 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001458610186929132\n",
      "-----Epoch:  9160 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014564890401691723\n",
      "-----Epoch:  9170 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001454374045321346\n",
      "-----Epoch:  9180 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014522651756812097\n",
      "-----Epoch:  9190 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014501624046986308\n",
      "-----Epoch:  9200 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014480657059765957\n",
      "-----Epoch:  9210 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014459750532701068\n",
      "-----Epoch:  9220 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014438904204851358\n",
      "-----Epoch:  9230 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014418117816776084\n",
      "-----Epoch:  9240 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014397391110521588\n",
      "-----Epoch:  9250 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014376723829612045\n",
      "-----Epoch:  9260 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014356115719038444\n",
      "-----Epoch:  9270 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014335566525248114\n",
      "-----Epoch:  9280 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001431507599613393\n",
      "-----Epoch:  9290 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014294643881024739\n",
      "-----Epoch:  9300 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014274269930675014\n",
      "-----Epoch:  9310 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014253953897254154\n",
      "-----Epoch:  9320 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014233695534337392\n",
      "-----Epoch:  9330 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001421349459689474\n",
      "-----Epoch:  9340 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001419335084128234\n",
      "-----Epoch:  9350 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014173264025232107\n",
      "-----Epoch:  9360 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014153233907840822\n",
      "-----Epoch:  9370 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014133260249563264\n",
      "-----Epoch:  9380 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001411334281220005\n",
      "-----Epoch:  9390 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014093481358889662\n",
      "-----Epoch:  9400 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014073675654098661\n",
      "-----Epoch:  9410 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001405392546361243\n",
      "-----Epoch:  9420 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014034230554525315\n",
      "-----Epoch:  9430 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0014014590695233392\n",
      "-----Epoch:  9440 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013995005655422907\n",
      "-----Epoch:  9450 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013975475206062706\n",
      "-----Epoch:  9460 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001395599911939595\n",
      "-----Epoch:  9470 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013936577168928948\n",
      "-----Epoch:  9480 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013917209129425644\n",
      "-----Epoch:  9490 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013897894776895717\n",
      "-----Epoch:  9500 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013878633888588475\n",
      "-----Epoch:  9510 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013859426242981993\n",
      "-----Epoch:  9520 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013840271619777421\n",
      "-----Epoch:  9530 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001382116979988779\n",
      "-----Epoch:  9540 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013802120565431807\n",
      "-----Epoch:  9550 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013783123699723954\n",
      "-----Epoch:  9560 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013764178987267406\n",
      "-----Epoch:  9570 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013745286213746044\n",
      "-----Epoch:  9580 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013726445166014894\n",
      "-----Epoch:  9590 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001370765563209394\n",
      "-----Epoch:  9600 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001368891740115936\n",
      "-----Epoch:  9610 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013670230263534634\n",
      "-----Epoch:  9620 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013651594010685027\n",
      "-----Epoch:  9630 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001363300843520771\n",
      "-----Epoch:  9640 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001361447333082544\n",
      "-----Epoch:  9650 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013595988492378591\n",
      "-----Epoch:  9660 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001357755371581695\n",
      "-----Epoch:  9670 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001355916879819328\n",
      "-----Epoch:  9680 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013540833537654584\n",
      "-----Epoch:  9690 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013522547733436229\n",
      "-----Epoch:  9700 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013504311185853631\n",
      "-----Epoch:  9710 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001348612369629553\n",
      "-----Epoch:  9720 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013467985067216193\n",
      "-----Epoch:  9730 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013449895102128698\n",
      "-----Epoch:  9740 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001343185360559774\n",
      "-----Epoch:  9750 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013413860383232625\n",
      "-----Epoch:  9760 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013395915241680562\n",
      "-----Epoch:  9770 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001337801798861944\n",
      "-----Epoch:  9780 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013360168432750765\n",
      "-----Epoch:  9790 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013342366383793025\n",
      "-----Epoch:  9800 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001332461165247581\n",
      "-----Epoch:  9810 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013306904050531432\n",
      "-----Epoch:  9820 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013289243390690046\n",
      "-----Epoch:  9830 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013271629486671364\n",
      "-----Epoch:  9840 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013254062153179684\n",
      "-----Epoch:  9850 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001323654120589615\n",
      "-----Epoch:  9860 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013219066461473109\n",
      "-----Epoch:  9870 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013201637737528067\n",
      "-----Epoch:  9880 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013184254852635021\n",
      "-----Epoch:  9890 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013166917626322207\n",
      "-----Epoch:  9900 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013149625879061981\n",
      "-----Epoch:  9910 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013132379432266345\n",
      "-----Epoch:  9920 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013115178108281734\n",
      "-----Epoch:  9930 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.001309802173038074\n",
      "-----Epoch:  9940 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013080910122757938\n",
      "-----Epoch:  9950 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013063843110522903\n",
      "-----Epoch:  9960 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013046820519694493\n",
      "-----Epoch:  9970 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013029842177194788\n",
      "-----Epoch:  9980 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0013012907910844452\n",
      "-----Epoch:  9990 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.0012996017549355002\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[0, 0, 1, 1], \n",
    "                    [0, 1, 0, 1]]) # 2 inputs and 4 samples, i.e 2x4\n",
    "y_train = np.array([0, 1, 1, 0]) #1 x num of samples\n",
    "Xor_net = NeuralNet('binary_logloss')\n",
    "Xor_net.addLayer(3, 'tanh')\n",
    "Xor_net.addLayer(1, 'sigmoid')\n",
    "costs, weights = Xor_net.fit(x_train, y_train, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20791ac4f40>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of the XOR b/w 1 and 0 is: 1\n"
     ]
    }
   ],
   "source": [
    "test = np.array([[1], [0]])\n",
    "pred = 1 if Xor_net.predict(test) else 0\n",
    "print(\"The result of the XOR b/w 1 and 0 is:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flower Problem (petal length and age --> color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  0 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.5289009890363979\n",
      "-----Epoch:  10 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.4780243920875343\n",
      "-----Epoch:  20 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.46348576502550726\n",
      "-----Epoch:  30 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.4504564519476011\n",
      "-----Epoch:  40 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.43869954206113315\n",
      "-----Epoch:  50 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.42804760205567033\n",
      "-----Epoch:  60 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.4183568012853366\n",
      "-----Epoch:  70 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.4095039264833418\n",
      "-----Epoch:  80 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.40138358478813285\n",
      "-----Epoch:  90 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.393905650251693\n",
      "-----Epoch:  100 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.3869929991702367\n",
      "-----Epoch:  110 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.38057954031748636\n",
      "-----Epoch:  120 -----\n",
      "Accuracy: 0.0\n",
      "Cost: 0.3746085245113549\n",
      "-----Epoch:  130 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.36903110717931603\n",
      "-----Epoch:  140 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.3638051335545932\n",
      "-----Epoch:  150 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.35889411603092425\n",
      "-----Epoch:  160 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.354266375205105\n",
      "-----Epoch:  170 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.34989431911019\n",
      "-----Epoch:  180 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.3457538384167101\n",
      "-----Epoch:  190 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.3418237985806933\n",
      "-----Epoch:  200 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.33808561285655886\n",
      "-----Epoch:  210 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.33452288269138236\n",
      "-----Epoch:  220 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.3311210942587335\n",
      "-----Epoch:  230 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.3278673617929042\n",
      "-----Epoch:  240 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.32475020998126025\n",
      "-----Epoch:  250 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.3217593890026435\n",
      "-----Epoch:  260 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.3188857169021475\n",
      "-----Epoch:  270 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.3161209449032127\n",
      "-----Epoch:  280 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.3134576420087608\n",
      "-----Epoch:  290 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.3108890958615152\n",
      "-----Epoch:  300 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.30840922734301324\n",
      "-----Epoch:  310 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.30601251681053365\n",
      "-----Epoch:  320 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.3036939402173531\n",
      "-----Epoch:  330 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.3014489136476546\n",
      "-----Epoch:  340 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.29927324503390923\n",
      "-----Epoch:  350 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.29716309202053853\n",
      "-----Epoch:  360 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.2951149251003808\n",
      "-----Epoch:  370 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.2931254952858792\n",
      "-----Epoch:  380 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.2911918056897946\n",
      "-----Epoch:  390 -----\n",
      "Accuracy: 0.125\n",
      "Cost: 0.2893110864845905\n",
      "-----Epoch:  400 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2874807727886581\n",
      "-----Epoch:  410 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.28569848509388746\n",
      "-----Epoch:  420 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2839620119049137\n",
      "-----Epoch:  430 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2822692943074515\n",
      "-----Epoch:  440 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.28061841222293304\n",
      "-----Epoch:  450 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2790075721403869\n",
      "-----Epoch:  460 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.27743509614514167\n",
      "-----Epoch:  470 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2758994120883106\n",
      "-----Epoch:  480 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.27439904476181287\n",
      "-----Epoch:  490 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.27293260796146607\n",
      "-----Epoch:  500 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.27149879733591253\n",
      "-----Epoch:  510 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.270096383932225\n",
      "-----Epoch:  520 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2687242083602867\n",
      "-----Epoch:  530 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2673811755077407\n",
      "-----Epoch:  540 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.26606624974568593\n",
      "-----Epoch:  550 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2647784505725486\n",
      "-----Epoch:  560 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.26351684864984976\n",
      "-----Epoch:  570 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.26228056218905427\n",
      "-----Epoch:  580 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2610687536534458\n",
      "-----Epoch:  590 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.25988062674312057\n",
      "-----Epoch:  600 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2587154236348175\n",
      "-----Epoch:  610 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.25757242245147616\n",
      "-----Epoch:  620 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.25645093493919413\n",
      "-----Epoch:  630 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.25535030433169614\n",
      "-----Epoch:  640 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.25426990338457883\n",
      "-----Epoch:  650 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2532091325634845\n",
      "-----Epoch:  660 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2521674183720274\n",
      "-----Epoch:  670 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2511442118067732\n",
      "-----Epoch:  680 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2501389869278771\n",
      "-----Epoch:  690 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.24915123953514376\n",
      "-----Epoch:  700 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.24818048594029984\n",
      "-----Epoch:  710 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.24722626182718466\n",
      "-----Epoch:  720 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.24628812119237492\n",
      "-----Epoch:  730 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.24536563535948772\n",
      "-----Epoch:  740 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.24445839206104997\n",
      "-----Epoch:  750 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.24356599458240222\n",
      "-----Epoch:  760 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.24268806096262197\n",
      "-----Epoch:  770 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.24182422324791572\n",
      "-----Epoch:  780 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.24097412679334368\n",
      "-----Epoch:  790 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.24013742960911594\n",
      "-----Epoch:  800 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.23931380174803552\n",
      "-----Epoch:  810 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.23850292473096313\n",
      "-----Epoch:  820 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2377044910074565\n",
      "-----Epoch:  830 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.23691820344897804\n",
      "-----Epoch:  840 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.23614377487229032\n",
      "-----Epoch:  850 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.23538092759085824\n",
      "-----Epoch:  860 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.23462939299226063\n",
      "-----Epoch:  870 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.23388891113977814\n",
      "-----Epoch:  880 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.23315923039647163\n",
      "-----Epoch:  890 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.23244010707020843\n",
      "-----Epoch:  900 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.23173130507820783\n",
      "-----Epoch:  910 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.23103259562979833\n",
      "-----Epoch:  920 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2303437569261771\n",
      "-----Epoch:  930 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.229664573876056\n",
      "-----Epoch:  940 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.22899483782616603\n",
      "-----Epoch:  950 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.22833434630566787\n",
      "-----Epoch:  960 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2276829027835881\n",
      "-----Epoch:  970 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2270403164384671\n",
      "-----Epoch:  980 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2264064019394624\n",
      "-----Epoch:  990 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2257809792382086\n",
      "-----Epoch:  1000 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2251638733707817\n",
      "-----Epoch:  1010 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.22455491426916688\n",
      "-----Epoch:  1020 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.22395393658166626\n",
      "-----Epoch:  1030 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.22336077950172517\n",
      "-----Epoch:  1040 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2227752866046916\n",
      "-----Epoch:  1050 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.22219730569205384\n",
      "-----Epoch:  1060 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2216266886427366\n",
      "-----Epoch:  1070 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2210632912710585\n",
      "-----Epoch:  1080 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2205069731909849\n",
      "-----Epoch:  1090 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.21995759768633186\n",
      "-----Epoch:  1100 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2194150315865982\n",
      "-----Epoch:  1110 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.2188791451481259\n",
      "-----Epoch:  1120 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.21834981194030664\n",
      "-----Epoch:  1130 -----\n",
      "Accuracy: 0.25\n",
      "Cost: 0.21782690873656982\n",
      "-----Epoch:  1140 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.21731031540990464\n",
      "-----Epoch:  1150 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.21679991483268443\n",
      "-----Epoch:  1160 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.2162955927805729\n",
      "-----Epoch:  1170 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.21579723784031093\n",
      "-----Epoch:  1180 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.21530474132118563\n",
      "-----Epoch:  1190 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.21481799717000705\n",
      "-----Epoch:  1200 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.21433690188941534\n",
      "-----Epoch:  1210 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.21386135445936214\n",
      "-----Epoch:  1220 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.21339125626161304\n",
      "-----Epoch:  1230 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.21292651100712723\n",
      "-----Epoch:  1240 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.21246702466618178\n",
      "-----Epoch:  1250 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.212012705401111\n",
      "-----Epoch:  1260 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.21156346350154262\n",
      "-----Epoch:  1270 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.21111921132201583\n",
      "-----Epoch:  1280 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.2106798632218747\n",
      "-----Epoch:  1290 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.2102453355073345\n",
      "-----Epoch:  1300 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20981554637562524\n",
      "-----Epoch:  1310 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20939041586112161\n",
      "-----Epoch:  1320 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.2089698657833724\n",
      "-----Epoch:  1330 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20855381969694753\n",
      "-----Epoch:  1340 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20814220284302618\n",
      "-----Epoch:  1350 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20773494210265106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  1360 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.2073319659515788\n",
      "-----Epoch:  1370 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.2069332044166622\n",
      "-----Epoch:  1380 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20653858903369807\n",
      "-----Epoch:  1390 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20614805280668358\n",
      "-----Epoch:  1400 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20576153016842139\n",
      "-----Epoch:  1410 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20537895694242217\n",
      "-----Epoch:  1420 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20500027030605028\n",
      "-----Epoch:  1430 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.2046254087548642\n",
      "-----Epoch:  1440 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20425431206810607\n",
      "-----Epoch:  1450 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20388692127529306\n",
      "-----Epoch:  1460 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20352317862387076\n",
      "-----Epoch:  1470 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20316302754788623\n",
      "-----Epoch:  1480 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20280641263764135\n",
      "-----Epoch:  1490 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20245327961029158\n",
      "-----Epoch:  1500 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.2021035752813532\n",
      "-----Epoch:  1510 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20175724753708493\n",
      "-----Epoch:  1520 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.2014142453077129\n",
      "-----Epoch:  1530 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20107451854146713\n",
      "-----Epoch:  1540 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.2007380181794003\n",
      "-----Epoch:  1550 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20040469613096082\n",
      "-----Epoch:  1560 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.20007450525029213\n",
      "-----Epoch:  1570 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19974739931323382\n",
      "-----Epoch:  1580 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19942333299499873\n",
      "-----Epoch:  1590 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19910226184850194\n",
      "-----Epoch:  1600 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19878414228332097\n",
      "-----Epoch:  1610 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19846893154526193\n",
      "-----Epoch:  1620 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19815658769651415\n",
      "-----Epoch:  1630 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19784706959637088\n",
      "-----Epoch:  1640 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19754033688249784\n",
      "-----Epoch:  1650 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.1972363499527311\n",
      "-----Epoch:  1660 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19693506994738535\n",
      "-----Epoch:  1670 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19663645873205762\n",
      "-----Epoch:  1680 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.1963404788809083\n",
      "-----Epoch:  1690 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19604709366040424\n",
      "-----Epoch:  1700 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19575626701350915\n",
      "-----Epoch:  1710 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19546796354430612\n",
      "-----Epoch:  1720 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19518214850303905\n",
      "-----Epoch:  1730 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19489878777155828\n",
      "-----Epoch:  1740 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19461784784915973\n",
      "-----Epoch:  1750 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19433929583880194\n",
      "-----Epoch:  1760 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19406309943369152\n",
      "-----Epoch:  1770 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.193789226904225\n",
      "-----Epoch:  1780 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19351764708527486\n",
      "-----Epoch:  1790 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19324832936380978\n",
      "-----Epoch:  1800 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.1929812436668397\n",
      "-----Epoch:  1810 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19271636044967322\n",
      "-----Epoch:  1820 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19245365068448028\n",
      "-----Epoch:  1830 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.1921930858491499\n",
      "-----Epoch:  1840 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19193463791643395\n",
      "-----Epoch:  1850 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19167827934336712\n",
      "-----Epoch:  1860 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.1914239830609583\n",
      "-----Epoch:  1870 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.1911717224641415\n",
      "-----Epoch:  1880 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19092147140198015\n",
      "-----Epoch:  1890 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19067320416811834\n",
      "-----Epoch:  1900 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19042689549146974\n",
      "-----Epoch:  1910 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.19018252052713877\n",
      "-----Epoch:  1920 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.18994005484756665\n",
      "-----Epoch:  1930 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.1896994744338958\n",
      "-----Epoch:  1940 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.18946075566754642\n",
      "-----Epoch:  1950 -----\n",
      "Accuracy: 0.375\n",
      "Cost: 0.18922387532199947\n",
      "-----Epoch:  1960 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18898881055477923\n",
      "-----Epoch:  1970 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1887555388996312\n",
      "-----Epoch:  1980 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18852403825888833\n",
      "-----Epoch:  1990 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18829428689602135\n",
      "-----Epoch:  2000 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18806626342836746\n",
      "-----Epoch:  2010 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1878399468200327\n",
      "-----Epoch:  2020 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1876153163749631\n",
      "-----Epoch:  2030 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18739235173017982\n",
      "-----Epoch:  2040 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18717103284917377\n",
      "-----Epoch:  2050 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18695134001545644\n",
      "-----Epoch:  2060 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18673325382625944\n",
      "-----Epoch:  2070 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18651675518638333\n",
      "-----Epoch:  2080 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18630182530218825\n",
      "-----Epoch:  2090 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18608844567572339\n",
      "-----Epoch:  2100 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18587659809899218\n",
      "-----Epoch:  2110 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1856662646483488\n",
      "-----Epoch:  2120 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18545742767902382\n",
      "-----Epoch:  2130 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18525006981977285\n",
      "-----Epoch:  2140 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18504417396764922\n",
      "-----Epoch:  2150 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18483972328289197\n",
      "-----Epoch:  2160 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18463670118393177\n",
      "-----Epoch:  2170 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18443509134250702\n",
      "-----Epoch:  2180 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18423487767889096\n",
      "-----Epoch:  2190 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18403604435722326\n",
      "-----Epoch:  2200 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18383857578094737\n",
      "-----Epoch:  2210 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1836424565883474\n",
      "-----Epoch:  2220 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1834476716481842\n",
      "-----Epoch:  2230 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18325420605542686\n",
      "-----Epoch:  2240 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18306204512707855\n",
      "-----Epoch:  2250 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18287117439809264\n",
      "-----Epoch:  2260 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18268157961737796\n",
      "-----Epoch:  2270 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18249324674389092\n",
      "-----Epoch:  2280 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1823061619428118\n",
      "-----Epoch:  2290 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18212031158180345\n",
      "-----Epoch:  2300 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18193568222735035\n",
      "-----Epoch:  2310 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1817522606411756\n",
      "-----Epoch:  2320 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1815700337767349\n",
      "-----Epoch:  2330 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18138898877578463\n",
      "-----Epoch:  2340 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18120911296502268\n",
      "-----Epoch:  2350 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18103039385280006\n",
      "-----Epoch:  2360 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18085281912590176\n",
      "-----Epoch:  2370 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1806763766463953\n",
      "-----Epoch:  2380 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1805010544485447\n",
      "-----Epoch:  2390 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.18032684073578908\n",
      "-----Epoch:  2400 -----\n",
      "Accuracy: 0.5\n",
      "Cost: 0.1801537238777839\n",
      "-----Epoch:  2410 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17998169240750372\n",
      "-----Epoch:  2420 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17981073501840356\n",
      "-----Epoch:  2430 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17964084056164015\n",
      "-----Epoch:  2440 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1794719980433495\n",
      "-----Epoch:  2450 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17930419662197955\n",
      "-----Epoch:  2460 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17913742560567802\n",
      "-----Epoch:  2470 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17897167444973255\n",
      "-----Epoch:  2480 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17880693275406373\n",
      "-----Epoch:  2490 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17864319026076667\n",
      "-----Epoch:  2500 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1784804368517047\n",
      "-----Epoch:  2510 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17831866254614895\n",
      "-----Epoch:  2520 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17815785749846633\n",
      "-----Epoch:  2530 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17799801199585358\n",
      "-----Epoch:  2540 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17783911645611553\n",
      "-----Epoch:  2550 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17768116142548832\n",
      "-----Epoch:  2560 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17752413757650523\n",
      "-----Epoch:  2570 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17736803570590381\n",
      "-----Epoch:  2580 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1772128467325752\n",
      "-----Epoch:  2590 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17705856169555312\n",
      "-----Epoch:  2600 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1769051717520418\n",
      "-----Epoch:  2610 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17675266817548207\n",
      "-----Epoch:  2620 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1766010423536565\n",
      "-----Epoch:  2630 -----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.625\n",
      "Cost: 0.17645028578682878\n",
      "-----Epoch:  2640 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17630039008592077\n",
      "-----Epoch:  2650 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17615134697072407\n",
      "-----Epoch:  2660 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17600314826814534\n",
      "-----Epoch:  2670 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17585578591048623\n",
      "-----Epoch:  2680 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17570925193375433\n",
      "-----Epoch:  2690 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17556353847600842\n",
      "-----Epoch:  2700 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17541863777573322\n",
      "-----Epoch:  2710 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1752745421702453\n",
      "-----Epoch:  2720 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17513124409412975\n",
      "-----Epoch:  2730 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17498873607770551\n",
      "-----Epoch:  2740 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17484701074551856\n",
      "-----Epoch:  2750 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1747060608148658\n",
      "-----Epoch:  2760 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17456587909434343\n",
      "-----Epoch:  2770 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1744264584824241\n",
      "-----Epoch:  2780 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17428779196605995\n",
      "-----Epoch:  2790 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1741498726193112\n",
      "-----Epoch:  2800 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1740126936020004\n",
      "-----Epoch:  2810 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1738762481583903\n",
      "-----Epoch:  2820 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1737405296158876\n",
      "-----Epoch:  2830 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1736055313837689\n",
      "-----Epoch:  2840 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17347124695193072\n",
      "-----Epoch:  2850 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17333766988966104\n",
      "-----Epoch:  2860 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17320479384443463\n",
      "-----Epoch:  2870 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17307261254072884\n",
      "-----Epoch:  2880 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1729411197788614\n",
      "-----Epoch:  2890 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17281030943384856\n",
      "-----Epoch:  2900 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1726801754542838\n",
      "-----Epoch:  2910 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17255071186123713\n",
      "-----Epoch:  2920 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17242191274717278\n",
      "-----Epoch:  2930 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17229377227488754\n",
      "-----Epoch:  2940 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1721662846764661\n",
      "-----Epoch:  2950 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17203944425225695\n",
      "-----Epoch:  2960 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1719132453698641\n",
      "-----Epoch:  2970 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17178768246315854\n",
      "-----Epoch:  2980 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1716627500313047\n",
      "-----Epoch:  2990 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17153844263780701\n",
      "-----Epoch:  3000 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17141475490956945\n",
      "-----Epoch:  3010 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17129168153597407\n",
      "-----Epoch:  3020 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17116921726797474\n",
      "-----Epoch:  3030 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17104735691720582\n",
      "-----Epoch:  3040 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17092609535510717\n",
      "-----Epoch:  3050 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1708054275120636\n",
      "-----Epoch:  3060 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17068534837655933\n",
      "-----Epoch:  3070 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17056585299434648\n",
      "-----Epoch:  3080 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17044693646762815\n",
      "-----Epoch:  3090 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17032859395425526\n",
      "-----Epoch:  3100 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.170210820666937\n",
      "-----Epoch:  3110 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.17009361187246447\n",
      "-----Epoch:  3120 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1699769628909477\n",
      "-----Epoch:  3130 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16986086909506487\n",
      "-----Epoch:  3140 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1697453259093248\n",
      "-----Epoch:  3150 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16963032880934206\n",
      "-----Epoch:  3160 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1695158733211223\n",
      "-----Epoch:  3170 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16940195502036196\n",
      "-----Epoch:  3180 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1692885695317582\n",
      "-----Epoch:  3190 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16917571252832986\n",
      "-----Epoch:  3200 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1690633797307511\n",
      "-----Epoch:  3210 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16895156690669413\n",
      "-----Epoch:  3220 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16884026987018486\n",
      "-----Epoch:  3230 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16872948448096703\n",
      "-----Epoch:  3240 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16861920664387808\n",
      "-----Epoch:  3250 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16850943230823476\n",
      "-----Epoch:  3260 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16840015746722858\n",
      "-----Epoch:  3270 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16829137815733147\n",
      "-----Epoch:  3280 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16818309045771057\n",
      "-----Epoch:  3290 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16807529048965258\n",
      "-----Epoch:  3300 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1679679744159978\n",
      "-----Epoch:  3310 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1678611384405827\n",
      "-----Epoch:  3320 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1677547788076913\n",
      "-----Epoch:  3330 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1676488918015164\n",
      "-----Epoch:  3340 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16754347374562775\n",
      "-----Epoch:  3350 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16743852100245027\n",
      "-----Epoch:  3360 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16733402997274896\n",
      "-----Epoch:  3370 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16722999709512382\n",
      "-----Epoch:  3380 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1671264188455104\n",
      "-----Epoch:  3390 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16702329173669095\n",
      "-----Epoch:  3400 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16692061231781033\n",
      "-----Epoch:  3410 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16681837717390213\n",
      "-----Epoch:  3420 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1667165829254198\n",
      "-----Epoch:  3430 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16661522622777775\n",
      "-----Epoch:  3440 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16651430377089646\n",
      "-----Epoch:  3450 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16641381227875768\n",
      "-----Epoch:  3460 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.166313748508964\n",
      "-----Epoch:  3470 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16621410925230704\n",
      "-----Epoch:  3480 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16611489133234117\n",
      "-----Epoch:  3490 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16601609160496378\n",
      "-----Epoch:  3500 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16591770695800376\n",
      "-----Epoch:  3510 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16581973431081257\n",
      "-----Epoch:  3520 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16572217061386585\n",
      "-----Epoch:  3530 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16562501284836767\n",
      "-----Epoch:  3540 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16552825802586302\n",
      "-----Epoch:  3550 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16543190318785456\n",
      "-----Epoch:  3560 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16533594540542623\n",
      "-----Epoch:  3570 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16524038177887188\n",
      "-----Epoch:  3580 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16514520943733024\n",
      "-----Epoch:  3590 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16505042553842394\n",
      "-----Epoch:  3600 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16495602726790592\n",
      "-----Epoch:  3610 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16486201183930888\n",
      "-----Epoch:  3620 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16476837649360224\n",
      "-----Epoch:  3630 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16467511849885183\n",
      "-----Epoch:  3640 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16458223514988657\n",
      "-----Epoch:  3650 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1644897237679692\n",
      "-----Epoch:  3660 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16439758170047158\n",
      "-----Epoch:  3670 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16430580632055555\n",
      "-----Epoch:  3680 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16421439502685797\n",
      "-----Epoch:  3690 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16412334524317973\n",
      "-----Epoch:  3700 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16403265441818118\n",
      "-----Epoch:  3710 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16394232002508008\n",
      "-----Epoch:  3720 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16385233956135484\n",
      "-----Epoch:  3730 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16376271054845137\n",
      "-----Epoch:  3740 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1636734305314956\n",
      "-----Epoch:  3750 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16358449707900868\n",
      "-----Epoch:  3760 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1634959077826269\n",
      "-----Epoch:  3770 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16340766025682518\n",
      "-----Epoch:  3780 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16331975213864544\n",
      "-----Epoch:  3790 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16323218108742826\n",
      "-----Epoch:  3800 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16314494478454764\n",
      "-----Epoch:  3810 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16305804093315102\n",
      "-----Epoch:  3820 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16297146725790237\n",
      "-----Epoch:  3830 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16288522150472756\n",
      "-----Epoch:  3840 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16279930144056642\n",
      "-----Epoch:  3850 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16271370485312475\n",
      "-----Epoch:  3860 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16262842955063206\n",
      "-----Epoch:  3870 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16254347336160246\n",
      "-----Epoch:  3880 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1624588341345983\n",
      "-----Epoch:  3890 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16237450973799725\n",
      "-----Epoch:  3900 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1622904980597628\n",
      "-----Epoch:  3910 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1622067970072179\n",
      "-----Epoch:  3920 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16212340450682197\n",
      "-----Epoch:  3930 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16204031850395012\n",
      "-----Epoch:  3940 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1619575369626767\n",
      "-----Epoch:  3950 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16187505786556097\n",
      "-----Epoch:  3960 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16179287921343585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  3970 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16171099902519934\n",
      "-----Epoch:  3980 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1616294153376096\n",
      "-----Epoch:  3990 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.161548126205082\n",
      "-----Epoch:  4000 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16146712969948956\n",
      "-----Epoch:  4010 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16138642390996497\n",
      "-----Epoch:  4020 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16130600694270714\n",
      "-----Epoch:  4030 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16122587692078869\n",
      "-----Epoch:  4040 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16114603198396668\n",
      "-----Epoch:  4050 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16106647028849622\n",
      "-----Epoch:  4060 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16098719000694606\n",
      "-----Epoch:  4070 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16090818932801654\n",
      "-----Epoch:  4080 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1608294664563606\n",
      "-----Epoch:  4090 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1607510196124071\n",
      "-----Epoch:  4100 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16067284703218546\n",
      "-----Epoch:  4110 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1605949469671542\n",
      "-----Epoch:  4120 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16051731768403046\n",
      "-----Epoch:  4130 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16043995746462256\n",
      "-----Epoch:  4140 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16036286460566432\n",
      "-----Epoch:  4150 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16028603741865152\n",
      "-----Epoch:  4160 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16020947422968146\n",
      "-----Epoch:  4170 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1601331733792927\n",
      "-----Epoch:  4180 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.16005713322230916\n",
      "-----Epoch:  4190 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15998135212768497\n",
      "-----Epoch:  4200 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15990582847835083\n",
      "-----Epoch:  4210 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15983056067106388\n",
      "-----Epoch:  4220 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15975554711625828\n",
      "-----Epoch:  4230 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15968078623789836\n",
      "-----Epoch:  4240 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15960627647333314\n",
      "-----Epoch:  4250 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15953201627315378\n",
      "-----Epoch:  4260 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15945800410105104\n",
      "-----Epoch:  4270 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15938423843367666\n",
      "-----Epoch:  4280 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1593107177605051\n",
      "-----Epoch:  4290 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15923744058369738\n",
      "-----Epoch:  4300 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1591644054179669\n",
      "-----Epoch:  4310 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1590916107904465\n",
      "-----Epoch:  4320 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15901905524055812\n",
      "-----Epoch:  4330 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15894673731988307\n",
      "-----Epoch:  4340 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15887465559203418\n",
      "-----Epoch:  4350 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15880280863253032\n",
      "-----Epoch:  4360 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15873119502867147\n",
      "-----Epoch:  4370 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15865981337941623\n",
      "-----Epoch:  4380 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15858866229526003\n",
      "-----Epoch:  4390 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1585177403981154\n",
      "-----Epoch:  4400 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.158447046321194\n",
      "-----Epoch:  4410 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1583765787088894\n",
      "-----Epoch:  4420 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1583063362166617\n",
      "-----Epoch:  4430 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15823631751092374\n",
      "-----Epoch:  4440 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15816652126892827\n",
      "-----Epoch:  4450 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.158096946178657\n",
      "-----Epoch:  4460 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1580275909387105\n",
      "-----Epoch:  4470 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1579584542582005\n",
      "-----Epoch:  4480 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15788953485664142\n",
      "-----Epoch:  4490 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15782083146384598\n",
      "-----Epoch:  4500 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15775234281981976\n",
      "-----Epoch:  4510 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15768406767465856\n",
      "-----Epoch:  4520 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15761600478844548\n",
      "-----Epoch:  4530 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15754815293115174\n",
      "-----Epoch:  4540 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15748051088253529\n",
      "-----Epoch:  4550 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15741307743204436\n",
      "-----Epoch:  4560 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15734585137871854\n",
      "-----Epoch:  4570 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15727883153109457\n",
      "-----Epoch:  4580 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15721201670710974\n",
      "-----Epoch:  4590 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15714540573401004\n",
      "-----Epoch:  4600 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15707899744825626\n",
      "-----Epoch:  4610 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15701279069543306\n",
      "-----Epoch:  4620 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15694678433015863\n",
      "-----Epoch:  4630 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15688097721599556\n",
      "-----Epoch:  4640 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15681536822536213\n",
      "-----Epoch:  4650 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15674995623944615\n",
      "-----Epoch:  4660 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15668474014811734\n",
      "-----Epoch:  4670 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15661971884984363\n",
      "-----Epoch:  4680 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15655489125160651\n",
      "-----Epoch:  4690 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15649025626881774\n",
      "-----Epoch:  4700 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15642581282523726\n",
      "-----Epoch:  4710 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15636155985289238\n",
      "-----Epoch:  4720 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15629749629199718\n",
      "-----Epoch:  4730 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15623362109087338\n",
      "-----Epoch:  4740 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1561699332058722\n",
      "-----Epoch:  4750 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.156106431601296\n",
      "-----Epoch:  4760 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15604311524932332\n",
      "-----Epoch:  4770 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15597998312993147\n",
      "-----Epoch:  4780 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15591703423082298\n",
      "-----Epoch:  4790 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15585426754735132\n",
      "-----Epoch:  4800 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15579168208244762\n",
      "-----Epoch:  4810 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15572927684654853\n",
      "-----Epoch:  4820 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15566705085752522\n",
      "-----Epoch:  4830 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15560500314061176\n",
      "-----Epoch:  4840 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15554313272833706\n",
      "-----Epoch:  4850 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1554814386604536\n",
      "-----Epoch:  4860 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15541991998387172\n",
      "-----Epoch:  4870 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15535857575259027\n",
      "-----Epoch:  4880 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15529740502763073\n",
      "-----Epoch:  4890 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1552364068769713\n",
      "-----Epoch:  4900 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15517558037548185\n",
      "-----Epoch:  4910 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15511492460485896\n",
      "-----Epoch:  4920 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15505443865356308\n",
      "-----Epoch:  4930 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1549941216167546\n",
      "-----Epoch:  4940 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15493397259623243\n",
      "-----Epoch:  4950 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15487399070037156\n",
      "-----Epoch:  4960 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15481417504406328\n",
      "-----Epoch:  4970 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15475452474865367\n",
      "-----Epoch:  4980 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15469503894188527\n",
      "-----Epoch:  4990 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15463571675783722\n",
      "-----Epoch:  5000 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15457655733686793\n",
      "-----Epoch:  5010 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15451755982555682\n",
      "-----Epoch:  5020 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15445872337664737\n",
      "-----Epoch:  5030 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15440004714899136\n",
      "-----Epoch:  5040 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1543415303074928\n",
      "-----Epoch:  5050 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15428317202305292\n",
      "-----Epoch:  5060 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1542249714725155\n",
      "-----Epoch:  5070 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15416692783861327\n",
      "-----Epoch:  5080 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15410904030991468\n",
      "-----Epoch:  5090 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15405130808077064\n",
      "-----Epoch:  5100 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.153993730351263\n",
      "-----Epoch:  5110 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15393630632715272\n",
      "-----Epoch:  5120 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1538790352198288\n",
      "-----Epoch:  5130 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.153821916246258\n",
      "-----Epoch:  5140 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15376494862893464\n",
      "-----Epoch:  5150 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1537081315958314\n",
      "-----Epoch:  5160 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1536514643803507\n",
      "-----Epoch:  5170 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15359494622127584\n",
      "-----Epoch:  5180 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15353857636272394\n",
      "-----Epoch:  5190 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15348235405409769\n",
      "-----Epoch:  5200 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15342627855003946\n",
      "-----Epoch:  5210 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15337034911038472\n",
      "-----Epoch:  5220 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15331456500011553\n",
      "-----Epoch:  5230 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15325892548931677\n",
      "-----Epoch:  5240 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15320342985312985\n",
      "-----Epoch:  5250 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15314807737170902\n",
      "-----Epoch:  5260 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1530928673301776\n",
      "-----Epoch:  5270 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15303779901858428\n",
      "-----Epoch:  5280 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15298287173186015\n",
      "-----Epoch:  5290 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1529280847697763\n",
      "-----Epoch:  5300 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15287343743690196\n",
      "-----Epoch:  5310 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15281892904256225\n",
      "-----Epoch:  5320 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15276455890079804\n",
      "-----Epoch:  5330 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15271032633032391\n",
      "-----Epoch:  5340 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15265623065448888\n",
      "-----Epoch:  5350 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1526022712012361\n",
      "-----Epoch:  5360 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15254844730306297\n",
      "-----Epoch:  5370 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1524947582969831\n",
      "-----Epoch:  5380 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15244120352448612\n",
      "-----Epoch:  5390 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15238778233150088\n",
      "-----Epoch:  5400 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15233449406835686\n",
      "-----Epoch:  5410 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1522813380897462\n",
      "-----Epoch:  5420 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15222831375468793\n",
      "-----Epoch:  5430 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15217542042648977\n",
      "-----Epoch:  5440 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15212265747271272\n",
      "-----Epoch:  5450 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1520700242651348\n",
      "-----Epoch:  5460 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15201752017971537\n",
      "-----Epoch:  5470 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15196514459655983\n",
      "-----Epoch:  5480 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15191289689988496\n",
      "-----Epoch:  5490 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15186077647798413\n",
      "-----Epoch:  5500 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15180878272319354\n",
      "-----Epoch:  5510 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15175691503185765\n",
      "-----Epoch:  5520 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1517051728042963\n",
      "-----Epoch:  5530 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15165355544477152\n",
      "-----Epoch:  5540 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15160206236145402\n",
      "-----Epoch:  5550 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15155069296639156\n",
      "-----Epoch:  5560 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1514994466754764\n",
      "-----Epoch:  5570 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1514483229084132\n",
      "-----Epoch:  5580 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15139732108868792\n",
      "-----Epoch:  5590 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1513464406435366\n",
      "-----Epoch:  5600 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1512956810039138\n",
      "-----Epoch:  5610 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15124504160446295\n",
      "-----Epoch:  5620 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15119452188348528\n",
      "-----Epoch:  5630 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15114412128291\n",
      "-----Epoch:  5640 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15109383924826503\n",
      "-----Epoch:  5650 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1510436752286473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  5660 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1509936286766933\n",
      "-----Epoch:  5670 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15094369904855068\n",
      "-----Epoch:  5680 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1508938858038497\n",
      "-----Epoch:  5690 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15084418840567487\n",
      "-----Epoch:  5700 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15079460632053676\n",
      "-----Epoch:  5710 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1507451390183446\n",
      "-----Epoch:  5720 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15069578597237882\n",
      "-----Epoch:  5730 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15064654665926336\n",
      "-----Epoch:  5740 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15059742055893952\n",
      "-----Epoch:  5750 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15054840715463902\n",
      "-----Epoch:  5760 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15049950593285705\n",
      "-----Epoch:  5770 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15045071638332727\n",
      "-----Epoch:  5780 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15040203799899476\n",
      "-----Epoch:  5790 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15035347027599108\n",
      "-----Epoch:  5800 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15030501271360858\n",
      "-----Epoch:  5810 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1502566648142757\n",
      "-----Epoch:  5820 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15020842608353147\n",
      "-----Epoch:  5830 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15016029603000114\n",
      "-----Epoch:  5840 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15011227416537193\n",
      "-----Epoch:  5850 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15006436000436846\n",
      "-----Epoch:  5860 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.15001655306472897\n",
      "-----Epoch:  5870 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14996885286718167\n",
      "-----Epoch:  5880 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1499212589354213\n",
      "-----Epoch:  5890 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14987377079608555\n",
      "-----Epoch:  5900 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14982638797873188\n",
      "-----Epoch:  5910 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14977911001581531\n",
      "-----Epoch:  5920 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14973193644266555\n",
      "-----Epoch:  5930 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14968486679746387\n",
      "-----Epoch:  5940 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14963790062122206\n",
      "-----Epoch:  5950 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14959103745775945\n",
      "-----Epoch:  5960 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1495442768536817\n",
      "-----Epoch:  5970 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14949761835835923\n",
      "-----Epoch:  5980 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14945106152390514\n",
      "-----Epoch:  5990 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14940460590515478\n",
      "-----Epoch:  6000 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14935825105964487\n",
      "-----Epoch:  6010 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1493119965475919\n",
      "-----Epoch:  6020 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14926584193187206\n",
      "-----Epoch:  6030 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14921978677800068\n",
      "-----Epoch:  6040 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14917383065411244\n",
      "-----Epoch:  6050 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14912797313094078\n",
      "-----Epoch:  6060 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14908221378179806\n",
      "-----Epoch:  6070 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14903655218255638\n",
      "-----Epoch:  6080 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14899098791162776\n",
      "-----Epoch:  6090 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14894552054994498\n",
      "-----Epoch:  6100 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14890014968094212\n",
      "-----Epoch:  6110 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1488548748905358\n",
      "-----Epoch:  6120 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14880969576710742\n",
      "-----Epoch:  6130 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14876461190148219\n",
      "-----Epoch:  6140 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14871962288691307\n",
      "-----Epoch:  6150 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14867472831906128\n",
      "-----Epoch:  6160 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14862992779597842\n",
      "-----Epoch:  6170 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14858522091808882\n",
      "-----Epoch:  6180 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14854060728817098\n",
      "-----Epoch:  6190 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14849608651134102\n",
      "-----Epoch:  6200 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14845165819503478\n",
      "-----Epoch:  6210 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14840732194899\n",
      "-----Epoch:  6220 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14836307738523\n",
      "-----Epoch:  6230 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14831892411804595\n",
      "-----Epoch:  6240 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14827486176398083\n",
      "-----Epoch:  6250 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14823088994181208\n",
      "-----Epoch:  6260 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1481870082725357\n",
      "-----Epoch:  6270 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14814321637934894\n",
      "-----Epoch:  6280 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.148099513887635\n",
      "-----Epoch:  6290 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1480559004249463\n",
      "-----Epoch:  6300 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14801237562098887\n",
      "-----Epoch:  6310 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14796893910760675\n",
      "-----Epoch:  6320 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1479255905187655\n",
      "-----Epoch:  6330 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14788232949053737\n",
      "-----Epoch:  6340 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14783915566108602\n",
      "-----Epoch:  6350 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14779606867065062\n",
      "-----Epoch:  6360 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1477530681615313\n",
      "-----Epoch:  6370 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1477101537780736\n",
      "-----Epoch:  6380 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1476673251666544\n",
      "-----Epoch:  6390 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14762458197566639\n",
      "-----Epoch:  6400 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14758192385550423\n",
      "-----Epoch:  6410 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14753935045854924\n",
      "-----Epoch:  6420 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14749686143915572\n",
      "-----Epoch:  6430 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14745445645363656\n",
      "-----Epoch:  6440 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14741213516024887\n",
      "-----Epoch:  6450 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14736989721918042\n",
      "-----Epoch:  6460 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14732774229253556\n",
      "-----Epoch:  6470 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14728567004432178\n",
      "-----Epoch:  6480 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14724368014043535\n",
      "-----Epoch:  6490 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14720177224864858\n",
      "-----Epoch:  6500 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14715994603859636\n",
      "-----Epoch:  6510 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14711820118176255\n",
      "-----Epoch:  6520 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14707653735146714\n",
      "-----Epoch:  6530 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14703495422285282\n",
      "-----Epoch:  6540 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14699345147287254\n",
      "-----Epoch:  6550 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14695202878027655\n",
      "-----Epoch:  6560 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1469106858255996\n",
      "-----Epoch:  6570 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14686942229114824\n",
      "-----Epoch:  6580 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1468282378609886\n",
      "-----Epoch:  6590 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14678713222093373\n",
      "-----Epoch:  6600 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14674610505853197\n",
      "-----Epoch:  6610 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14670515606305365\n",
      "-----Epoch:  6620 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14666428492548014\n",
      "-----Epoch:  6630 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14662349133849115\n",
      "-----Epoch:  6640 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14658277499645317\n",
      "-----Epoch:  6650 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14654213559540785\n",
      "-----Epoch:  6660 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14650157283306\n",
      "-----Epoch:  6670 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14646108640876634\n",
      "-----Epoch:  6680 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1464206760235235\n",
      "-----Epoch:  6690 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14638034137995726\n",
      "-----Epoch:  6700 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1463400821823115\n",
      "-----Epoch:  6710 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14629989813643562\n",
      "-----Epoch:  6720 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14625978894977512\n",
      "-----Epoch:  6730 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14621975433135947\n",
      "-----Epoch:  6740 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1461797939917919\n",
      "-----Epoch:  6750 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1461399076432379\n",
      "-----Epoch:  6760 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1461000949994155\n",
      "-----Epoch:  6770 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1460603557755835\n",
      "-----Epoch:  6780 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14602068968853144\n",
      "-----Epoch:  6790 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14598109645656962\n",
      "-----Epoch:  6800 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1459415757995177\n",
      "-----Epoch:  6810 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14590212743869535\n",
      "-----Epoch:  6820 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14586275109691169\n",
      "-----Epoch:  6830 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1458234464984547\n",
      "-----Epoch:  6840 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14578421336908237\n",
      "-----Epoch:  6850 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1457450514360111\n",
      "-----Epoch:  6860 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14570596042790768\n",
      "-----Epoch:  6870 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14566694007487782\n",
      "-----Epoch:  6880 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1456279901084575\n",
      "-----Epoch:  6890 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14558911026160323\n",
      "-----Epoch:  6900 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14555030026868201\n",
      "-----Epoch:  6910 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14551155986546188\n",
      "-----Epoch:  6920 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14547288878910353\n",
      "-----Epoch:  6930 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14543428677814949\n",
      "-----Epoch:  6940 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14539575357251597\n",
      "-----Epoch:  6950 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1453572889134833\n",
      "-----Epoch:  6960 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14531889254368693\n",
      "-----Epoch:  6970 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14528056420710814\n",
      "-----Epoch:  6980 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14524230364906554\n",
      "-----Epoch:  6990 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14520411061620586\n",
      "-----Epoch:  7000 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1451659848564952\n",
      "-----Epoch:  7010 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14512792611921022\n",
      "-----Epoch:  7020 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14508993415493002\n",
      "-----Epoch:  7030 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14505200871552645\n",
      "-----Epoch:  7040 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14501414955415667\n",
      "-----Epoch:  7050 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14497635642525428\n",
      "-----Epoch:  7060 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14493862908452068\n",
      "-----Epoch:  7070 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.144900967288917\n",
      "-----Epoch:  7080 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1448633707966561\n",
      "-----Epoch:  7090 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14482583936719365\n",
      "-----Epoch:  7100 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14478837276122059\n",
      "-----Epoch:  7110 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14475097074065515\n",
      "-----Epoch:  7120 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1447136330686342\n",
      "-----Epoch:  7130 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14467635950950605\n",
      "-----Epoch:  7140 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14463914982882206\n",
      "-----Epoch:  7150 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1446020037933291\n",
      "-----Epoch:  7160 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14456492117096162\n",
      "-----Epoch:  7170 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14452790173083405\n",
      "-----Epoch:  7180 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14449094524323342\n",
      "-----Epoch:  7190 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14445405147961085\n",
      "-----Epoch:  7200 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14441722021257558\n",
      "-----Epoch:  7210 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14438045121588608\n",
      "-----Epoch:  7220 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14434374426444357\n",
      "-----Epoch:  7230 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14430709913428405\n",
      "-----Epoch:  7240 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1442705156025714\n",
      "-----Epoch:  7250 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1442339934475903\n",
      "-----Epoch:  7260 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14419753244873842\n",
      "-----Epoch:  7270 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14416113238651995\n",
      "-----Epoch:  7280 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14412479304253853\n",
      "-----Epoch:  7290 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14408851419948962\n",
      "-----Epoch:  7300 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1440522956411541\n",
      "-----Epoch:  7310 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14401613715239156\n",
      "-----Epoch:  7320 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14398003851913244\n",
      "-----Epoch:  7330 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14394399952837245\n",
      "-----Epoch:  7340 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14390801996816546\n",
      "-----Epoch:  7350 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14387209962761607\n",
      "-----Epoch:  7360 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14383623829687417\n",
      "-----Epoch:  7370 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14380043576712734\n",
      "-----Epoch:  7380 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14376469183059487\n",
      "-----Epoch:  7390 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14372900628052115\n",
      "-----Epoch:  7400 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14369337891116943\n",
      "-----Epoch:  7410 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14365780951781498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  7420 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14362229789673883\n",
      "-----Epoch:  7430 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14358684384522227\n",
      "-----Epoch:  7440 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1435514471615391\n",
      "-----Epoch:  7450 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14351610764495146\n",
      "-----Epoch:  7460 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14348082509570148\n",
      "-----Epoch:  7470 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14344559931500675\n",
      "-----Epoch:  7480 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1434104301050538\n",
      "-----Epoch:  7490 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14337531726899205\n",
      "-----Epoch:  7500 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1433402606109275\n",
      "-----Epoch:  7510 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14330525993591775\n",
      "-----Epoch:  7520 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14327031504996496\n",
      "-----Epoch:  7530 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.143235425760011\n",
      "-----Epoch:  7540 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14320059187393094\n",
      "-----Epoch:  7550 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14316581320052787\n",
      "-----Epoch:  7560 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1431310895495269\n",
      "-----Epoch:  7570 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1430964207315694\n",
      "-----Epoch:  7580 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14306180655820758\n",
      "-----Epoch:  7590 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14302724684189924\n",
      "-----Epoch:  7600 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1429927413960012\n",
      "-----Epoch:  7610 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14295829003476498\n",
      "-----Epoch:  7620 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14292389257333063\n",
      "-----Epoch:  7630 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14288954882772165\n",
      "-----Epoch:  7640 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14285525861483928\n",
      "-----Epoch:  7650 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14282102175245767\n",
      "-----Epoch:  7660 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14278683805921785\n",
      "-----Epoch:  7670 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14275270735462353\n",
      "-----Epoch:  7680 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14271862945903468\n",
      "-----Epoch:  7690 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14268460419366347\n",
      "-----Epoch:  7700 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14265063138056802\n",
      "-----Epoch:  7710 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14261671084264857\n",
      "-----Epoch:  7720 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14258284240364114\n",
      "-----Epoch:  7730 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14254902588811333\n",
      "-----Epoch:  7740 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14251526112145926\n",
      "-----Epoch:  7750 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14248154792989418\n",
      "-----Epoch:  7760 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14244788614044995\n",
      "-----Epoch:  7770 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14241427558096983\n",
      "-----Epoch:  7780 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14238071608010397\n",
      "-----Epoch:  7790 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14234720746730484\n",
      "-----Epoch:  7800 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14231374957282134\n",
      "-----Epoch:  7810 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14228034222769503\n",
      "-----Epoch:  7820 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14224698526375562\n",
      "-----Epoch:  7830 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14221367851361516\n",
      "-----Epoch:  7840 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1421804218106646\n",
      "-----Epoch:  7850 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14214721498906843\n",
      "-----Epoch:  7860 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14211405788376036\n",
      "-----Epoch:  7870 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14208095033043888\n",
      "-----Epoch:  7880 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1420478921655624\n",
      "-----Epoch:  7890 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1420148832263453\n",
      "-----Epoch:  7900 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14198192335075271\n",
      "-----Epoch:  7910 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14194901237749719\n",
      "-----Epoch:  7920 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14191615014603318\n",
      "-----Epoch:  7930 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14188333649655357\n",
      "-----Epoch:  7940 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14185057126998477\n",
      "-----Epoch:  7950 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14181785430798274\n",
      "-----Epoch:  7960 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1417851854529288\n",
      "-----Epoch:  7970 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14175256454792495\n",
      "-----Epoch:  7980 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14171999143679\n",
      "-----Epoch:  7990 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14168746596405576\n",
      "-----Epoch:  8000 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14165498797496198\n",
      "-----Epoch:  8010 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.141622557315453\n",
      "-----Epoch:  8020 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1415901738321735\n",
      "-----Epoch:  8030 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14155783737246438\n",
      "-----Epoch:  8040 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14152554778435852\n",
      "-----Epoch:  8050 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14149330491657727\n",
      "-----Epoch:  8060 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.141461108618526\n",
      "-----Epoch:  8070 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14142895874029052\n",
      "-----Epoch:  8080 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1413968551326327\n",
      "-----Epoch:  8090 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14136479764698737\n",
      "-----Epoch:  8100 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14133278613545766\n",
      "-----Epoch:  8110 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14130082045081158\n",
      "-----Epoch:  8120 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14126890044647814\n",
      "-----Epoch:  8130 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14123702597654347\n",
      "-----Epoch:  8140 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14120519689574748\n",
      "-----Epoch:  8150 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1411734130594791\n",
      "-----Epoch:  8160 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14114167432377417\n",
      "-----Epoch:  8170 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14110998054531027\n",
      "-----Epoch:  8180 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1410783315814038\n",
      "-----Epoch:  8190 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14104672729000647\n",
      "-----Epoch:  8200 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1410151675297011\n",
      "-----Epoch:  8210 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14098365215969874\n",
      "-----Epoch:  8220 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14095218103983462\n",
      "-----Epoch:  8230 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14092075403056484\n",
      "-----Epoch:  8240 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1408893709929627\n",
      "-----Epoch:  8250 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14085803178871545\n",
      "-----Epoch:  8260 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14082673628012068\n",
      "-----Epoch:  8270 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14079548433008315\n",
      "-----Epoch:  8280 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14076427580211084\n",
      "-----Epoch:  8290 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1407331105603121\n",
      "-----Epoch:  8300 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14070198846939178\n",
      "-----Epoch:  8310 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14067090939464882\n",
      "-----Epoch:  8320 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14063987320197147\n",
      "-----Epoch:  8330 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14060887975783562\n",
      "-----Epoch:  8340 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14057792892930002\n",
      "-----Epoch:  8350 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14054702058400456\n",
      "-----Epoch:  8360 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1405161545901654\n",
      "-----Epoch:  8370 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14048533081657308\n",
      "-----Epoch:  8380 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14045454913258892\n",
      "-----Epoch:  8390 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14042380940814148\n",
      "-----Epoch:  8400 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1403931115137238\n",
      "-----Epoch:  8410 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14036245532039046\n",
      "-----Epoch:  8420 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1403318406997539\n",
      "-----Epoch:  8430 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14030126752398198\n",
      "-----Epoch:  8440 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1402707356657944\n",
      "-----Epoch:  8450 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1402402449984598\n",
      "-----Epoch:  8460 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14020979539579317\n",
      "-----Epoch:  8470 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1401793867321522\n",
      "-----Epoch:  8480 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14014901888243456\n",
      "-----Epoch:  8490 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1401186917220753\n",
      "-----Epoch:  8500 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14008840512704343\n",
      "-----Epoch:  8510 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1400581589738389\n",
      "-----Epoch:  8520 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.14002795313949057\n",
      "-----Epoch:  8530 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1399977875015523\n",
      "-----Epoch:  8540 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1399676619381004\n",
      "-----Epoch:  8550 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1399375763277313\n",
      "-----Epoch:  8560 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13990753054955835\n",
      "-----Epoch:  8570 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13987752448320853\n",
      "-----Epoch:  8580 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13984755800882048\n",
      "-----Epoch:  8590 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13981763100704145\n",
      "-----Epoch:  8600 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13978774335902439\n",
      "-----Epoch:  8610 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13975789494642518\n",
      "-----Epoch:  8620 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13972808565140024\n",
      "-----Epoch:  8630 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13969831535660385\n",
      "-----Epoch:  8640 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13966858394518455\n",
      "-----Epoch:  8650 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13963889130078408\n",
      "-----Epoch:  8660 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13960923730753325\n",
      "-----Epoch:  8670 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1395796218500502\n",
      "-----Epoch:  8680 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13955004481343733\n",
      "-----Epoch:  8690 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13952050608327923\n",
      "-----Epoch:  8700 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1394910055456394\n",
      "-----Epoch:  8710 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13946154308705816\n",
      "-----Epoch:  8720 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13943211859455015\n",
      "-----Epoch:  8730 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13940273195560135\n",
      "-----Epoch:  8740 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13937338305816746\n",
      "-----Epoch:  8750 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13934407179067007\n",
      "-----Epoch:  8760 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1393147980419955\n",
      "-----Epoch:  8770 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13928556170149184\n",
      "-----Epoch:  8780 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13925636265896585\n",
      "-----Epoch:  8790 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1392272008046816\n",
      "-----Epoch:  8800 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1391980760293576\n",
      "-----Epoch:  8810 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13916898822416413\n",
      "-----Epoch:  8820 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13913993728072152\n",
      "-----Epoch:  8830 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13911092309109674\n",
      "-----Epoch:  8840 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13908194554780218\n",
      "-----Epoch:  8850 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13905300454379282\n",
      "-----Epoch:  8860 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13902409997246368\n",
      "-----Epoch:  8870 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13899523172764774\n",
      "-----Epoch:  8880 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13896639970361382\n",
      "-----Epoch:  8890 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13893760379506412\n",
      "-----Epoch:  8900 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13890884389713173\n",
      "-----Epoch:  8910 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1388801199053788\n",
      "-----Epoch:  8920 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13885143171579395\n",
      "-----Epoch:  8930 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1388227792247905\n",
      "-----Epoch:  8940 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1387941623292036\n",
      "-----Epoch:  8950 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13876558092628896\n",
      "-----Epoch:  8960 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13873703491371925\n",
      "-----Epoch:  8970 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13870852418958393\n",
      "-----Epoch:  8980 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13868004865238492\n",
      "-----Epoch:  8990 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13865160820103622\n",
      "-----Epoch:  9000 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13862320273486078\n",
      "-----Epoch:  9010 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1385948321535888\n",
      "-----Epoch:  9020 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13856649635735532\n",
      "-----Epoch:  9030 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13853819524669872\n",
      "-----Epoch:  9040 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13850992872255763\n",
      "-----Epoch:  9050 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13848169668627042\n",
      "-----Epoch:  9060 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13845349903957127\n",
      "-----Epoch:  9070 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13842533568459\n",
      "-----Epoch:  9080 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13839720652384824\n",
      "-----Epoch:  9090 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1383691114602591\n",
      "-----Epoch:  9100 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13834105039712397\n",
      "-----Epoch:  9110 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13831302323813122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  9120 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13828502988735367\n",
      "-----Epoch:  9130 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13825707024924758\n",
      "-----Epoch:  9140 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13822914422864915\n",
      "-----Epoch:  9150 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13820125173077416\n",
      "-----Epoch:  9160 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1381733926612154\n",
      "-----Epoch:  9170 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1381455669259402\n",
      "-----Epoch:  9180 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13811777443128967\n",
      "-----Epoch:  9190 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13809001508397598\n",
      "-----Epoch:  9200 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13806228879108068\n",
      "-----Epoch:  9210 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13803459546005278\n",
      "-----Epoch:  9220 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1380069349987073\n",
      "-----Epoch:  9230 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1379793073152229\n",
      "-----Epoch:  9240 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13795171231814046\n",
      "-----Epoch:  9250 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13792414991636068\n",
      "-----Epoch:  9260 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13789662001914313\n",
      "-----Epoch:  9270 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13786912253610367\n",
      "-----Epoch:  9280 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13784165737721313\n",
      "-----Epoch:  9290 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1378142244527952\n",
      "-----Epoch:  9300 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13778682367352513\n",
      "-----Epoch:  9310 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1377594549504276\n",
      "-----Epoch:  9320 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13773211819487485\n",
      "-----Epoch:  9330 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1377048133185856\n",
      "-----Epoch:  9340 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13767754023362233\n",
      "-----Epoch:  9350 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13765029885239044\n",
      "-----Epoch:  9360 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13762308908763657\n",
      "-----Epoch:  9370 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13759591085244607\n",
      "-----Epoch:  9380 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13756876406024188\n",
      "-----Epoch:  9390 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13754164862478313\n",
      "-----Epoch:  9400 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13751456446016283\n",
      "-----Epoch:  9410 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1374875114808068\n",
      "-----Epoch:  9420 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1374604896014715\n",
      "-----Epoch:  9430 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13743349873724295\n",
      "-----Epoch:  9440 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13740653880353468\n",
      "-----Epoch:  9450 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1373796097160863\n",
      "-----Epoch:  9460 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1373527113909619\n",
      "-----Epoch:  9470 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13732584374454856\n",
      "-----Epoch:  9480 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13729900669355397\n",
      "-----Epoch:  9490 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1372722001550066\n",
      "-----Epoch:  9500 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13724542404625206\n",
      "-----Epoch:  9510 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13721867828495307\n",
      "-----Epoch:  9520 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13719196278908732\n",
      "-----Epoch:  9530 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13716527747694596\n",
      "-----Epoch:  9540 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13713862226713192\n",
      "-----Epoch:  9550 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13711199707855895\n",
      "-----Epoch:  9560 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13708540183044934\n",
      "-----Epoch:  9570 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13705883644233327\n",
      "-----Epoch:  9580 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13703230083404644\n",
      "-----Epoch:  9590 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13700579492572912\n",
      "-----Epoch:  9600 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13697931863782475\n",
      "-----Epoch:  9610 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13695287189107833\n",
      "-----Epoch:  9620 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13692645460653463\n",
      "-----Epoch:  9630 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13690006670553714\n",
      "-----Epoch:  9640 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13687370810972685\n",
      "-----Epoch:  9650 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1368473787410399\n",
      "-----Epoch:  9660 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13682107852170722\n",
      "-----Epoch:  9670 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13679480737425267\n",
      "-----Epoch:  9680 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13676856522149136\n",
      "-----Epoch:  9690 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1367423519865286\n",
      "-----Epoch:  9700 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1367161675927585\n",
      "-----Epoch:  9710 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13669001196386246\n",
      "-----Epoch:  9720 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13666388502380755\n",
      "-----Epoch:  9730 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13663778669684604\n",
      "-----Epoch:  9740 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.136611716907513\n",
      "-----Epoch:  9750 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13658567558062543\n",
      "-----Epoch:  9760 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13655966264128105\n",
      "-----Epoch:  9770 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13653367801485647\n",
      "-----Epoch:  9780 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13650772162700647\n",
      "-----Epoch:  9790 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13648179340366218\n",
      "-----Epoch:  9800 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13645589327103044\n",
      "-----Epoch:  9810 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1364300211555913\n",
      "-----Epoch:  9820 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1364041769840983\n",
      "-----Epoch:  9830 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13637836068357542\n",
      "-----Epoch:  9840 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13635257218131755\n",
      "-----Epoch:  9850 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13632681140488784\n",
      "-----Epoch:  9860 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1363010782821172\n",
      "-----Epoch:  9870 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1362753727411029\n",
      "-----Epoch:  9880 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13624969471020706\n",
      "-----Epoch:  9890 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13622404411805575\n",
      "-----Epoch:  9900 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13619842089353726\n",
      "-----Epoch:  9910 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1361728249658015\n",
      "-----Epoch:  9920 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13614725626425853\n",
      "-----Epoch:  9930 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13612171471857668\n",
      "-----Epoch:  9940 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13609620025868305\n",
      "-----Epoch:  9950 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13607071281475994\n",
      "-----Epoch:  9960 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.13604525231724587\n",
      "-----Epoch:  9970 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1360198186968328\n",
      "-----Epoch:  9980 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.135994411884466\n",
      "-----Epoch:  9990 -----\n",
      "Accuracy: 0.625\n",
      "Cost: 0.1359690318113422\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[3, 2, 4, 3, 3.5, 2, 5.5, 1],\n",
    "                    [1.5, 1, 1.5, 1, 0.5, 0.5, 1, 1]])\n",
    "y_train = np.array([1, 0, 1, 0, 1, 0, 1, 0])\n",
    "\n",
    "Flower_net = NeuralNet('binary_logloss')\n",
    "Flower_net.addLayer(1, 'sigmoid')\n",
    "costs, weights = Flower_net.fit(X_train, y_train, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x207917d2ee0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD4CAYAAAANbUbJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhGklEQVR4nO3de3RcZ3nv8e8zMxpdRjdLli1b8i2J08SYOBjFEEISUhKaBIqhpItwp4R6pSU9pZd1CKc9PT2lXQtKV0tbAmlWCNBSSFNCwA0BwwmXEALEcprEdhIniq+yY1uyZN3ves4fsyWPxyNrZI800p7fZy2tmf3uvTXP68vv3fPOnr3N3RERkXCL5LsAERGZfQp7EZECoLAXESkACnsRkQKgsBcRKQCxfBeQyeLFi3316tX5LkNEZMHYsWNHu7vXTbV+Xob96tWraW5uzncZIiILhpkdONt6TeOIiBQAhb2ISAFQ2IuIFACFvYhIAVDYi4gUAIW9iEgBUNiLiBSAUIX9Pz/6Ej99sS3fZYiIzDuhCvsv/ORlft7Snu8yRETmnVCFfcRgfFw3YxERSReysDeU9SIiZwpV2JvBuG6zKCJyhlCFfSRi6J66IiJnClXYG2gaR0Qkg1CFfcQMR2kvIpIuVGFv+oBWRCSjUIV9xNCcvYhIBiELe2N8PN9ViIjMPyELe516KSKSSajCXnP2IiKZZRX2Znajme0xsxYzuzPD+jeZWZeZPR38/EW2++aSGTobR0Qkg9h0G5hZFLgLuAFoBbab2VZ3fy5t05+5+9vOcd+ciJihWRwRkTNlc2S/CWhx973uPgzcD2zO8vefz74zpjl7EZHMsgn7BuBQynJr0JbuSjN7xsy+Z2avmuG+mNkWM2s2s+a2tnO7Jr0uhCYiklk2YW8Z2tIj9SlglbtvAP4Z+PYM9k02ut/j7k3u3lRXV5dFWRkK1ZG9iEhG2YR9K7AiZbkROJK6gbt3u3tv8PwRoMjMFmezby6Z6UJoIiKZZBP224G1ZrbGzOLArcDW1A3MrN7MLHi+Kfi9J7LZN5eS36Cdrd8uIrJwTXs2jruPmtkdwDYgCtzn7rvN7PZg/d3ALcDvmdkoMADc6slD7Iz7zlJfgjl7pb2ISLppwx4mp2YeSWu7O+X554HPZ7vvbNGXqkREMgvVN2ijERhT2ouInCFUYV8SizI4MpbvMkRE5p1QhX1pPMqAwl5E5AyhCvuSoiiDI7rGsYhIulCFfWmRpnFERDIJXdgPDCvsRUTShSrsS4oimrMXEckgXGEf1zSOiEgmoQr70qIoQ6PjjOtcexGR04Qu7AEGR3V0LyKSKlxhH0+GvT6kFRE5XbjCPjiy71fYi4icJlRhX16cvK5b3/BonisREZlfQhX2iYmwH1LYi4ikClnYJ6dxeoc0jSMikipkYa8jexGRTLIKezO70cz2mFmLmd15lu2uMLMxM7slpW2/me00s6fNrDkXRU8lEVfYi4hkMu2dqswsCtwF3EDyBuLbzWyruz+XYbvPkLwFYbrr3L09B/WelY7sRUQyy+bIfhPQ4u573X0YuB/YnGG7PwAeBI7nsL4ZmZiz79OplyIip8km7BuAQynLrUHbJDNrAN4J3M2ZHPiBme0wsy1TvYiZbTGzZjNrbmtry6KsMxXHohRFjV4d2YuInCabsLcMbekXn/kc8Al3z3RIfZW7bwRuAj5mZtdkehF3v8fdm9y9qa6uLouyMksUx+hX2IuInGbaOXuSR/IrUpYbgSNp2zQB95sZwGLgZjMbdfdvu/sRAHc/bmYPkZwWeuy8K59CIh7TqZciImmyObLfDqw1szVmFgduBbambuDua9x9tbuvBr4J/L67f9vMEmZWAWBmCeAtwK6c9iBNojiqD2hFRNJMe2Tv7qNmdgfJs2yiwH3uvtvMbg/WZ5qnn7AUeCg44o8BX3f3759/2VNLFMd0uQQRkTTZTOPg7o8Aj6S1ZQx5d/9wyvO9wIbzqG/GEvGYjuxFRNKE6hu0MDGNozl7EZFUIQz7mE69FBFJE7qwL9ecvYjIGUIX9hUlMXoGR3HXfWhFRCaELuwrS4oYG3ddMkFEJEXowr6qtAiA7oGRPFciIjJ/hDbsuxT2IiKTQhf2lQp7EZEzhC7sdWQvInKm0Ia95uxFRE4JXdhrGkdE5EyhC/uK4hhmOrIXEUkVurCPRIyK4piO7EVEUoQu7AGqyoroHtQlE0REJoQz7EuLdGQvIpIilGFfWaKwFxFJlVXYm9mNZrbHzFrM7M6zbHeFmY2Z2S0z3TeXqsuKONk/PBcvJSKyIEwb9mYWBe4CbgLWAe8xs3VTbPcZkrcvnNG+uVaTiNPRp7AXEZmQzZH9JqDF3fe6+zBwP7A5w3Z/ADwIHD+HfXOqNlHMyYERRsfGZ/ulREQWhGzCvgE4lLLcGrRNMrMG4J1A+n1pp9035XdsMbNmM2tua2vLoqyp1ZbHcYfOfs3bi4hAdmFvGdrS7wzyOeAT7p5+Efls9k02ut/j7k3u3lRXV5dFWVOrTRQDaCpHRCQQy2KbVmBFynIjcCRtmybgfjMDWAzcbGajWe6bczWJOAAneoeAitl+ORGReS+bsN8OrDWzNcBh4FbgvakbuPuaiedm9hXgYXf/tpnFptt3NiwuT4Z9u47sRUSALMLe3UfN7A6SZ9lEgfvcfbeZ3R6sT5+nn3bf3JQ+tYkj+47eodl+KRGRBSGbI3vc/RHgkbS2jCHv7h+ebt/ZVl0WJ2JwQkf2IiJASL9BG40YNYm4wl5EJBDKsIfkVM4JTeOIiAAhDvvaRDEnenVkLyICIQ77pZXFHO0ezHcZIiLzQnjDvqqE491DuGf8DpeISEEJbdjXV5YwPDaub9GKiBDisF9WVQLAK12ayhERCW3YL61Mhv0xzduLiIQ37OuDI3t9SCsiEuKwrysvJmJwVNM4IiLhDftYNEJdRbHCXkSEEIc9JM/I0TSOiEjIw35ZVSmHTw7kuwwRkbwLddivrC2jtXOA8XF9sUpECluow35FTRnDo+Mc79EF0USksIU67FfWlAFwsKM/z5WIiORXVmFvZjea2R4zazGzOzOs32xmz5rZ02bWbGZvTFm338x2TqzLZfHTmQj7Qwp7ESlw096pysyiwF3ADSRvIL7dzLa6+3Mpmz0KbHV3N7PLgAeAS1LWX+fu7TmsOysN1aWY6cheRCSbI/tNQIu773X3YeB+YHPqBu7e66cuL5kA5sUnovFYhOVVpTqyF5GCl03YNwCHUpZbg7bTmNk7zewF4LvAR1JWOfADM9thZlumehEz2xJMATW3tbVlV30WVtSU6sheRApeNmFvGdrOOHJ394fc/RLgHcCnUlZd5e4bgZuAj5nZNZlexN3vcfcmd2+qq6vLoqzsrKwp44DCXkQKXDZh3wqsSFluBI5MtbG7PwZcaGaLg+UjweNx4CGS00Jz5oK6ctp6hugaGJnLlxURmVeyCfvtwFozW2NmceBWYGvqBmZ2kZlZ8HwjEAdOmFnCzCqC9gTwFmBXLjswnYvqygFoOd47ly8rIjKvTHs2jruPmtkdwDYgCtzn7rvN7PZg/d3Au4APmtkIMAC8OzgzZynwUDAOxICvu/v3Z6kvGa1dOhH2Pbx21aK5fGkRkXlj2rAHcPdHgEfS2u5Oef4Z4DMZ9tsLbDjPGs9L46IyimMRHdmLSEEL9TdoAaIR48K6cl5S2ItIAQt92ENyKuelYwp7ESlcBRH2F9WVc/jkAH1Do/kuRUQkLwoi7C+urwBgz7GePFciIpIfBRH26xuqANh9pDvPlYiI5EdBhP3yqhIWlRWxq7Ur36WIiORFQYS9mbG+oYpdRxT2IlKYCiLsAV61vIoXj/UwNDqW71JEROZcwYT9qxuqGBlzXjyqUzBFpPAUTNivb6gEYOdhTeWISOEpmLBfWVPGorIi/vtgZ75LERGZcwUT9mbGa1fV0HxAYS8ihadgwh7gitWL2NfeR1vPUL5LERGZUwUV9k2rawDYcaAjz5WIiMytggr79Q2VxGMRtu/XVI6IFJaCCvviWJTLG6tp3q8jexEpLFmFvZndaGZ7zKzFzO7MsH6zmT1rZk+bWbOZvTHbfefa6y+oYefhLrr6dU9aESkc04a9mUWBu4CbgHXAe8xsXdpmjwIb3P1y4CPAvTPYd05dc3Ed4w5PvNyezzJEROZUNkf2m4AWd9/r7sPA/cDm1A3cvdfdPVhMAJ7tvnNtw4pqKopjPPaSwl5ECkc2Yd8AHEpZbg3aTmNm7zSzF4Dvkjy6z3rfYP8twRRQc1tbWza1n5OiaIQrL6zlsRfbODU+iYiEWzZhbxnazkhJd3/I3S8B3gF8aib7Bvvf4+5N7t5UV1eXRVnn7uqL6zh8coB97X2z+joiIvNFNmHfCqxIWW4Ejky1sbs/BlxoZotnuu9cuXZtcjD58Z7ZewchIjKfZBP224G1ZrbGzOLArcDW1A3M7CIzs+D5RiAOnMhm33xYWVvGJfUVbNt9NN+liIjMiWnD3t1HgTuAbcDzwAPuvtvMbjez24PN3gXsMrOnSZ59825PyrjvLPRjxn7jVfU07++gvVeXThCR8LP5+CFlU1OTNzc3z+prPHekm5v/6Wd8+rdeza2bVs7qa4mIzDYz2+HuTVOtL6hv0Ka6dFkFK2vK+L6mckSkABRs2JsZN66v5+ct7XT2Dee7HBGRWVWwYQ+w+fLljIw5D+98Jd+liIjMqoIO+3XLKrmkvoIHd7TmuxQRkVlV0GFvZrxrYyNPHzrJy226EbmIhFdBhz0kp3IiBt96Skf3IhJeBR/2SypLuPbiOv6zuZWRsfF8lyMiMisKPuwBPnDlKo73DOkbtSISWgp74NqLl7Cypox/feJAvksREZkVCnsgGjE+eOUqntzfwXNHuvNdjohIzinsA7/92hWUFEX40uP78l2KiEjOKewDVWVFvGfTSr7z9GEOdfTnuxwRkZxS2KfYcs0FmMG/PPZyvksREckphX2KZVWl3PLaFTzQ3Mrx7sF8lyMikjMK+zS/d+2FjI07X/iJju5FJDwU9mlW1pbx7itW8LVfHmC/7lErIiGRVdib2Y1mtsfMWszszgzr32dmzwY/T5jZhpR1+81sp5k9bWaze0eSHPn49WuJxyJ8dtuefJciIpIT04a9mUVJ3mrwJmAd8B4zW5e22T7gWne/DPgUcE/a+uvc/fKz3UVlPllSUcLvXn0B3935Ck8d7Mx3OSIi5y2bI/tNQIu773X3YeB+YHPqBu7+hLtPpOIvgcbcljn3tlxzAXUVxfzl1t2Mjc+/WzeKiMxENmHfABxKWW4N2qZyG/C9lGUHfmBmO8xsy8xLzI9EcYw/f+ulPNvaxdd+qcsoiMjClk3YW4a2jIe6ZnYdybD/RErzVe6+keQ00MfM7Jop9t1iZs1m1tzW1pZFWbPv7RuWc/XaxXx22x6O6VRMEVnAsgn7VmBFynIjcCR9IzO7DLgX2OzuJyba3f1I8HgceIjktNAZ3P0ed29y96a6urrsezCLzIxPbV7P8Ng4f/GdXbhrOkdEFqZswn47sNbM1phZHLgV2Jq6gZmtBL4FfMDdX0xpT5hZxcRz4C3ArlwVPxdWL07wxzdczLbdx3jwqcP5LkdE5JxMG/buPgrcAWwDngcecPfdZna7md0ebPYXQC3whbRTLJcCj5vZM8CTwHfd/fs578Us+92rL2DTmhr+cutuXTdHRBYkm49TE01NTd7cPL9OyW/t7Oemz/2MX6uv4BtbXk9RVN9HE5H5w8x2nO30diVWlhoXlfHX71xP84FOPv29F/JdjojIjCjsZ2Dz5Q18+A2r+dLj+9j6zBmfUYuIzFsK+xn6XzdfyhWrF/GJbz6ru1qJyIKhsJ+heCzCXe/bSFVpEb/zlSc5fHIg3yWJiExLYX8OllSU8JWPXEH/8Bgfuu9JuvpH8l2SiMhZKezP0SX1ldzzgSYOnujntq9up29oNN8liYhMSWF/Hq68sJZ/ePflPHWwk9/5ynb6hxX4IjI/KezP01svW8bnbn0Nzfs7+PCXFfgiMj8p7HPg7RuWTwb+++/9FZ19w/kuSUTkNAr7HHn7huV84X0b2XWkm3fd/YQuqyAi84rCPoduXL+Mr932Otp7hvitLz7BztaufJckIgIo7HNu05oavvl7byAejXDL3U/w4I7WfJckIqKwnw0XL61g6x1X8ZqV1fzJfz7D//nOLkbGxvNdlogUMIX9LKktL+Zrt72Oj75xDV/9xQFuufsX7Gvvy3dZIlKgFPazKBaN8OdvW8cX37eR/e193PyPP+MbTx7UHa9EZM4p7OfATa9exraPX8PGVdV88ls7+ehXmzmia+qIyBxS2M+R+qoS/u0jr+N/v20dT7x8guv//qd86fF9jI3rKF9EZl9WYW9mN5rZHjNrMbM7M6x/n5k9G/w8YWYbst23kEQixm1vXMMP/ugaNq2p4VMPP8c77vo5Ow505rs0EQm5acPezKLAXcBNwDrgPWa2Lm2zfcC17n4Z8CngnhnsW3BW1JTx5Q9fweff+xqOdQ/yri8+wcf+/SkOntAXsURkdmRzZL8JaHH3ve4+DNwPbE7dwN2fcPeJw9NfAo3Z7luozIy3XbacH//pm/jDN6/lRy8c5/q//yl//fBztPcO5bs8EQmZbMK+ATiUstwatE3lNuB7M93XzLaYWbOZNbe1tWVRVjgkimP80Q0X8+M/fRObL1/OfT/fx9Wf+TF/893naOtR6ItIbmQT9pahLeOnimZ2Hcmw/8RM93X3e9y9yd2b6urqsigrXOqrSvjsb2/gh398LTeur+dLj+/j6r/9EX/1X8/R2qnpHRE5P9mEfSuwImW5ETjjbttmdhlwL7DZ3U/MZF855cK6cv7h3Zfz6J+8ibe+ejlf/cV+rv3sT/jYvz/FjgMdOkdfRM6JTRceZhYDXgTeDBwGtgPvdffdKdusBH4EfNDdn5jJvpk0NTV5c3PzOXUobA6fHOBff7Gfb/zqIN2Do2xYUc2HrlzFTeuXURqP5rs8EZknzGyHuzdNuT6bI0Uzuxn4HBAF7nP3vzGz2wHc/W4zuxd4F3Ag2GV04kUz7Tvd6ynsz9Q3NMqDT7Xy5Z/vZ197HxUlMd5xeQPvvmIF6xuq8l2eiORZTsJ+rinsp+bu/HJvBw80H+KRna8wNDrOq5ZX8lsbG3nrq5dRX1WS7xJFJA8U9iHWNTDC1qcP8x/Nh9h1uBszuGJ1Db+5YTk3r6+ntrw43yWKyBxR2BeIvW29PPzsK2x95ggtx3uJRozXX1DD9Zcu5fpLl7KipizfJYrILFLYFxh354WjPfzXM0fYtvsoL7clL6t8SX0Fb750CddfupQNjdVEIpnOihWRhUphX+D2tffx6PPH+OFzx2g+0MnYuLOorIg3XLSYqy9azFUXLdZRv0gIKOxl0sn+YX6yp43HXmrj8ZfaOR58Q3dVbRlvDIL/itU11FVorl9koVHYS0buTsvxXh5vaefxl9r55d4T9A2PAbBmcYKmVYu4Yk0NV6yuYXVtGWaa9hGZzxT2kpWRsXGebe2ieX8H2/d30nygg5P9IwAsLo/z2lWLuHzFIjY0VrG+sYrKkqI8VywiqRT2ck7Gx5297b1s39/J9v0dNO/v5GDHqWv0XLA4wWWNVby6sZoNjVW8anmVvtErkkcKe8mZzr5hdh7u4tnWkzzT2sXO1i6Odg8CEDG4oK6cS+oruHRZ5eTjsqoSTQGJzIHpwj42l8XIwrYoEeeai+u45uJTVyU93j0YBP9Jnj/awzOtJ3n42Vcm11eWxLhkWSWX1ldwybJK1i4p58K6chYl4vnogkjBUtjLeVlSWcIN60q4Yd3SybaewRH2HO3h+aM9vPBKNy8c7eGbO1onPwAGqE3EubCunAuXJILHci6qK6ehulTfARCZBQp7ybmKkiKaVtfQtLpmsm183Dl8coCW47283Jb8aTney7bdx+joO3V/m+JYhDWLE6xZnGBlbRmrahKsqi1jZU0Zy6tLiWogEDknCnuZE5GIsaKmjBU1ZVx3yZLT1nX0DbM3CP+JQWDP0R7+3/PHGBk79ZlSUdRYsagsGATKWFmbCB7LaKguJVGsf84iU9H/Dsm7mkScmsTp7wQAxsadV7oGOHiinwMd/Rw40c/Bjj4OnOhnx/5OeoZGT9u+uqyI5VWlNCwqpaE6+bO8+tTy4vK4PiyWgqWwl3krGjEaF5XRuKiMN6Stc3c6+oY50NHPoY5+jpwc5PDJfg53JgeHX7x8gt60wSAeiwQDQAnLqkqpryxhaVUJSyuKqa8qob6yhNryYk0VSSgp7GVBMjNqy4upLS9m48pFZ6x3d7oHRzncOcCRkwMcTv3pHOBnL7XR1jPEeNqZx9GIUVdezNLKYpZWllBfVcLSyuRPfWUJSyuLWVJRQmVpTO8SZEHJKuzN7EbgH0nebeped/902vpLgC8DG4E/c/e/S1m3H+gBxki5g5XIbDIzqkqLqCotYt3yyozbjI077b1DHOse5GjXIMd6hjjWNcjR7kGOdQ+y/0Qfv9rXQdfAyBn7xqMRasvjLC4vZvHEY0Xx5HJdynJ1aZHOMJK8mzbszSwK3AXcQPIG4tvNbKu7P5eyWQfwP4B3TPFrrnP39vOsVSSnohGbPGq/rHHq7QaGxzjekxwQjnYP0tYzRHvvMO29Q7T3DtHWO8Tzr/TQ3jvEaPpbBSAWMWoSqQNCnNpEnJpEMTWJIhaVxalJxFmUiFNTFqdKg4PMgmyO7DcBLe6+F8DM7gc2A5Nh7+7HgeNm9tZZqVIkj0rjUVbVJlhVmzjrdu5O18AIbT3JAaC9d5j2nqHJQWFigGg51sOJvmGGRscz/p6IQXVZnEVlRdQmilmUKEoOBhODQtrgUJ0ooqJY00pydtmEfQNwKGW5FXjdDF7DgR+YmQP/4u73ZNrIzLYAWwBWrlw5g18vMj+YGdVlcarL4qxdWjHt9gPDY3T0D9PZN0xHyk9n/+mP+9v7eergSTr7hjO+c4DkADExbVUVvDuonlguLaK6rIjK1LayIqpLk9uVFEU0UBSAbMI+07+CmVxQ5yp3P2JmS4AfmtkL7v7YGb8wOQjcA8lr48zg94ssSKXxKA3x5Gmh2XB3eoZG6ewb5kTf8ORj98AIJ/tH6BoY4eRA8rFrYISDJ/omn08xRgDJs5TSB4eqsiIqS5IDRGVJjIqSGBUlRVSUxKgMHieWS4p0AbyFIJuwbwVWpCw3AkeyfQF3PxI8Hjezh0hOC50R9iJydmaWDOCSommnlFKNjzu9w6N09Z8aCE4NDsN0DYycNmC80jXIC0d76B4YoXd4lOmulRiPRqgsPRX+FSUxKoqDgaG0KG2gmBgsTm1bXhKjOKYBY7ZlE/bbgbVmtgY4DNwKvDebX25mCSDi7j3B87cAf3WuxYrIzEUipwaJFdNvfpqJgaJncJSewRG6B5KPk8uDo3RPLp9a19bTO7lt6jWRplIUNcqLYySKY5OPieIYFcUxEsXR09rLT9smOvm8Ingsi0c1LZXBtGHv7qNmdgewjeSpl/e5+24zuz1Yf7eZ1QPNQCUwbmYfB9YBi4GHgj/4GPB1d//+rPRERHIudaCA7Kab0o2NO73BoJBpYOgdCn4GR+kLnvcNj9I1MMLhzn76hsaS7Vm8ywAwg0T81ECQOnicthyPUhYMDsmfZFtpPDo5aJTFk4/FsYX/uYauZy8iC4K70z88dmpAGBqbHCj60h4nnp9tm9TrLk0nEgwgEwNBaVGURPGpwWDysThKWVFyoCmNR0mkri9ODiqTvycey+mH47qevYiEgplNHqEvmX7zaQ2PjjMwPEbf8Cj9w2P0pz8OJZ/3pbeNjNE/lFw+2T/MkZNjk/v1DY8xPMUptZn7BGVFUUqDQaG+soQHbr8yB707k8JeRApSPBZJnolUltv7KY+OjQcDQuoAkhxUBoJ3JgMjY/RNDCZDYwyMjDE4MkZJUSSntaRS2IuI5FAsGqEyGgk+55g/Zm8YERGReUNhLyJSABT2IiIFQGEvIlIAFPYiIgVAYS8iUgAU9iIiBUBhLyJSAObltXHMrA04cI67LwYK7RaI6nP4FVp/QX2eqVXuXjfVynkZ9ufDzJoL7abm6nP4FVp/QX3ONU3jiIgUAIW9iEgBCGPYZ7yhecipz+FXaP0F9TmnQjdnLyIiZwrjkb2IiKRR2IuIFIDQhL2Z3Whme8ysxczuzHc958PMVpjZj83seTPbbWZ/GLTXmNkPzeyl4HFRyj6fDPq+x8x+I6X9tWa2M1j3TzaP75psZlEz+28zezhYDnt/q83sm2b2QvB3fWUB9PmPgn/Tu8zsG2ZWErY+m9l9ZnbczHaltOWsj2ZWbGb/EbT/ysxWZ1WYuy/4HyAKvAxcAMSBZ4B1+a7rPPqzDNgYPK8AXgTWAX8L3Bm03wl8Jni+LuhzMbAm+LOIBuueBK4EDPgecFO++3eWfv8x8HXg4WA57P39KvDR4HkcqA5zn4EGYB9QGiw/AHw4bH0GrgE2ArtS2nLWR+D3gbuD57cC/5FVXfn+g8nRH+6VwLaU5U8Cn8x3XTns33eAG4A9wLKgbRmwJ1N/gW3Bn8ky4IWU9vcA/5Lv/kzRx0bgUeDXORX2Ye5vZRB8ltYe5j43AIeAGpK3RH0YeEsY+wysTgv7nPVxYpvgeYzkN25tuprCMo0z8Y9oQmvQtuAFb9FeA/wKWOrurwAEj0uCzabqf0PwPL19Pvoc8D+B8ZS2MPf3AqAN+HIwdXWvmSUIcZ/d/TDwd8BB4BWgy91/QIj7nCKXfZzcx91HgS6gdroCwhL2mebrFvw5pWZWDjwIfNzdu8+2aYY2P0v7vGJmbwOOu/uObHfJ0LZg+huIkXyr/0V3fw3QR/Lt/VQWfJ+DeerNJKcrlgMJM3v/2XbJ0Lag+pyFc+njOfU/LGHfCqxIWW4EjuSplpwwsyKSQf/v7v6toPmYmS0L1i8DjgftU/W/NXie3j7fXAW83cz2A/cDv25mXyO8/YVkra3u/qtg+Zskwz/Mfb4e2Ofube4+AnwLeAPh7vOEXPZxch8ziwFVQMd0BYQl7LcDa81sjZnFSX5osTXPNZ2z4FP3LwHPu/vfp6zaCnwoeP4hknP5E+23Bp/SrwHWAk8Gbxd7zOz1we/8YMo+84a7f9LdG919Ncm/ux+5+/sJaX8B3P0ocMjMfi1oejPwHCHuM8npm9ebWVlQ65uB5wl3nyfkso+pv+sWkv9fpn9nk+8PMnL4gcjNJM9aeRn4s3zXc559eSPJt2XPAk8HPzeTnJd7FHgpeKxJ2efPgr7vIeXMBKAJ2BWs+zxZfJCT576/iVMf0Ia6v8DlQHPw9/xtYFEB9Pn/Ai8E9f4bybNQQtVn4BskP5MYIXkUflsu+wiUAP8JtJA8Y+eCbOrS5RJERApAWKZxRETkLBT2IiIFQGEvIlIAFPYiIgVAYS8iUgAU9iIiBUBhLyJSAP4/hIPfI4Wm/OYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing Flower problem\n",
    "\n",
    "def vis_data():\n",
    "    plt.axis([0, 6, 0 ,3])\n",
    "    plt.grid()\n",
    "    for i in range(X_train.shape[1]):\n",
    "        point  = X_train[:, i]\n",
    "        color = 'r'\n",
    "        if y_train[i] == 0:\n",
    "            color = 'b'\n",
    "        plt.scatter(point[0], point[1], c=color)\n",
    "    \n",
    "# check out the networks predictions in the x,y plane\n",
    "for x in np.linspace(0, 6, 30):\n",
    "    for y in np.linspace(0, 3, 30):\n",
    "        test = np.array([[x],[y]])\n",
    "        pred = Flower_net.predict(test)\n",
    "        c = 'b'\n",
    "        if pred > .5:\n",
    "            c = 'r'\n",
    "        plt.scatter([x],[y],c=c, alpha=.2)\n",
    "\n",
    "vis_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset --> Recognizing Handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is trained on the neural network in 4 different ways.\n",
    "- Without batches and with only a single hidden layer\n",
    "- With batches and with only a single hidden layer\n",
    "- Without batches and with multiple hidden layers\n",
    "- With batches and with multiple hdden layers\n",
    "The last 'test_mnist_net' is there to show the ability to use the weights of a pre trained network into another of matching architecture.\n",
    "\n",
    "Additional configuration can also take place including changing around the activation functions, num of layers, num of neurons per layer, learning rate and batches (I expect many bugs to arise from this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation].astype(np.int)\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=60000, test_size=10000)\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_accuracy(net, test_images, test_labels):\n",
    "    y_pred = net.predict(test_images)\n",
    "    test_accuracy = 100*np.sum(test_labels == np.argmax(y_pred, 0), axis=0) / test_images.shape[1]\n",
    "    print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(img_array, label_array):\n",
    "    fig, ax = plt.subplots(nrows=8, ncols=8, sharex=True, sharey=True)\n",
    "    ax = ax.flatten()\n",
    "    count = -1\n",
    "    for i, num in enumerate(label_array):\n",
    "        if num == 9 and count < 63:\n",
    "            count+=1\n",
    "            img = img_array[:, i].reshape(28, 28)\n",
    "            ax[count].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    plt.show()\n",
    "visualize_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_misclassified_test_images(net, test_images, test_labels, numToShow):\n",
    "    y_pred = np.argmax(net.predict(test_images), 0)\n",
    "    misclassified_img = test_images[:, test_labels != y_pred][:, :numToShow]\n",
    "    correct_label = test_labels[test_labels != y_pred][:numToShow]\n",
    "    misclassified_label = y_pred[test_labels != y_pred][:numToShow]\n",
    "    fig, ax = plt.subplots(np.sqrt(numToShow), np.sqrt(numToShow), sharex=True, sharey=True)\n",
    "    ax = ax.flatten()\n",
    "    for i in range(numToShow):\n",
    "        img = misclassified_img[:, i].reshape(28, 28)\n",
    "        ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "        ax[i].set_title('%d) t: %d p: %d' % (i + 1, correct_label[i], misclassified_label[i]))\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_costs(costs):\n",
    "    plt.plot(costs)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Multi Logloss Cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_image_from_test_set(net, test_images, img_index):\n",
    "    plt.imshow(test_images[:, img_index].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    print(np.argmax(net.predict(test_images[:, img_index].reshape(784, 1)), 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_image(net, image):\n",
    "    plt.imshow(image.reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    print(np.argmax(net.predict(image), 0)[0])\n",
    "    print(net.predict(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset With 1 Hidden Layer + No Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  0 -----\n",
      "Accuracy: 0.05366666666666667\n",
      "Cost: 2.466751337389721\n",
      "-----Epoch:  10 -----\n",
      "Accuracy: 0.5485833333333333\n",
      "Cost: 1.1847559238008232\n",
      "-----Epoch:  20 -----\n",
      "Accuracy: 0.6806333333333333\n",
      "Cost: 1.0634580287879696\n",
      "-----Epoch:  30 -----\n",
      "Accuracy: 0.7345166666666667\n",
      "Cost: 1.01875193701685\n",
      "-----Epoch:  40 -----\n",
      "Accuracy: 0.7653333333333333\n",
      "Cost: 0.9945794771551656\n",
      "-----Epoch:  50 -----\n",
      "Accuracy: 0.7847333333333333\n",
      "Cost: 0.9789735521116122\n",
      "-----Epoch:  60 -----\n",
      "Accuracy: 0.7982333333333334\n",
      "Cost: 0.9678508900275774\n",
      "-----Epoch:  70 -----\n",
      "Accuracy: 0.8084833333333333\n",
      "Cost: 0.9594198001384417\n",
      "-----Epoch:  80 -----\n",
      "Accuracy: 0.8174333333333333\n",
      "Cost: 0.952721881550396\n",
      "-----Epoch:  90 -----\n",
      "Accuracy: 0.8249166666666666\n",
      "Cost: 0.9472703462011776\n",
      "-----Epoch:  100 -----\n",
      "Accuracy: 0.8311666666666667\n",
      "Cost: 0.9427519321851021\n",
      "-----Epoch:  110 -----\n",
      "Accuracy: 0.8369\n",
      "Cost: 0.9389668832966809\n",
      "-----Epoch:  120 -----\n",
      "Accuracy: 0.8417666666666667\n",
      "Cost: 0.9357393265870206\n",
      "-----Epoch:  130 -----\n",
      "Accuracy: 0.8460833333333333\n",
      "Cost: 0.9329611914627803\n",
      "-----Epoch:  140 -----\n",
      "Accuracy: 0.8493666666666667\n",
      "Cost: 0.9305348933803383\n",
      "-----Epoch:  150 -----\n",
      "Accuracy: 0.8523833333333334\n",
      "Cost: 0.9284105808050778\n",
      "-----Epoch:  160 -----\n",
      "Accuracy: 0.8552666666666666\n",
      "Cost: 0.9265336379921957\n",
      "-----Epoch:  170 -----\n",
      "Accuracy: 0.8580833333333333\n",
      "Cost: 0.9248718055305692\n",
      "-----Epoch:  180 -----\n",
      "Accuracy: 0.8607833333333333\n",
      "Cost: 0.9233896400122164\n",
      "-----Epoch:  190 -----\n",
      "Accuracy: 0.863\n",
      "Cost: 0.9220556358072736\n",
      "-----Epoch:  200 -----\n",
      "Accuracy: 0.86535\n",
      "Cost: 0.9208558164497477\n",
      "-----Epoch:  210 -----\n",
      "Accuracy: 0.8673166666666666\n",
      "Cost: 0.9197682102365786\n",
      "-----Epoch:  220 -----\n",
      "Accuracy: 0.8690833333333333\n",
      "Cost: 0.9187718257182053\n",
      "-----Epoch:  230 -----\n",
      "Accuracy: 0.8707\n",
      "Cost: 0.9178577226303264\n",
      "-----Epoch:  240 -----\n",
      "Accuracy: 0.8721833333333333\n",
      "Cost: 0.9170136830205647\n",
      "-----Epoch:  250 -----\n",
      "Accuracy: 0.87345\n",
      "Cost: 0.9162430760735086\n",
      "-----Epoch:  260 -----\n",
      "Accuracy: 0.8740833333333333\n",
      "Cost: 0.9156102667767905\n",
      "-----Epoch:  270 -----\n",
      "Accuracy: 0.8753\n",
      "Cost: 0.915066087515686\n",
      "-----Epoch:  280 -----\n",
      "Accuracy: 0.8761666666666666\n",
      "Cost: 0.9144862883058037\n",
      "-----Epoch:  290 -----\n",
      "Accuracy: 0.8772166666666666\n",
      "Cost: 0.9138970477243223\n",
      "-----Epoch:  300 -----\n",
      "Accuracy: 0.8784333333333333\n",
      "Cost: 0.9133391762485499\n",
      "-----Epoch:  310 -----\n",
      "Accuracy: 0.8794166666666666\n",
      "Cost: 0.9128224102245157\n",
      "-----Epoch:  320 -----\n",
      "Accuracy: 0.8805\n",
      "Cost: 0.9123426794662652\n",
      "-----Epoch:  330 -----\n",
      "Accuracy: 0.8812\n",
      "Cost: 0.9118911571322097\n",
      "-----Epoch:  340 -----\n",
      "Accuracy: 0.8822166666666666\n",
      "Cost: 0.9114546083727282\n",
      "-----Epoch:  350 -----\n",
      "Accuracy: 0.8832333333333333\n",
      "Cost: 0.9110215962364269\n",
      "-----Epoch:  360 -----\n",
      "Accuracy: 0.8842666666666666\n",
      "Cost: 0.910590133811244\n",
      "-----Epoch:  370 -----\n",
      "Accuracy: 0.8852666666666666\n",
      "Cost: 0.9101645410029197\n",
      "-----Epoch:  380 -----\n",
      "Accuracy: 0.88645\n",
      "Cost: 0.9097586943887246\n",
      "-----Epoch:  390 -----\n",
      "Accuracy: 0.8875166666666666\n",
      "Cost: 0.9093811939859036\n",
      "-----Epoch:  400 -----\n",
      "Accuracy: 0.88815\n",
      "Cost: 0.9090286151487035\n",
      "-----Epoch:  410 -----\n",
      "Accuracy: 0.8889333333333334\n",
      "Cost: 0.9086940450715824\n",
      "-----Epoch:  420 -----\n",
      "Accuracy: 0.8896833333333334\n",
      "Cost: 0.9083716892510961\n",
      "-----Epoch:  430 -----\n",
      "Accuracy: 0.8905666666666666\n",
      "Cost: 0.9080550538901199\n",
      "-----Epoch:  440 -----\n",
      "Accuracy: 0.8914\n",
      "Cost: 0.9077439705819661\n",
      "-----Epoch:  450 -----\n",
      "Accuracy: 0.8920833333333333\n",
      "Cost: 0.9074406062262506\n",
      "-----Epoch:  460 -----\n",
      "Accuracy: 0.8926666666666667\n",
      "Cost: 0.9071471811844195\n",
      "-----Epoch:  470 -----\n",
      "Accuracy: 0.8933333333333333\n",
      "Cost: 0.9068641206898298\n",
      "-----Epoch:  480 -----\n",
      "Accuracy: 0.8938833333333334\n",
      "Cost: 0.9065914427150555\n",
      "-----Epoch:  490 -----\n",
      "Accuracy: 0.89445\n",
      "Cost: 0.9063279388174644\n",
      "-----Epoch:  500 -----\n",
      "Accuracy: 0.8950333333333333\n",
      "Cost: 0.9060724467965054\n",
      "-----Epoch:  510 -----\n",
      "Accuracy: 0.8954666666666666\n",
      "Cost: 0.9058240139171246\n",
      "-----Epoch:  520 -----\n",
      "Accuracy: 0.8961\n",
      "Cost: 0.9055821145656715\n",
      "-----Epoch:  530 -----\n",
      "Accuracy: 0.8969333333333334\n",
      "Cost: 0.9053466663579993\n",
      "-----Epoch:  540 -----\n",
      "Accuracy: 0.8977833333333334\n",
      "Cost: 0.9051171168934811\n",
      "-----Epoch:  550 -----\n",
      "Accuracy: 0.89845\n",
      "Cost: 0.9048930167331073\n",
      "-----Epoch:  560 -----\n",
      "Accuracy: 0.8989833333333334\n",
      "Cost: 0.9046736552408708\n",
      "-----Epoch:  570 -----\n",
      "Accuracy: 0.8993666666666666\n",
      "Cost: 0.9044596769756929\n",
      "-----Epoch:  580 -----\n",
      "Accuracy: 0.8999666666666667\n",
      "Cost: 0.9042508780789816\n",
      "-----Epoch:  590 -----\n",
      "Accuracy: 0.9006666666666666\n",
      "Cost: 0.9040469544791503\n",
      "-----Epoch:  600 -----\n",
      "Accuracy: 0.9012833333333333\n",
      "Cost: 0.9038476102837388\n",
      "-----Epoch:  610 -----\n",
      "Accuracy: 0.9019666666666667\n",
      "Cost: 0.9036499154170268\n",
      "-----Epoch:  620 -----\n",
      "Accuracy: 0.9023833333333333\n",
      "Cost: 0.9034558366221624\n",
      "-----Epoch:  630 -----\n",
      "Accuracy: 0.90295\n",
      "Cost: 0.9032641341386186\n",
      "-----Epoch:  640 -----\n",
      "Accuracy: 0.90355\n",
      "Cost: 0.9030763198568869\n",
      "-----Epoch:  650 -----\n",
      "Accuracy: 0.9041333333333333\n",
      "Cost: 0.9028921978966349\n",
      "-----Epoch:  660 -----\n",
      "Accuracy: 0.9048\n",
      "Cost: 0.9027117462982968\n",
      "-----Epoch:  670 -----\n",
      "Accuracy: 0.9051\n",
      "Cost: 0.902534995824392\n",
      "-----Epoch:  680 -----\n",
      "Accuracy: 0.9057\n",
      "Cost: 0.9023619837873216\n",
      "-----Epoch:  690 -----\n",
      "Accuracy: 0.9061\n",
      "Cost: 0.9021924396403375\n",
      "-----Epoch:  700 -----\n",
      "Accuracy: 0.9066666666666666\n",
      "Cost: 0.9020262714890922\n",
      "-----Epoch:  710 -----\n",
      "Accuracy: 0.9072833333333333\n",
      "Cost: 0.9018633542436562\n",
      "-----Epoch:  720 -----\n",
      "Accuracy: 0.9078\n",
      "Cost: 0.9017034606993832\n",
      "-----Epoch:  730 -----\n",
      "Accuracy: 0.9081833333333333\n",
      "Cost: 0.901546505969944\n",
      "-----Epoch:  740 -----\n",
      "Accuracy: 0.9085833333333333\n",
      "Cost: 0.9013924870186268\n",
      "-----Epoch:  750 -----\n",
      "Accuracy: 0.9090833333333334\n",
      "Cost: 0.901241381412989\n",
      "-----Epoch:  760 -----\n",
      "Accuracy: 0.9095333333333333\n",
      "Cost: 0.9010929231861503\n",
      "-----Epoch:  770 -----\n",
      "Accuracy: 0.9100333333333334\n",
      "Cost: 0.900947230353861\n",
      "-----Epoch:  780 -----\n",
      "Accuracy: 0.9104333333333333\n",
      "Cost: 0.9008040797327842\n",
      "-----Epoch:  790 -----\n",
      "Accuracy: 0.9108666666666667\n",
      "Cost: 0.9006632118540491\n",
      "-----Epoch:  800 -----\n",
      "Accuracy: 0.9112333333333333\n",
      "Cost: 0.9005245413316377\n",
      "-----Epoch:  810 -----\n",
      "Accuracy: 0.91155\n",
      "Cost: 0.9003880330882188\n",
      "-----Epoch:  820 -----\n",
      "Accuracy: 0.9117\n",
      "Cost: 0.9002537804370896\n",
      "-----Epoch:  830 -----\n",
      "Accuracy: 0.9120333333333334\n",
      "Cost: 0.9001217732062669\n",
      "-----Epoch:  840 -----\n",
      "Accuracy: 0.9124666666666666\n",
      "Cost: 0.8999919280346613\n",
      "-----Epoch:  850 -----\n",
      "Accuracy: 0.9128333333333334\n",
      "Cost: 0.8998641400293758\n",
      "-----Epoch:  860 -----\n",
      "Accuracy: 0.9133166666666667\n",
      "Cost: 0.8997385008453811\n",
      "-----Epoch:  870 -----\n",
      "Accuracy: 0.91375\n",
      "Cost: 0.89961494331418\n",
      "-----Epoch:  880 -----\n",
      "Accuracy: 0.91405\n",
      "Cost: 0.8994933522745304\n",
      "-----Epoch:  890 -----\n",
      "Accuracy: 0.9144333333333333\n",
      "Cost: 0.8993737467384362\n",
      "-----Epoch:  900 -----\n",
      "Accuracy: 0.9148\n",
      "Cost: 0.8992548532476656\n",
      "-----Epoch:  910 -----\n",
      "Accuracy: 0.9152333333333333\n",
      "Cost: 0.8991373488156119\n",
      "-----Epoch:  920 -----\n",
      "Accuracy: 0.9155833333333333\n",
      "Cost: 0.8990217196071603\n",
      "-----Epoch:  930 -----\n",
      "Accuracy: 0.9159666666666667\n",
      "Cost: 0.8989077596684901\n",
      "-----Epoch:  940 -----\n",
      "Accuracy: 0.9163333333333333\n",
      "Cost: 0.8987954819728846\n",
      "-----Epoch:  950 -----\n",
      "Accuracy: 0.9167333333333333\n",
      "Cost: 0.8986847583956709\n",
      "-----Epoch:  960 -----\n",
      "Accuracy: 0.9171666666666667\n",
      "Cost: 0.8985756215837183\n",
      "-----Epoch:  970 -----\n",
      "Accuracy: 0.9175166666666666\n",
      "Cost: 0.8984676640483903\n",
      "-----Epoch:  980 -----\n",
      "Accuracy: 0.91785\n",
      "Cost: 0.8983610900100959\n",
      "-----Epoch:  990 -----\n",
      "Accuracy: 0.91805\n",
      "Cost: 0.8982558093141347\n",
      "-----Epoch:  1000 -----\n",
      "Accuracy: 0.9183\n",
      "Cost: 0.8981517745929437\n",
      "-----Epoch:  1010 -----\n",
      "Accuracy: 0.91845\n",
      "Cost: 0.898048974008282\n",
      "-----Epoch:  1020 -----\n",
      "Accuracy: 0.9188166666666666\n",
      "Cost: 0.8979474122368025\n",
      "-----Epoch:  1030 -----\n",
      "Accuracy: 0.91905\n",
      "Cost: 0.8978470546207242\n",
      "-----Epoch:  1040 -----\n",
      "Accuracy: 0.91945\n",
      "Cost: 0.8977478547430252\n",
      "-----Epoch:  1050 -----\n",
      "Accuracy: 0.9197666666666666\n",
      "Cost: 0.8976498532182491\n",
      "-----Epoch:  1060 -----\n",
      "Accuracy: 0.9199666666666667\n",
      "Cost: 0.8975530021292107\n",
      "-----Epoch:  1070 -----\n",
      "Accuracy: 0.9201166666666667\n",
      "Cost: 0.8974572291299469\n",
      "-----Epoch:  1080 -----\n",
      "Accuracy: 0.9203666666666667\n",
      "Cost: 0.8973625816642417\n",
      "-----Epoch:  1090 -----\n",
      "Accuracy: 0.9205666666666666\n",
      "Cost: 0.8972690410020577\n",
      "-----Epoch:  1100 -----\n",
      "Accuracy: 0.9207666666666666\n",
      "Cost: 0.897176596834448\n",
      "-----Epoch:  1110 -----\n",
      "Accuracy: 0.92095\n",
      "Cost: 0.8970852771432799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  1120 -----\n",
      "Accuracy: 0.9211166666666667\n",
      "Cost: 0.8969950933757889\n",
      "-----Epoch:  1130 -----\n",
      "Accuracy: 0.9213833333333333\n",
      "Cost: 0.8969059696043845\n",
      "-----Epoch:  1140 -----\n",
      "Accuracy: 0.9216166666666666\n",
      "Cost: 0.8968178192898236\n",
      "-----Epoch:  1150 -----\n",
      "Accuracy: 0.9219\n",
      "Cost: 0.8967305909876926\n",
      "-----Epoch:  1160 -----\n",
      "Accuracy: 0.92215\n",
      "Cost: 0.8966442188457926\n",
      "-----Epoch:  1170 -----\n",
      "Accuracy: 0.9224666666666667\n",
      "Cost: 0.8965587502233902\n",
      "-----Epoch:  1180 -----\n",
      "Accuracy: 0.9226166666666666\n",
      "Cost: 0.8964741619479671\n",
      "-----Epoch:  1190 -----\n",
      "Accuracy: 0.9229166666666667\n",
      "Cost: 0.8963904228775983\n",
      "-----Epoch:  1200 -----\n",
      "Accuracy: 0.92315\n",
      "Cost: 0.8963075975413127\n",
      "-----Epoch:  1210 -----\n",
      "Accuracy: 0.9233\n",
      "Cost: 0.8962256434704728\n",
      "-----Epoch:  1220 -----\n",
      "Accuracy: 0.9235333333333333\n",
      "Cost: 0.8961445165489202\n",
      "-----Epoch:  1230 -----\n",
      "Accuracy: 0.9237333333333333\n",
      "Cost: 0.8960641535189456\n",
      "-----Epoch:  1240 -----\n",
      "Accuracy: 0.9239833333333334\n",
      "Cost: 0.8959845692014519\n",
      "-----Epoch:  1250 -----\n",
      "Accuracy: 0.9241833333333334\n",
      "Cost: 0.8959057731265481\n",
      "-----Epoch:  1260 -----\n",
      "Accuracy: 0.92445\n",
      "Cost: 0.8958277100966929\n",
      "-----Epoch:  1270 -----\n",
      "Accuracy: 0.9248\n",
      "Cost: 0.8957504124159626\n",
      "-----Epoch:  1280 -----\n",
      "Accuracy: 0.9250833333333334\n",
      "Cost: 0.8956738113211156\n",
      "-----Epoch:  1290 -----\n",
      "Accuracy: 0.9253333333333333\n",
      "Cost: 0.8955979756868163\n",
      "-----Epoch:  1300 -----\n",
      "Accuracy: 0.9255833333333333\n",
      "Cost: 0.8955228978150789\n",
      "-----Epoch:  1310 -----\n",
      "Accuracy: 0.9257666666666666\n",
      "Cost: 0.8954485552867033\n",
      "-----Epoch:  1320 -----\n",
      "Accuracy: 0.9259\n",
      "Cost: 0.8953749258346668\n",
      "-----Epoch:  1330 -----\n",
      "Accuracy: 0.9262166666666667\n",
      "Cost: 0.8953019702627637\n",
      "-----Epoch:  1340 -----\n",
      "Accuracy: 0.9265\n",
      "Cost: 0.8952296588360474\n",
      "-----Epoch:  1350 -----\n",
      "Accuracy: 0.9266833333333333\n",
      "Cost: 0.8951579634914172\n",
      "-----Epoch:  1360 -----\n",
      "Accuracy: 0.9269333333333334\n",
      "Cost: 0.8950863934954877\n",
      "-----Epoch:  1370 -----\n",
      "Accuracy: 0.92715\n",
      "Cost: 0.89501531229048\n",
      "-----Epoch:  1380 -----\n",
      "Accuracy: 0.9273333333333333\n",
      "Cost: 0.8949448234153773\n",
      "-----Epoch:  1390 -----\n",
      "Accuracy: 0.9276333333333333\n",
      "Cost: 0.894874922273922\n",
      "-----Epoch:  1400 -----\n",
      "Accuracy: 0.928\n",
      "Cost: 0.8948056398395655\n",
      "-----Epoch:  1410 -----\n",
      "Accuracy: 0.9282\n",
      "Cost: 0.8947369314345159\n",
      "-----Epoch:  1420 -----\n",
      "Accuracy: 0.92835\n",
      "Cost: 0.8946687264991913\n",
      "-----Epoch:  1430 -----\n",
      "Accuracy: 0.9284333333333333\n",
      "Cost: 0.8946010651490822\n",
      "-----Epoch:  1440 -----\n",
      "Accuracy: 0.9285333333333333\n",
      "Cost: 0.8945339326148617\n",
      "-----Epoch:  1450 -----\n",
      "Accuracy: 0.9287\n",
      "Cost: 0.8944673437405041\n",
      "-----Epoch:  1460 -----\n",
      "Accuracy: 0.9289166666666666\n",
      "Cost: 0.8944013161216662\n",
      "-----Epoch:  1470 -----\n",
      "Accuracy: 0.9290833333333334\n",
      "Cost: 0.8943358553911092\n",
      "-----Epoch:  1480 -----\n",
      "Accuracy: 0.9292\n",
      "Cost: 0.8942709264149078\n",
      "-----Epoch:  1490 -----\n",
      "Accuracy: 0.9294166666666667\n",
      "Cost: 0.8942064873877231\n",
      "-----Epoch:  1500 -----\n",
      "Accuracy: 0.9296166666666666\n",
      "Cost: 0.8941425756825918\n",
      "-----Epoch:  1510 -----\n",
      "Accuracy: 0.9298166666666666\n",
      "Cost: 0.8940791681404364\n",
      "-----Epoch:  1520 -----\n",
      "Accuracy: 0.9300166666666667\n",
      "Cost: 0.8940162864449435\n",
      "-----Epoch:  1530 -----\n",
      "Accuracy: 0.9301333333333334\n",
      "Cost: 0.8939538951462649\n",
      "-----Epoch:  1540 -----\n",
      "Accuracy: 0.9303333333333333\n",
      "Cost: 0.8938919468259151\n",
      "-----Epoch:  1550 -----\n",
      "Accuracy: 0.9305166666666667\n",
      "Cost: 0.893830477146906\n",
      "-----Epoch:  1560 -----\n",
      "Accuracy: 0.9307333333333333\n",
      "Cost: 0.8937694698085799\n",
      "-----Epoch:  1570 -----\n",
      "Accuracy: 0.93085\n",
      "Cost: 0.8937089262563506\n",
      "-----Epoch:  1580 -----\n",
      "Accuracy: 0.9309666666666667\n",
      "Cost: 0.8936488253652819\n",
      "-----Epoch:  1590 -----\n",
      "Accuracy: 0.9311333333333334\n",
      "Cost: 0.8935892001499672\n",
      "-----Epoch:  1600 -----\n",
      "Accuracy: 0.9313\n",
      "Cost: 0.8935300381163788\n",
      "-----Epoch:  1610 -----\n",
      "Accuracy: 0.9314166666666667\n",
      "Cost: 0.8934713680904652\n",
      "-----Epoch:  1620 -----\n",
      "Accuracy: 0.9316166666666666\n",
      "Cost: 0.8934131402241077\n",
      "-----Epoch:  1630 -----\n",
      "Accuracy: 0.93175\n",
      "Cost: 0.8933553717964614\n",
      "-----Epoch:  1640 -----\n",
      "Accuracy: 0.9319333333333333\n",
      "Cost: 0.8932980027532529\n",
      "-----Epoch:  1650 -----\n",
      "Accuracy: 0.9321\n",
      "Cost: 0.8932410334910496\n",
      "-----Epoch:  1660 -----\n",
      "Accuracy: 0.9323833333333333\n",
      "Cost: 0.8931844357750065\n",
      "-----Epoch:  1670 -----\n",
      "Accuracy: 0.9325166666666667\n",
      "Cost: 0.8931282120248027\n",
      "-----Epoch:  1680 -----\n",
      "Accuracy: 0.9327666666666666\n",
      "Cost: 0.8930723750846331\n",
      "-----Epoch:  1690 -----\n",
      "Accuracy: 0.9329333333333333\n",
      "Cost: 0.8930168803125103\n",
      "-----Epoch:  1700 -----\n",
      "Accuracy: 0.93305\n",
      "Cost: 0.8929617388320744\n",
      "-----Epoch:  1710 -----\n",
      "Accuracy: 0.9333166666666667\n",
      "Cost: 0.8929069754020974\n",
      "-----Epoch:  1720 -----\n",
      "Accuracy: 0.9335333333333333\n",
      "Cost: 0.8928525646830444\n",
      "-----Epoch:  1730 -----\n",
      "Accuracy: 0.9337166666666666\n",
      "Cost: 0.8927985680398866\n",
      "-----Epoch:  1740 -----\n",
      "Accuracy: 0.9338666666666666\n",
      "Cost: 0.892744939019428\n",
      "-----Epoch:  1750 -----\n",
      "Accuracy: 0.9340666666666667\n",
      "Cost: 0.8926917257931926\n",
      "-----Epoch:  1760 -----\n",
      "Accuracy: 0.9341333333333334\n",
      "Cost: 0.8926388833479274\n",
      "-----Epoch:  1770 -----\n",
      "Accuracy: 0.9343333333333333\n",
      "Cost: 0.8925863626390392\n",
      "-----Epoch:  1780 -----\n",
      "Accuracy: 0.93445\n",
      "Cost: 0.8925341609431007\n",
      "-----Epoch:  1790 -----\n",
      "Accuracy: 0.9346166666666667\n",
      "Cost: 0.8924823067122033\n",
      "-----Epoch:  1800 -----\n",
      "Accuracy: 0.9347666666666666\n",
      "Cost: 0.892430786860555\n",
      "-----Epoch:  1810 -----\n",
      "Accuracy: 0.9349\n",
      "Cost: 0.8923794966562901\n",
      "-----Epoch:  1820 -----\n",
      "Accuracy: 0.9350833333333334\n",
      "Cost: 0.8923285226595478\n",
      "-----Epoch:  1830 -----\n",
      "Accuracy: 0.9351333333333334\n",
      "Cost: 0.8922779086694762\n",
      "-----Epoch:  1840 -----\n",
      "Accuracy: 0.9352666666666667\n",
      "Cost: 0.8922276516645329\n",
      "-----Epoch:  1850 -----\n",
      "Accuracy: 0.9353666666666667\n",
      "Cost: 0.8921777464543429\n",
      "-----Epoch:  1860 -----\n",
      "Accuracy: 0.9354666666666667\n",
      "Cost: 0.8921281524419128\n",
      "-----Epoch:  1870 -----\n",
      "Accuracy: 0.93555\n",
      "Cost: 0.8920788569253602\n",
      "-----Epoch:  1880 -----\n",
      "Accuracy: 0.9356833333333333\n",
      "Cost: 0.89202985890155\n",
      "-----Epoch:  1890 -----\n",
      "Accuracy: 0.93575\n",
      "Cost: 0.8919811425365786\n",
      "-----Epoch:  1900 -----\n",
      "Accuracy: 0.936\n",
      "Cost: 0.8919326683747552\n",
      "-----Epoch:  1910 -----\n",
      "Accuracy: 0.9362\n",
      "Cost: 0.8918844923742657\n",
      "-----Epoch:  1920 -----\n",
      "Accuracy: 0.93625\n",
      "Cost: 0.8918366230347237\n",
      "-----Epoch:  1930 -----\n",
      "Accuracy: 0.9364166666666667\n",
      "Cost: 0.8917890540445488\n",
      "-----Epoch:  1940 -----\n",
      "Accuracy: 0.9365333333333333\n",
      "Cost: 0.8917417782941147\n",
      "-----Epoch:  1950 -----\n",
      "Accuracy: 0.9366833333333333\n",
      "Cost: 0.8916948211599066\n",
      "-----Epoch:  1960 -----\n",
      "Accuracy: 0.9368166666666666\n",
      "Cost: 0.8916481704145052\n",
      "-----Epoch:  1970 -----\n",
      "Accuracy: 0.9369\n",
      "Cost: 0.8916018074442728\n",
      "-----Epoch:  1980 -----\n",
      "Accuracy: 0.9370333333333334\n",
      "Cost: 0.8915557142504222\n",
      "-----Epoch:  1990 -----\n",
      "Accuracy: 0.9371166666666667\n",
      "Cost: 0.8915099042330473\n"
     ]
    }
   ],
   "source": [
    "mnist_net = NeuralNet('multi_logloss')\n",
    "mnist_net.addLayer(64, 'reLu')\n",
    "mnist_net.addLayer(10, 'softmax')\n",
    "costs, mnist_weights = mnist_net.fit(X_train, y_train, 2000, 60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_costs(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.75\n"
     ]
    }
   ],
   "source": [
    "get_test_accuracy(mnist_net, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset With 1 Hidden Layer + Batches (size 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  0 -----\n",
      "Accuracy: 0.082\n",
      "Cost: 1.8670070512182189\n",
      "-----Epoch:  10 -----\n",
      "Accuracy: 0.914\n",
      "Cost: 0.4131290202732777\n",
      "-----Epoch:  20 -----\n",
      "Accuracy: 0.95\n",
      "Cost: 0.4071646881766138\n",
      "-----Epoch:  30 -----\n",
      "Accuracy: 0.964\n",
      "Cost: 0.4038190533468727\n",
      "-----Epoch:  40 -----\n",
      "Accuracy: 0.972\n",
      "Cost: 0.4017133983554123\n",
      "-----Epoch:  50 -----\n",
      "Accuracy: 0.976\n",
      "Cost: 0.4002542345086098\n",
      "-----Epoch:  60 -----\n",
      "Accuracy: 0.978\n",
      "Cost: 0.39914979751896873\n",
      "-----Epoch:  70 -----\n",
      "Accuracy: 0.984\n",
      "Cost: 0.39823904259612075\n",
      "-----Epoch:  80 -----\n",
      "Accuracy: 0.984\n",
      "Cost: 0.39747688331145065\n",
      "-----Epoch:  90 -----\n",
      "Accuracy: 0.984\n",
      "Cost: 0.3968398622594358\n",
      "-----Epoch:  100 -----\n",
      "Accuracy: 0.984\n",
      "Cost: 0.3963591773502795\n",
      "-----Epoch:  110 -----\n",
      "Accuracy: 0.988\n",
      "Cost: 0.39592973532992265\n",
      "-----Epoch:  120 -----\n",
      "Accuracy: 0.988\n",
      "Cost: 0.39552235627921084\n",
      "-----Epoch:  130 -----\n",
      "Accuracy: 0.988\n",
      "Cost: 0.39516122783506447\n",
      "-----Epoch:  140 -----\n",
      "Accuracy: 0.99\n",
      "Cost: 0.394804468355694\n",
      "-----Epoch:  150 -----\n",
      "Accuracy: 0.99\n",
      "Cost: 0.3945006966802894\n",
      "-----Epoch:  160 -----\n",
      "Accuracy: 0.992\n",
      "Cost: 0.39422935196270004\n",
      "-----Epoch:  170 -----\n",
      "Accuracy: 0.992\n",
      "Cost: 0.3939787248589921\n",
      "-----Epoch:  180 -----\n",
      "Accuracy: 0.994\n",
      "Cost: 0.3937445705485291\n",
      "-----Epoch:  190 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 0.393519617444778\n",
      "-----Epoch:  200 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 0.39331910285685706\n",
      "-----Epoch:  210 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 0.3931346056855205\n",
      "-----Epoch:  220 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 0.3929608519566249\n",
      "-----Epoch:  230 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 0.3927790710816083\n",
      "-----Epoch:  240 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 0.3926100163676923\n",
      "-----Epoch:  250 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 0.3924550660595893\n",
      "-----Epoch:  260 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 0.3923226105497165\n",
      "-----Epoch:  270 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 0.39221747084358016\n",
      "-----Epoch:  280 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 0.392136234723109\n",
      "-----Epoch:  290 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3920695481155472\n",
      "-----Epoch:  300 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3920185618844717\n",
      "-----Epoch:  310 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3919768927711962\n",
      "-----Epoch:  320 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39194373168067465\n",
      "-----Epoch:  330 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39191393310156897\n",
      "-----Epoch:  340 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39188910530717647\n",
      "-----Epoch:  350 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3918668432879036\n",
      "-----Epoch:  360 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39184611490206833\n",
      "-----Epoch:  370 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3918277539963804\n",
      "-----Epoch:  380 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39181103212694957\n",
      "-----Epoch:  390 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39179588939904036\n",
      "-----Epoch:  400 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39178128640430254\n",
      "-----Epoch:  410 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3917684436912326\n",
      "-----Epoch:  420 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3917565913552325\n",
      "-----Epoch:  430 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39174584259622264\n",
      "-----Epoch:  440 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3917358283948676\n",
      "-----Epoch:  450 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3917269224320245\n",
      "-----Epoch:  460 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3917183487041227\n",
      "-----Epoch:  470 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3917107083339462\n",
      "-----Epoch:  480 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3917037308072266\n",
      "-----Epoch:  490 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916972752553898\n",
      "-----Epoch:  500 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39169141100235083\n",
      "-----Epoch:  510 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916859912065161\n",
      "-----Epoch:  520 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39168070861419846\n",
      "-----Epoch:  530 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916759293800016\n",
      "-----Epoch:  540 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916713711404572\n",
      "-----Epoch:  550 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39166707427895797\n",
      "-----Epoch:  560 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39166315028826826\n",
      "-----Epoch:  570 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916594546412097\n",
      "-----Epoch:  580 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916560096599984\n",
      "-----Epoch:  590 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916527069067114\n",
      "-----Epoch:  600 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916496582776737\n",
      "-----Epoch:  610 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39164676264161746\n",
      "-----Epoch:  620 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916440796806611\n",
      "-----Epoch:  630 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916414978796676\n",
      "-----Epoch:  640 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.391639110975721\n",
      "-----Epoch:  650 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39163681069102135\n",
      "-----Epoch:  660 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916346512967226\n",
      "-----Epoch:  670 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39163260079449574\n",
      "-----Epoch:  680 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39163067176707705\n",
      "-----Epoch:  690 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39162886775188005\n",
      "-----Epoch:  700 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.391627174818023\n",
      "-----Epoch:  710 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39162552669926853\n",
      "-----Epoch:  720 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916239507702406\n",
      "-----Epoch:  730 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.391622489512835\n",
      "-----Epoch:  740 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39162101828750057\n",
      "-----Epoch:  750 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916196354938261\n",
      "-----Epoch:  760 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916183134353388\n",
      "-----Epoch:  770 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39161707756312897\n",
      "-----Epoch:  780 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916158574547092\n",
      "-----Epoch:  790 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39161469356382117\n",
      "-----Epoch:  800 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916135811280702\n",
      "-----Epoch:  810 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916125010040444\n",
      "-----Epoch:  820 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916114724451766\n",
      "-----Epoch:  830 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39161048830182577\n",
      "-----Epoch:  840 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39160952977151475\n",
      "-----Epoch:  850 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916086310981056\n",
      "-----Epoch:  860 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916077535904259\n",
      "-----Epoch:  870 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916069008116859\n",
      "-----Epoch:  880 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.391606081607484\n",
      "-----Epoch:  890 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916053036958438\n",
      "-----Epoch:  900 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39160454264519273\n",
      "-----Epoch:  910 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39160378760108744\n",
      "-----Epoch:  920 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916030790695855\n",
      "-----Epoch:  930 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916023834014052\n",
      "-----Epoch:  940 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39160171813005246\n",
      "-----Epoch:  950 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.3916010879802779\n",
      "-----Epoch:  960 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39160045210966576\n",
      "-----Epoch:  970 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39159983775988566\n",
      "-----Epoch:  980 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.39159923782627887\n",
      "-----Epoch:  990 -----\n",
      "Accuracy: 1.0\n",
      "Cost: 0.391598673373063\n"
     ]
    }
   ],
   "source": [
    "mnist_net_batches = NeuralNet('multi_logloss')\n",
    "mnist_net_batches.addLayer(64, 'reLu')\n",
    "mnist_net_batches.addLayer(10, 'softmax')\n",
    "costs, weights_batches = mnist_net_batches.fit(X_train, y_train, 1000, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x207923ee160>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_costs(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-7fa85942c107>:22: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(Z) / sum(np.exp(Z))\n",
      "<ipython-input-2-7fa85942c107>:22: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(Z) / sum(np.exp(Z))\n"
     ]
    }
   ],
   "source": [
    "get_test_accuracy(mnist_net_batches, , X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset With 2 Hidden Layer + No Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one takes forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  0 -----\n",
      "Accuracy: 0.0841\n",
      "Cost: 2.8222368903670825\n",
      "-----Epoch:  10 -----\n",
      "Accuracy: 0.5922166666666666\n",
      "Cost: 1.1273622771816205\n",
      "-----Epoch:  20 -----\n",
      "Accuracy: 0.6584666666666666\n",
      "Cost: 1.043132122107689\n",
      "-----Epoch:  30 -----\n",
      "Accuracy: 0.6947666666666666\n",
      "Cost: 1.0067442745550457\n",
      "-----Epoch:  40 -----\n",
      "Accuracy: 0.7184333333333334\n",
      "Cost: 0.9863990696782855\n",
      "-----Epoch:  50 -----\n",
      "Accuracy: 0.73615\n",
      "Cost: 0.973303536713376\n",
      "-----Epoch:  60 -----\n",
      "Accuracy: 0.75145\n",
      "Cost: 0.9641254855392057\n",
      "-----Epoch:  70 -----\n",
      "Accuracy: 0.7639666666666667\n",
      "Cost: 0.9572808897760625\n",
      "-----Epoch:  80 -----\n",
      "Accuracy: 0.77415\n",
      "Cost: 0.9518831416965255\n",
      "-----Epoch:  90 -----\n",
      "Accuracy: 0.78455\n",
      "Cost: 0.9474808204216365\n",
      "-----Epoch:  100 -----\n",
      "Accuracy: 0.7933666666666667\n",
      "Cost: 0.9437837393486436\n",
      "-----Epoch:  110 -----\n",
      "Accuracy: 0.8008833333333333\n",
      "Cost: 0.94062947604732\n",
      "-----Epoch:  120 -----\n",
      "Accuracy: 0.8022666666666667\n",
      "Cost: 0.9390361356779319\n",
      "-----Epoch:  130 -----\n",
      "Accuracy: 0.8104166666666667\n",
      "Cost: 0.9363409145281736\n",
      "-----Epoch:  140 -----\n",
      "Accuracy: 0.8183666666666667\n",
      "Cost: 0.9336669482751636\n",
      "-----Epoch:  150 -----\n",
      "Accuracy: 0.8231166666666667\n",
      "Cost: 0.9318060830876397\n",
      "-----Epoch:  160 -----\n",
      "Accuracy: 0.8268833333333333\n",
      "Cost: 0.930173913394029\n",
      "-----Epoch:  170 -----\n",
      "Accuracy: 0.8312166666666667\n",
      "Cost: 0.9285106010518218\n",
      "-----Epoch:  180 -----\n",
      "Accuracy: 0.8343666666666667\n",
      "Cost: 0.9269599939080088\n",
      "-----Epoch:  190 -----\n",
      "Accuracy: 0.8376833333333333\n",
      "Cost: 0.9255910647465373\n",
      "-----Epoch:  200 -----\n",
      "Accuracy: 0.8403833333333334\n",
      "Cost: 0.9243568518570221\n",
      "-----Epoch:  210 -----\n",
      "Accuracy: 0.8432833333333334\n",
      "Cost: 0.9232102537649475\n",
      "-----Epoch:  220 -----\n",
      "Accuracy: 0.8462833333333334\n",
      "Cost: 0.9221223800291674\n",
      "-----Epoch:  230 -----\n",
      "Accuracy: 0.8492833333333333\n",
      "Cost: 0.921068218482931\n",
      "-----Epoch:  240 -----\n",
      "Accuracy: 0.8522666666666666\n",
      "Cost: 0.9200554754731324\n",
      "-----Epoch:  250 -----\n",
      "Accuracy: 0.85505\n",
      "Cost: 0.9191006172377115\n",
      "-----Epoch:  260 -----\n",
      "Accuracy: 0.8575666666666667\n",
      "Cost: 0.918215760901867\n",
      "-----Epoch:  270 -----\n",
      "Accuracy: 0.8592\n",
      "Cost: 0.917386957430329\n",
      "-----Epoch:  280 -----\n",
      "Accuracy: 0.8614166666666667\n",
      "Cost: 0.9165986224585303\n",
      "-----Epoch:  290 -----\n",
      "Accuracy: 0.8635833333333334\n",
      "Cost: 0.9158429853458858\n",
      "-----Epoch:  300 -----\n",
      "Accuracy: 0.8655666666666667\n",
      "Cost: 0.9151216884155554\n",
      "-----Epoch:  310 -----\n",
      "Accuracy: 0.8674166666666666\n",
      "Cost: 0.9144332141245952\n",
      "-----Epoch:  320 -----\n",
      "Accuracy: 0.8694166666666666\n",
      "Cost: 0.9137769517470582\n",
      "-----Epoch:  330 -----\n",
      "Accuracy: 0.8712833333333333\n",
      "Cost: 0.9131505175157251\n",
      "-----Epoch:  340 -----\n",
      "Accuracy: 0.8730166666666667\n",
      "Cost: 0.9125490518427107\n",
      "-----Epoch:  350 -----\n",
      "Accuracy: 0.8748833333333333\n",
      "Cost: 0.9119684262359397\n",
      "-----Epoch:  360 -----\n",
      "Accuracy: 0.8767333333333334\n",
      "Cost: 0.9114034046117011\n",
      "-----Epoch:  370 -----\n",
      "Accuracy: 0.8781333333333333\n",
      "Cost: 0.9108568595441819\n",
      "-----Epoch:  380 -----\n",
      "Accuracy: 0.88005\n",
      "Cost: 0.9103329822811679\n",
      "-----Epoch:  390 -----\n",
      "Accuracy: 0.8817166666666667\n",
      "Cost: 0.9098313103422261\n",
      "-----Epoch:  400 -----\n",
      "Accuracy: 0.8831\n",
      "Cost: 0.9093535378308131\n",
      "-----Epoch:  410 -----\n",
      "Accuracy: 0.8841833333333333\n",
      "Cost: 0.9088943227849946\n",
      "-----Epoch:  420 -----\n",
      "Accuracy: 0.8855333333333333\n",
      "Cost: 0.9084485264606968\n",
      "-----Epoch:  430 -----\n",
      "Accuracy: 0.8867\n",
      "Cost: 0.9080141097052861\n",
      "-----Epoch:  440 -----\n",
      "Accuracy: 0.88805\n",
      "Cost: 0.907589532077954\n",
      "-----Epoch:  450 -----\n",
      "Accuracy: 0.8891166666666667\n",
      "Cost: 0.9071767985224796\n",
      "-----Epoch:  460 -----\n",
      "Accuracy: 0.8902833333333333\n",
      "Cost: 0.9067772912804494\n",
      "-----Epoch:  470 -----\n",
      "Accuracy: 0.8913166666666666\n",
      "Cost: 0.9063935871688841\n",
      "-----Epoch:  480 -----\n",
      "Accuracy: 0.8922833333333333\n",
      "Cost: 0.9060221359498223\n",
      "-----Epoch:  490 -----\n",
      "Accuracy: 0.8933666666666666\n",
      "Cost: 0.9056621953169509\n",
      "-----Epoch:  500 -----\n",
      "Accuracy: 0.8945333333333333\n",
      "Cost: 0.9053133207499995\n",
      "-----Epoch:  510 -----\n",
      "Accuracy: 0.89565\n",
      "Cost: 0.9049728981893355\n",
      "-----Epoch:  520 -----\n",
      "Accuracy: 0.8966333333333333\n",
      "Cost: 0.9046394163120345\n",
      "-----Epoch:  530 -----\n",
      "Accuracy: 0.8975166666666666\n",
      "Cost: 0.904314406374387\n",
      "-----Epoch:  540 -----\n",
      "Accuracy: 0.8985666666666666\n",
      "Cost: 0.9039983402280523\n",
      "-----Epoch:  550 -----\n",
      "Accuracy: 0.8993333333333333\n",
      "Cost: 0.9036906633969018\n",
      "-----Epoch:  560 -----\n",
      "Accuracy: 0.8999166666666667\n",
      "Cost: 0.9033896208264067\n",
      "-----Epoch:  570 -----\n",
      "Accuracy: 0.90035\n",
      "Cost: 0.9030957700413192\n",
      "-----Epoch:  580 -----\n",
      "Accuracy: 0.9010166666666667\n",
      "Cost: 0.9028086876228217\n",
      "-----Epoch:  590 -----\n",
      "Accuracy: 0.9016666666666666\n",
      "Cost: 0.9025300662787372\n",
      "-----Epoch:  600 -----\n",
      "Accuracy: 0.9022833333333333\n",
      "Cost: 0.9022582297105327\n",
      "-----Epoch:  610 -----\n",
      "Accuracy: 0.9032666666666667\n",
      "Cost: 0.9019938034602567\n",
      "-----Epoch:  620 -----\n",
      "Accuracy: 0.9040166666666667\n",
      "Cost: 0.9017352732665866\n",
      "-----Epoch:  630 -----\n",
      "Accuracy: 0.9048833333333334\n",
      "Cost: 0.901482083830718\n",
      "-----Epoch:  640 -----\n",
      "Accuracy: 0.90565\n",
      "Cost: 0.9012338487032797\n",
      "-----Epoch:  650 -----\n",
      "Accuracy: 0.90625\n",
      "Cost: 0.9009909110644966\n",
      "-----Epoch:  660 -----\n",
      "Accuracy: 0.90695\n",
      "Cost: 0.9007530807353494\n",
      "-----Epoch:  670 -----\n",
      "Accuracy: 0.9074333333333333\n",
      "Cost: 0.9005208265461684\n",
      "-----Epoch:  680 -----\n",
      "Accuracy: 0.9081166666666667\n",
      "Cost: 0.9002948216818601\n",
      "-----Epoch:  690 -----\n",
      "Accuracy: 0.9086666666666666\n",
      "Cost: 0.9000739998833056\n",
      "-----Epoch:  700 -----\n",
      "Accuracy: 0.9095\n",
      "Cost: 0.8998587318181115\n",
      "-----Epoch:  710 -----\n",
      "Accuracy: 0.9100833333333334\n",
      "Cost: 0.8996495039497874\n",
      "-----Epoch:  720 -----\n",
      "Accuracy: 0.9104833333333333\n",
      "Cost: 0.8994463716692862\n",
      "-----Epoch:  730 -----\n",
      "Accuracy: 0.9110166666666667\n",
      "Cost: 0.8992494630105732\n",
      "-----Epoch:  740 -----\n",
      "Accuracy: 0.9115666666666666\n",
      "Cost: 0.8990591568036531\n",
      "-----Epoch:  750 -----\n",
      "Accuracy: 0.9118166666666667\n",
      "Cost: 0.8988785697361428\n",
      "-----Epoch:  760 -----\n",
      "Accuracy: 0.9124833333333333\n",
      "Cost: 0.8987111860951956\n",
      "-----Epoch:  770 -----\n",
      "Accuracy: 0.9127333333333333\n",
      "Cost: 0.8985578542938601\n",
      "-----Epoch:  780 -----\n",
      "Accuracy: 0.91315\n",
      "Cost: 0.8984067710348447\n",
      "-----Epoch:  790 -----\n",
      "Accuracy: 0.9136333333333333\n",
      "Cost: 0.8982355367485875\n",
      "-----Epoch:  800 -----\n",
      "Accuracy: 0.91395\n",
      "Cost: 0.898038925903873\n",
      "-----Epoch:  810 -----\n",
      "Accuracy: 0.9146666666666666\n",
      "Cost: 0.8978339352079319\n",
      "-----Epoch:  820 -----\n",
      "Accuracy: 0.9152166666666667\n",
      "Cost: 0.8976350871385081\n",
      "-----Epoch:  830 -----\n",
      "Accuracy: 0.9159166666666667\n",
      "Cost: 0.8974471704412027\n",
      "-----Epoch:  840 -----\n",
      "Accuracy: 0.9163666666666667\n",
      "Cost: 0.8972689161575946\n",
      "-----Epoch:  850 -----\n",
      "Accuracy: 0.91695\n",
      "Cost: 0.897097864090296\n",
      "-----Epoch:  860 -----\n",
      "Accuracy: 0.9176\n",
      "Cost: 0.8969318051906244\n",
      "-----Epoch:  870 -----\n",
      "Accuracy: 0.9180333333333334\n",
      "Cost: 0.8967704674242319\n",
      "-----Epoch:  880 -----\n",
      "Accuracy: 0.9183833333333333\n",
      "Cost: 0.8966127500278882\n",
      "-----Epoch:  890 -----\n",
      "Accuracy: 0.91895\n",
      "Cost: 0.8964580466810669\n",
      "-----Epoch:  900 -----\n",
      "Accuracy: 0.9194833333333333\n",
      "Cost: 0.896306192455695\n",
      "-----Epoch:  910 -----\n",
      "Accuracy: 0.9199666666666667\n",
      "Cost: 0.8961568924674235\n",
      "-----Epoch:  920 -----\n",
      "Accuracy: 0.9203833333333333\n",
      "Cost: 0.8960099401224333\n",
      "-----Epoch:  930 -----\n",
      "Accuracy: 0.9208833333333334\n",
      "Cost: 0.8958658511280762\n",
      "-----Epoch:  940 -----\n",
      "Accuracy: 0.9212333333333333\n",
      "Cost: 0.8957239855381026\n",
      "-----Epoch:  950 -----\n",
      "Accuracy: 0.92165\n",
      "Cost: 0.8955838800986194\n",
      "-----Epoch:  960 -----\n",
      "Accuracy: 0.9221333333333334\n",
      "Cost: 0.8954458483945814\n",
      "-----Epoch:  970 -----\n",
      "Accuracy: 0.9223333333333333\n",
      "Cost: 0.8953095547240769\n",
      "-----Epoch:  980 -----\n",
      "Accuracy: 0.9226666666666666\n",
      "Cost: 0.8951748892769668\n",
      "-----Epoch:  990 -----\n",
      "Accuracy: 0.9232166666666667\n",
      "Cost: 0.8950423445614094\n",
      "-----Epoch:  1000 -----\n",
      "Accuracy: 0.9236666666666666\n",
      "Cost: 0.8949117143004933\n",
      "-----Epoch:  1010 -----\n",
      "Accuracy: 0.9239333333333334\n",
      "Cost: 0.894783084416305\n",
      "-----Epoch:  1020 -----\n",
      "Accuracy: 0.92445\n",
      "Cost: 0.8946561366290221\n",
      "-----Epoch:  1030 -----\n",
      "Accuracy: 0.9247\n",
      "Cost: 0.8945309550384962\n",
      "-----Epoch:  1040 -----\n",
      "Accuracy: 0.9252833333333333\n",
      "Cost: 0.8944074143188955\n",
      "-----Epoch:  1050 -----\n",
      "Accuracy: 0.9257666666666666\n",
      "Cost: 0.8942853870379937\n",
      "-----Epoch:  1060 -----\n",
      "Accuracy: 0.9261166666666667\n",
      "Cost: 0.8941648178459446\n",
      "-----Epoch:  1070 -----\n",
      "Accuracy: 0.9266\n",
      "Cost: 0.8940457572047756\n",
      "-----Epoch:  1080 -----\n",
      "Accuracy: 0.9269666666666667\n",
      "Cost: 0.8939283321826389\n",
      "-----Epoch:  1090 -----\n",
      "Accuracy: 0.9273\n",
      "Cost: 0.8938123858578811\n",
      "-----Epoch:  1100 -----\n",
      "Accuracy: 0.9275833333333333\n",
      "Cost: 0.8936978067548264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  1110 -----\n",
      "Accuracy: 0.9277666666666666\n",
      "Cost: 0.8935845816554815\n",
      "-----Epoch:  1120 -----\n",
      "Accuracy: 0.92815\n",
      "Cost: 0.8934722956699694\n",
      "-----Epoch:  1130 -----\n",
      "Accuracy: 0.9286666666666666\n",
      "Cost: 0.8933609697358796\n",
      "-----Epoch:  1140 -----\n",
      "Accuracy: 0.9289666666666667\n",
      "Cost: 0.8932510212333156\n",
      "-----Epoch:  1150 -----\n",
      "Accuracy: 0.9295166666666667\n",
      "Cost: 0.8931421610554877\n",
      "-----Epoch:  1160 -----\n",
      "Accuracy: 0.9297\n",
      "Cost: 0.8930348704627118\n",
      "-----Epoch:  1170 -----\n",
      "Accuracy: 0.9299\n",
      "Cost: 0.8929289814127997\n",
      "-----Epoch:  1180 -----\n",
      "Accuracy: 0.9301333333333334\n",
      "Cost: 0.8928243546137911\n",
      "-----Epoch:  1190 -----\n",
      "Accuracy: 0.9304\n",
      "Cost: 0.8927208862594936\n",
      "-----Epoch:  1200 -----\n",
      "Accuracy: 0.9304833333333333\n",
      "Cost: 0.8926187489329185\n",
      "-----Epoch:  1210 -----\n",
      "Accuracy: 0.93075\n",
      "Cost: 0.8925179863169524\n",
      "-----Epoch:  1220 -----\n",
      "Accuracy: 0.9309\n",
      "Cost: 0.8924184896672595\n",
      "-----Epoch:  1230 -----\n",
      "Accuracy: 0.9313\n",
      "Cost: 0.8923199332012443\n",
      "-----Epoch:  1240 -----\n",
      "Accuracy: 0.9315666666666667\n",
      "Cost: 0.892222228593628\n",
      "-----Epoch:  1250 -----\n",
      "Accuracy: 0.9317833333333333\n",
      "Cost: 0.8921255717734489\n",
      "-----Epoch:  1260 -----\n",
      "Accuracy: 0.93205\n",
      "Cost: 0.8920301339948987\n",
      "-----Epoch:  1270 -----\n",
      "Accuracy: 0.9322666666666667\n",
      "Cost: 0.8919358580566762\n",
      "-----Epoch:  1280 -----\n",
      "Accuracy: 0.9326\n",
      "Cost: 0.8918427797060282\n",
      "-----Epoch:  1290 -----\n",
      "Accuracy: 0.9328333333333333\n",
      "Cost: 0.8917507983273242\n",
      "-----Epoch:  1300 -----\n",
      "Accuracy: 0.9331\n",
      "Cost: 0.8916595690503989\n",
      "-----Epoch:  1310 -----\n",
      "Accuracy: 0.9333666666666667\n",
      "Cost: 0.8915693905976604\n",
      "-----Epoch:  1320 -----\n",
      "Accuracy: 0.93365\n",
      "Cost: 0.8914804123953629\n",
      "-----Epoch:  1330 -----\n",
      "Accuracy: 0.93385\n",
      "Cost: 0.891392361466518\n",
      "-----Epoch:  1340 -----\n",
      "Accuracy: 0.9340666666666667\n",
      "Cost: 0.8913053389024655\n",
      "-----Epoch:  1350 -----\n",
      "Accuracy: 0.9342333333333334\n",
      "Cost: 0.8912193676789216\n",
      "-----Epoch:  1360 -----\n",
      "Accuracy: 0.9344333333333333\n",
      "Cost: 0.8911342662841821\n",
      "-----Epoch:  1370 -----\n",
      "Accuracy: 0.9348666666666666\n",
      "Cost: 0.8910498350976787\n",
      "-----Epoch:  1380 -----\n",
      "Accuracy: 0.9350833333333334\n",
      "Cost: 0.8909663925532143\n",
      "-----Epoch:  1390 -----\n",
      "Accuracy: 0.9353333333333333\n",
      "Cost: 0.8908837081808896\n",
      "-----Epoch:  1400 -----\n",
      "Accuracy: 0.9356\n",
      "Cost: 0.8908017296669402\n",
      "-----Epoch:  1410 -----\n",
      "Accuracy: 0.9358666666666666\n",
      "Cost: 0.8907204844878143\n",
      "-----Epoch:  1420 -----\n",
      "Accuracy: 0.9362\n",
      "Cost: 0.8906400977943923\n",
      "-----Epoch:  1430 -----\n",
      "Accuracy: 0.9363833333333333\n",
      "Cost: 0.8905604872060847\n",
      "-----Epoch:  1440 -----\n",
      "Accuracy: 0.9366\n",
      "Cost: 0.8904816101191382\n",
      "-----Epoch:  1450 -----\n",
      "Accuracy: 0.93685\n",
      "Cost: 0.8904035436173837\n",
      "-----Epoch:  1460 -----\n",
      "Accuracy: 0.9370666666666667\n",
      "Cost: 0.8903262736427698\n",
      "-----Epoch:  1470 -----\n",
      "Accuracy: 0.9373333333333334\n",
      "Cost: 0.8902496268809837\n",
      "-----Epoch:  1480 -----\n",
      "Accuracy: 0.9376\n",
      "Cost: 0.8901736712258831\n",
      "-----Epoch:  1490 -----\n",
      "Accuracy: 0.9378\n",
      "Cost: 0.8900984485070704\n",
      "-----Epoch:  1500 -----\n",
      "Accuracy: 0.9381\n",
      "Cost: 0.8900240053784568\n",
      "-----Epoch:  1510 -----\n",
      "Accuracy: 0.9383833333333333\n",
      "Cost: 0.889950184931909\n",
      "-----Epoch:  1520 -----\n",
      "Accuracy: 0.93855\n",
      "Cost: 0.8898770456681616\n",
      "-----Epoch:  1530 -----\n",
      "Accuracy: 0.9388\n",
      "Cost: 0.889804561700918\n",
      "-----Epoch:  1540 -----\n",
      "Accuracy: 0.9390833333333334\n",
      "Cost: 0.8897326644712412\n",
      "-----Epoch:  1550 -----\n",
      "Accuracy: 0.9391833333333334\n",
      "Cost: 0.8896614376342569\n",
      "-----Epoch:  1560 -----\n",
      "Accuracy: 0.9393166666666667\n",
      "Cost: 0.8895908020013127\n",
      "-----Epoch:  1570 -----\n",
      "Accuracy: 0.9396\n",
      "Cost: 0.8895207347721585\n",
      "-----Epoch:  1580 -----\n",
      "Accuracy: 0.9398\n",
      "Cost: 0.8894512201510386\n",
      "-----Epoch:  1590 -----\n",
      "Accuracy: 0.9401\n",
      "Cost: 0.8893823056240198\n",
      "-----Epoch:  1600 -----\n",
      "Accuracy: 0.9402666666666667\n",
      "Cost: 0.8893140810259383\n",
      "-----Epoch:  1610 -----\n",
      "Accuracy: 0.94045\n",
      "Cost: 0.8892464762215833\n",
      "-----Epoch:  1620 -----\n",
      "Accuracy: 0.9405333333333333\n",
      "Cost: 0.8891794335548873\n",
      "-----Epoch:  1630 -----\n",
      "Accuracy: 0.9408\n",
      "Cost: 0.8891129466393766\n",
      "-----Epoch:  1640 -----\n",
      "Accuracy: 0.9410166666666666\n",
      "Cost: 0.8890470344942225\n",
      "-----Epoch:  1650 -----\n",
      "Accuracy: 0.9412166666666667\n",
      "Cost: 0.8889815614967008\n",
      "-----Epoch:  1660 -----\n",
      "Accuracy: 0.9414666666666667\n",
      "Cost: 0.8889164669476811\n",
      "-----Epoch:  1670 -----\n",
      "Accuracy: 0.9415833333333333\n",
      "Cost: 0.8888517657996056\n",
      "-----Epoch:  1680 -----\n",
      "Accuracy: 0.9417666666666666\n",
      "Cost: 0.8887876372068436\n",
      "-----Epoch:  1690 -----\n",
      "Accuracy: 0.9419666666666666\n",
      "Cost: 0.8887240859809559\n",
      "-----Epoch:  1700 -----\n",
      "Accuracy: 0.9422166666666667\n",
      "Cost: 0.8886610132522287\n",
      "-----Epoch:  1710 -----\n",
      "Accuracy: 0.9423833333333334\n",
      "Cost: 0.8885984482098588\n",
      "-----Epoch:  1720 -----\n",
      "Accuracy: 0.9426\n",
      "Cost: 0.8885364500735617\n",
      "-----Epoch:  1730 -----\n",
      "Accuracy: 0.9427833333333333\n",
      "Cost: 0.8884749817827102\n",
      "-----Epoch:  1740 -----\n",
      "Accuracy: 0.9430333333333333\n",
      "Cost: 0.8884140968049408\n",
      "-----Epoch:  1750 -----\n",
      "Accuracy: 0.9432333333333334\n",
      "Cost: 0.8883536693528714\n",
      "-----Epoch:  1760 -----\n",
      "Accuracy: 0.9434666666666667\n",
      "Cost: 0.8882936346536309\n",
      "-----Epoch:  1770 -----\n",
      "Accuracy: 0.9436833333333333\n",
      "Cost: 0.888234032524379\n",
      "-----Epoch:  1780 -----\n",
      "Accuracy: 0.9438833333333333\n",
      "Cost: 0.8881748968285978\n",
      "-----Epoch:  1790 -----\n",
      "Accuracy: 0.9439666666666666\n",
      "Cost: 0.8881162329999703\n",
      "-----Epoch:  1800 -----\n",
      "Accuracy: 0.9441333333333334\n",
      "Cost: 0.8880579479627583\n",
      "-----Epoch:  1810 -----\n",
      "Accuracy: 0.9443166666666667\n",
      "Cost: 0.8880002448108484\n",
      "-----Epoch:  1820 -----\n",
      "Accuracy: 0.9444666666666667\n",
      "Cost: 0.8879430708801312\n",
      "-----Epoch:  1830 -----\n",
      "Accuracy: 0.9447666666666666\n",
      "Cost: 0.8878862693979935\n",
      "-----Epoch:  1840 -----\n",
      "Accuracy: 0.94495\n",
      "Cost: 0.8878298077988926\n",
      "-----Epoch:  1850 -----\n",
      "Accuracy: 0.9452333333333334\n",
      "Cost: 0.8877738150950318\n",
      "-----Epoch:  1860 -----\n",
      "Accuracy: 0.9454\n",
      "Cost: 0.8877182864134251\n",
      "-----Epoch:  1870 -----\n",
      "Accuracy: 0.9455666666666667\n",
      "Cost: 0.8876632171235869\n",
      "-----Epoch:  1880 -----\n",
      "Accuracy: 0.94585\n",
      "Cost: 0.8876086740852798\n",
      "-----Epoch:  1890 -----\n",
      "Accuracy: 0.9459833333333333\n",
      "Cost: 0.887554475916674\n",
      "-----Epoch:  1900 -----\n",
      "Accuracy: 0.9461\n",
      "Cost: 0.887500713216717\n",
      "-----Epoch:  1910 -----\n",
      "Accuracy: 0.94625\n",
      "Cost: 0.887447406686221\n",
      "-----Epoch:  1920 -----\n",
      "Accuracy: 0.9465833333333333\n",
      "Cost: 0.8873944573197692\n",
      "-----Epoch:  1930 -----\n",
      "Accuracy: 0.9467666666666666\n",
      "Cost: 0.8873417801868378\n",
      "-----Epoch:  1940 -----\n",
      "Accuracy: 0.94695\n",
      "Cost: 0.8872893807167915\n",
      "-----Epoch:  1950 -----\n",
      "Accuracy: 0.9472\n",
      "Cost: 0.8872372451027718\n",
      "-----Epoch:  1960 -----\n",
      "Accuracy: 0.9472833333333334\n",
      "Cost: 0.8871854316699527\n",
      "-----Epoch:  1970 -----\n",
      "Accuracy: 0.9475833333333333\n",
      "Cost: 0.887134054177638\n",
      "-----Epoch:  1980 -----\n",
      "Accuracy: 0.9477333333333333\n",
      "Cost: 0.8870831117545898\n",
      "-----Epoch:  1990 -----\n",
      "Accuracy: 0.9480166666666666\n",
      "Cost: 0.8870325222816033\n"
     ]
    }
   ],
   "source": [
    "mnist_net_multipHidden = NeuralNet('multi_logloss')\n",
    "mnist_net_multipHidden.addLayer(64, 'reLu')\n",
    "mnist_net_multipHidden.addLayer(64, 'reLu')\n",
    "mnist_net_multipHidden.addLayer(10, 'softmax')\n",
    "costs, weights_multipHidden = mnist_net_multipHidden.fit(X_train, y_train, 2000, 60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20791cba340>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_costs(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-7fa85942c107>:22: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(Z) / sum(np.exp(Z))\n",
      "<ipython-input-2-7fa85942c107>:22: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(Z) / sum(np.exp(Z))\n"
     ]
    }
   ],
   "source": [
    "get_test_accuracy(mnist_net_multipHidden, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset With 2 Hidden Layer + Batches (size 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Epoch:  0 -----\n",
      "Accuracy: 0.749\n",
      "Cost: 36.50090538517477\n",
      "-----Epoch:  10 -----\n",
      "Accuracy: 0.909\n",
      "Cost: 29.36260137950539\n",
      "-----Epoch:  20 -----\n",
      "Accuracy: 0.931\n",
      "Cost: 28.92827721686502\n",
      "-----Epoch:  30 -----\n",
      "Accuracy: 0.94\n",
      "Cost: 28.709133524366177\n",
      "-----Epoch:  40 -----\n",
      "Accuracy: 0.948\n",
      "Cost: 28.56709930260407\n",
      "-----Epoch:  50 -----\n",
      "Accuracy: 0.954\n",
      "Cost: 28.464824137842985\n",
      "-----Epoch:  60 -----\n",
      "Accuracy: 0.957\n",
      "Cost: 28.385518839057248\n",
      "-----Epoch:  70 -----\n",
      "Accuracy: 0.958\n",
      "Cost: 28.322134521137485\n",
      "-----Epoch:  80 -----\n",
      "Accuracy: 0.963\n",
      "Cost: 28.26968917832101\n",
      "-----Epoch:  90 -----\n",
      "Accuracy: 0.966\n",
      "Cost: 28.224806580132867\n",
      "-----Epoch:  100 -----\n",
      "Accuracy: 0.968\n",
      "Cost: 28.18599912506319\n",
      "-----Epoch:  110 -----\n",
      "Accuracy: 0.969\n",
      "Cost: 28.15139089007836\n",
      "-----Epoch:  120 -----\n",
      "Accuracy: 0.972\n",
      "Cost: 28.120420267194202\n",
      "-----Epoch:  130 -----\n",
      "Accuracy: 0.972\n",
      "Cost: 28.09211268468257\n",
      "-----Epoch:  140 -----\n",
      "Accuracy: 0.971\n",
      "Cost: 28.06592607837998\n",
      "-----Epoch:  150 -----\n",
      "Accuracy: 0.972\n",
      "Cost: 28.041701903145796\n",
      "-----Epoch:  160 -----\n",
      "Accuracy: 0.973\n",
      "Cost: 28.019300865025134\n",
      "-----Epoch:  170 -----\n",
      "Accuracy: 0.975\n",
      "Cost: 27.998353909977308\n",
      "-----Epoch:  180 -----\n",
      "Accuracy: 0.976\n",
      "Cost: 27.978739384364054\n",
      "-----Epoch:  190 -----\n",
      "Accuracy: 0.979\n",
      "Cost: 27.960246137696167\n",
      "-----Epoch:  200 -----\n",
      "Accuracy: 0.982\n",
      "Cost: 27.94273727536007\n",
      "-----Epoch:  210 -----\n",
      "Accuracy: 0.985\n",
      "Cost: 27.926439013777674\n",
      "-----Epoch:  220 -----\n",
      "Accuracy: 0.987\n",
      "Cost: 27.910994432711867\n",
      "-----Epoch:  230 -----\n",
      "Accuracy: 0.989\n",
      "Cost: 27.89641629689175\n",
      "-----Epoch:  240 -----\n",
      "Accuracy: 0.99\n",
      "Cost: 27.88265717321209\n",
      "-----Epoch:  250 -----\n",
      "Accuracy: 0.99\n",
      "Cost: 27.869839922899644\n",
      "-----Epoch:  260 -----\n",
      "Accuracy: 0.99\n",
      "Cost: 27.857808128411772\n",
      "-----Epoch:  270 -----\n",
      "Accuracy: 0.991\n",
      "Cost: 27.846222295081976\n",
      "-----Epoch:  280 -----\n",
      "Accuracy: 0.991\n",
      "Cost: 27.835294612466594\n",
      "-----Epoch:  290 -----\n",
      "Accuracy: 0.991\n",
      "Cost: 27.825027461359223\n",
      "-----Epoch:  300 -----\n",
      "Accuracy: 0.991\n",
      "Cost: 27.815284954526135\n",
      "-----Epoch:  310 -----\n",
      "Accuracy: 0.993\n",
      "Cost: 27.80604062423399\n",
      "-----Epoch:  320 -----\n",
      "Accuracy: 0.994\n",
      "Cost: 27.797430686377616\n",
      "-----Epoch:  330 -----\n",
      "Accuracy: 0.995\n",
      "Cost: 27.789378210454043\n",
      "-----Epoch:  340 -----\n",
      "Accuracy: 0.996\n",
      "Cost: 27.78181612731463\n",
      "-----Epoch:  350 -----\n",
      "Accuracy: 0.996\n",
      "Cost: 27.774684200966103\n",
      "-----Epoch:  360 -----\n",
      "Accuracy: 0.996\n",
      "Cost: 27.76805497666761\n",
      "-----Epoch:  370 -----\n",
      "Accuracy: 0.996\n",
      "Cost: 27.761893790860043\n",
      "-----Epoch:  380 -----\n",
      "Accuracy: 0.996\n",
      "Cost: 27.756190891087417\n",
      "-----Epoch:  390 -----\n",
      "Accuracy: 0.997\n",
      "Cost: 27.7507615097817\n",
      "-----Epoch:  400 -----\n",
      "Accuracy: 0.997\n",
      "Cost: 27.745832924749397\n",
      "-----Epoch:  410 -----\n",
      "Accuracy: 0.997\n",
      "Cost: 27.7411272450879\n",
      "-----Epoch:  420 -----\n",
      "Accuracy: 0.997\n",
      "Cost: 27.736688377827786\n",
      "-----Epoch:  430 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 27.73266108968899\n",
      "-----Epoch:  440 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 27.728834423994606\n",
      "-----Epoch:  450 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 27.725324615666228\n",
      "-----Epoch:  460 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 27.722053714184984\n",
      "-----Epoch:  470 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 27.719002735994298\n",
      "-----Epoch:  480 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 27.71617225518646\n",
      "-----Epoch:  490 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 27.713522145682262\n",
      "-----Epoch:  500 -----\n",
      "Accuracy: 0.998\n",
      "Cost: 27.711059624326616\n",
      "-----Epoch:  510 -----\n",
      "Accuracy: 0.999\n",
      "Cost: 27.708778509991546\n",
      "-----Epoch:  520 -----\n",
      "Accuracy: 0.999\n",
      "Cost: 27.70665547623245\n",
      "-----Epoch:  530 -----\n",
      "Accuracy: 0.999\n",
      "Cost: 27.704652664956537\n",
      "-----Epoch:  540 -----\n",
      "Accuracy: 0.999\n",
      "Cost: 27.70277326578218\n",
      "-----Epoch:  550 -----\n",
      "Accuracy: 0.999\n",
      "Cost: 27.701006766829853\n",
      "-----Epoch:  560 -----\n",
      "Accuracy: 0.999\n",
      "Cost: 27.69934931713742\n",
      "-----Epoch:  570 -----\n",
      "Accuracy: 0.999\n",
      "Cost: 27.697788821645748\n",
      "-----Epoch:  580 -----\n",
      "Accuracy: 0.999\n",
      "Cost: 27.696306547128188\n",
      "-----Epoch:  590 -----\n",
      "Accuracy: 0.999\n",
      "Cost: 27.694916329974454\n"
     ]
    }
   ],
   "source": [
    "mnist_net_multipHidden_batches = NeuralNet('multi_logloss')\n",
    "mnist_net_multipHidden_batches.addLayer(64, 'reLu')\n",
    "mnist_net_multipHidden_batches.addLayer(64, 'reLu')\n",
    "mnist_net_multipHidden_batches.addLayer(10, 'softmax')\n",
    "costs, weights_multipHidden_batches, biases_multipHidden_batches = mnist_net_multipHidden_batches.fit(X_train, y_train, 600, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x207b30ca490>]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_costs(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.28\n"
     ]
    }
   ],
   "source": [
    "get_test_accuracy(mnist_net_multipHidden_batches, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Ability to use Previously Determined Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mnist_net = NeuralNet('multi_logloss')\n",
    "test_mnist_net.addLayer(64, 'reLu')\n",
    "test_mnist_net.addLayer(64, 'reLu')\n",
    "test_mnist_net.addLayer(10, 'softmax')\n",
    "test_mnist_net.set_weights(weights_multipHidden_batches)\n",
    "test_mnist_net.set_biases(biases_multipHidden_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.28\n"
     ]
    }
   ],
   "source": [
    "get_test_accuracy(test_mnist_net, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Data for Use in Flask App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting weights and biases of the multiple hidden batched net above (since it had the highest test accuracy) as well ss the X_test and y_test as pickle files for use in the flask app being developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#operations is either 'wb' for write or 'rb' for read\n",
    "def usePickle(content, fileName, operation):\n",
    "    file = open(fileName, operation)\n",
    "    dataToReturn = None\n",
    "    if (operation == 'wb'):\n",
    "        pickle.dump(content, file)\n",
    "    elif (operation == 'rb'):\n",
    "        dataToReturn = pickle.load(file)\n",
    "    else:\n",
    "        raise ValueError(\"Operation is either 'wb' for write or 'rb' for read\")\n",
    "    return dataToReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "usePickle(weights_multipHidden_batches, 'mnist_weights', 'wb')\n",
    "usePickle(biases_multipHidden_batches, 'mnist_biases', 'wb')\n",
    "usePickle(X_test, 'test_images', 'wb')\n",
    "usePickle(y_test, 'test_labels', 'wb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local testing tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[5.46352642e-09]\n",
      " [1.74076425e-06]\n",
      " [8.36703787e-01]\n",
      " [9.70223698e-02]\n",
      " [9.06364024e-09]\n",
      " [2.71011874e-10]\n",
      " [7.66469873e-10]\n",
      " [1.41174891e-05]\n",
      " [6.62579135e-02]\n",
      " [5.55559361e-08]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testimg = usePickle(None, '../native-ann-mnist-classifier/random_img', 'rb')\n",
    "test_single_image(mnist_net_multipHidden_batches, testimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_single_image_from_test_set(test_mnist_net, X_test, 7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
