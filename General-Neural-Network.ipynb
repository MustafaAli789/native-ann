{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipdb\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    #Activation Functions\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    def d_tanh(self, x):\n",
    "        return 1 - np.square(np.tanh(x))\n",
    "    def sigmoid(self, x):\n",
    "#         return 1/(1+ np.exp(-x))\n",
    "        return expit(x)\n",
    "    def d_sigmoid(self, x):\n",
    "        return (1 - self.sigmoid(x)) * self.sigmoid(x)\n",
    "    def ReLu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    def d_ReLu(self, Z):\n",
    "        return Z > 0\n",
    "    \n",
    "    #For output layer, useful for multiclass classification\n",
    "    def softmax(self, Z):\n",
    "#         expZ = np.exp(Z - np.max(Z))\n",
    "#         return expZ / expZ.sum(axis=0, keepdims=True)\n",
    "         return np.exp(Z) / sum(np.exp(Z))\n",
    "    def d_softmax(self, Z):\n",
    "        pass\n",
    "    \n",
    "    activationFunctions = {\n",
    "        'tanh': (tanh, d_tanh),\n",
    "        'sigmoid': (sigmoid, d_sigmoid),\n",
    "        'reLu': (ReLu, d_ReLu),\n",
    "        'softmax': (softmax, d_softmax)\n",
    "    }\n",
    "    \n",
    "    #Input -> num of neurons in prev layer, Neurons --> num neurons in cur layer, Activation -> activation fxn to use\n",
    "    def __init__(self, inputs, neurons, activation):\n",
    "        self.neurons = neurons\n",
    "        self.W = np.random.rand(neurons, inputs) - 0.5\n",
    "        self.b = np.random.rand(neurons, 1) - 0.5\n",
    "        self.Z = None\n",
    "        self.A_prev = None\n",
    "        self.act, self.d_act = self.activationFunctions.get(activation)\n",
    "        \n",
    "    def initializeWeights(self, inputs, neurons):\n",
    "        self.W = np.random.rand(neurons, inputs) - 0.5\n",
    "        \n",
    "    def getNeuronCount(self):\n",
    "        return self.neurons\n",
    "    \n",
    "    def feedForward(self, A_prev):\n",
    "        #ipdb.set_trace()\n",
    "        self.A_prev = A_prev\n",
    "        self.Z = self.W.dot(self.A_prev) + self.b\n",
    "        self.A = self.act(self, self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    #All derivatives are wrt to cost\n",
    "    #Expects dA of cur layer\n",
    "    #Special case where doing multi class classification with mutli class logloss, you can get the dZ wrt dC directly without having to first get dA\n",
    "    def backprop(self, dA, learning_rate, dZ_Special):\n",
    "        \n",
    "        #ipdb.set_trace()\n",
    "        \n",
    "        #elementt by element matrix multip, not a normal dot prod since both matrices have same shape (essentialyl scalar)\n",
    "        dZ = np.multiply(self.d_act(self, self.Z), dA) if dZ_Special.any() == None else dZ_Special\n",
    "        \n",
    "         # need to normalize weights and divide by number of samples\n",
    "        # because it is actually a sum of weights\n",
    "        dW = 1/dZ.shape[1] * np.dot(dZ, self.A_prev.T)\n",
    "        \n",
    "        # this is to match shape since biases is supposed to be a col vector with 1 col but dZ has m cols\n",
    "        # w/ m being num of samples, we want to take avg of all samples in dZ (i.e on a row by row basis, sum of cols\n",
    "        # and divide by total num of smamples)\n",
    "        db = 1 / dZ.shape[1] * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        \n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "        \n",
    "        self.W = self.W - learning_rate * dW\n",
    "        self.b = self.b - learning_rate * db\n",
    "        return dA_prev\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    #Loss Functions, mse for regression, logloss for classification\n",
    "    def mse(self, a, target):\n",
    "        return np.square(a-target)\n",
    "    \n",
    "    def d_mse(self, a, target):\n",
    "        return 2*(a-target)\n",
    "    \n",
    "    def binary_logloss(self, a, target):\n",
    "        return -(target*np.log(a) + (1-target)*np.log(1-a))\n",
    "    \n",
    "    def d_binary_logloss(self, a, target):\n",
    "        return (a - target)/(a*(1 - a))\n",
    "    \n",
    "    def multi_logloss(self, a, target, eps=1e-15):\n",
    "        predictions = np.clip(a, eps, 1 - eps)\n",
    "\n",
    "        # normalize row sums to 1\n",
    "        predictions /= predictions.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        return -np.sum(target * np.log(predictions))/predictions.shape[0]\n",
    "    \n",
    "    def d_multi_logloss(self, a, target):\n",
    "        return np.zeros(a.shape) # kinda just a placeholder\n",
    "    \n",
    "    lossFunctions = {\n",
    "        'mse': (mse, d_mse),\n",
    "        'binary_logloss': (binary_logloss, d_binary_logloss),\n",
    "        'multi_logloss': (multi_logloss, d_multi_logloss)\n",
    "    }\n",
    "        \n",
    "    #LossFunction is either mse of logloss\n",
    "    def __init__(self, lossFunction):\n",
    "        self.layers = []\n",
    "        self.learning_rate = 0.1\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 10\n",
    "        self.classification = False if lossFunction == 'mse' else True\n",
    "        self.lossFunction = lossFunction\n",
    "        self.loss, self.d_loss = self.lossFunctions.get(lossFunction)\n",
    "    \n",
    "    #Units is 1-n and activationFunction is 'ReLu', 'sigmoid', 'tanh', or 'softmax'\n",
    "    def addLayer(self, units, activationFunction):\n",
    "        prevLayerNeuronCount = self.layers[-1].getNeuronCount() if len(self.layers) > 0 else 0\n",
    "        self.layers.append(Layer(prevLayerNeuronCount, units, activationFunction))\n",
    "        \n",
    "    def getNumBatches(self, num_samples, batch_size):\n",
    "        if (num_samples == batch_size):\n",
    "            return 1\n",
    "        elif (num_samples > batch_size):\n",
    "            if (num_samples % batch_size == 0):\n",
    "                return num_samples // batch_size\n",
    "            else:\n",
    "                return (num_samples // batch_size) + 1\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def oneHot(self, x):\n",
    "        one_hot_X = np.zeros((x.max() + 1, x.size)) #making a matrix of 10 x m\n",
    "        one_hot_X[x, np.arange(x.size)] = 1 #going through all cols and setting the row w/ index corresponding to the y to 1, its very easy to iterate over numpy arays like this apparently\n",
    "        return one_hot_X\n",
    "        \n",
    "    def rev_one_hot(self, target):\n",
    "        rev_one_hot = np.argmax(target, 0)\n",
    "        return rev_one_hot\n",
    "    \n",
    "    def get_accuracy(self, target, Y):\n",
    "        #ipdb.set_trace()\n",
    "        return np.sum(target == Y) / Y.size\n",
    "    \n",
    "    def fit(self, X, y, epochs = None, batch_size = None, learning_rate = None):\n",
    "        self.learning_rate = learning_rate if learning_rate != None else self.learning_rate\n",
    "        self.epochs = epochs if epochs != None else self.epochs\n",
    "        self.batch_size = batch_size if batch_size != None else self.batch_size\n",
    "        \n",
    "        #need at min one layer\n",
    "        if (len(self.layers) == 0):\n",
    "            raise ValueError('No layers have been added. Need at least one layer. Please add a layer') \n",
    "        \n",
    "        #multi class classificaiton problem need y to be one hot encoded and must use multi log loss\n",
    "        multiClassProblem = self.classification and (y.max() - y.min() > 1)\n",
    "        if (multiClassProblem):\n",
    "            y = self.oneHot(y)\n",
    "            if (self.lossFunction != 'multi_logloss'):\n",
    "                raise ValueError('Loss Function Must be multi_logloss for multi class classification')\n",
    "        \n",
    "        epoch_costs = []\n",
    "        batches_cost_sum = 0\n",
    "        num_batches = self.getNumBatches(X.shape[1], self.batch_size)\n",
    "        \n",
    "        #Initializing weights of the first layer \n",
    "        #need to do it right now because shape of input isnt known until now\n",
    "        self.layers[0].initializeWeights(X.shape[0], self.layers[0].getNeuronCount())\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            batches_cost_sum = 0\n",
    "            for batch in range(num_batches):\n",
    "                #ipdb.set_trace()\n",
    "                A = X[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
    "                \n",
    "                if (multiClassProblem): \n",
    "                    y_curBatch = y[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
    "                else:\n",
    "                    y_CurBatch = y[batch*self.batch_size:(batch+1)*self.batch_size]\n",
    "            \n",
    "                #ipdb.set_trace()\n",
    "                for layer in self.layers:\n",
    "                    A = layer.feedForward(A)\n",
    "                batches_cost_sum += 1/self.batch_size * np.sum(self.loss(self, A, y_curBatch))\n",
    "                dZ_Special = A - y_curBatch if multiClassProblem else np.array([None])\n",
    "                dA = self.d_loss(self, A, y_curBatch) # after the final output layer dA is found like this since A is just the output\n",
    "                for layer in reversed(self.layers):\n",
    "                    if (layer == self.layers[-1]):\n",
    "                        dA = layer.backprop(dA, self.learning_rate, dZ_Special)\n",
    "                    else:\n",
    "                        dA = layer.backprop(dA, self.learning_rate, np.array([None]))\n",
    "                if (epoch % 10 == 0 and batch == 0): #beginning of every batch\n",
    "                    print(\"Epoch: \", epoch)\n",
    "                    print(\"Accuracy:\", self.get_accuracy(self.rev_one_hot(A), self.rev_one_hot(y_curBatch)))\n",
    "            epoch_costs.append(batches_cost_sum) \n",
    "            print(\"Epoch: \", epoch, \"Cost:\", batches_cost_sum)\n",
    "        return epoch_costs\n",
    "        \n",
    "    def predict(self, X):\n",
    "        #ipdb.set_trace()\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.feedForward(A)\n",
    "        return A\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.array([[0, 0, 1, 1], \n",
    "#                     [0, 1, 0, 1]]) # 2 inputs and 4 samples, i.e 2x4\n",
    "# y_train = np.array([0, 1, 1, 0]) #1 x num of samples\n",
    "# net = NeuralNet('logloss')\n",
    "# net.addLayer(3, 'tanh')\n",
    "# net.addLayer(1, 'sigmoid')\n",
    "# costs = net.fit(x_train, y_train, 10000)\n",
    "# plt.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.array([[1], [1]])\n",
    "# a = net.predict(test)\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 1 ... 7 1 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation].astype(np.int)\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "print(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=60000, test_size=10000)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).T\n",
    "X_test = scaler.transform(X_test).T\n",
    "\n",
    "# X_train = X_train.T\n",
    "# X_test = X_test.T\n",
    "\n",
    "plt.gray()\n",
    "plt.imshow(X_test[:, 50].reshape(28, 28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revOneHot(x):\n",
    "    rev_one_hot = np.zeros(x.shape[1]) #making an aray of m length\n",
    "    rev_one_hot[np.arange(x.shape[1])] = np.argmax(x, 0)\n",
    "    return rev_one_hot\n",
    "def test(target):\n",
    "    preds = np.argmax(target, 0)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0, 1, 0, 0], [0, 0, 0, 1]]).reshape(4, 2)\n",
    "b = test(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Accuracy: 0.07331666666666667\n",
      "Epoch:  0 Cost: 2.463045004424733\n",
      "Epoch:  1 Cost: 1.9382295299665209\n",
      "Epoch:  2 Cost: 1.6614447018747343\n",
      "Epoch:  3 Cost: 1.5070249130027404\n",
      "Epoch:  4 Cost: 1.4082391321960352\n",
      "Epoch:  5 Cost: 1.3394122629284\n",
      "Epoch:  6 Cost: 1.2886181475206149\n",
      "Epoch:  7 Cost: 1.2494386894314888\n",
      "Epoch:  8 Cost: 1.2181883319393907\n",
      "Epoch:  9 Cost: 1.1926855964413985\n",
      "Epoch:  10\n",
      "Accuracy: 0.5846666666666667\n",
      "Epoch:  10 Cost: 1.171489579624289\n",
      "Epoch:  11 Cost: 1.1535737922296752\n",
      "Epoch:  12 Cost: 1.1382017116499037\n",
      "Epoch:  13 Cost: 1.1248477973040518\n",
      "Epoch:  14 Cost: 1.1130904201044716\n",
      "Epoch:  15 Cost: 1.1026440145490626\n",
      "Epoch:  16 Cost: 1.0932859007795925\n",
      "Epoch:  17 Cost: 1.084841813905291\n",
      "Epoch:  18 Cost: 1.07718792504233\n",
      "Epoch:  19 Cost: 1.0702108444756988\n",
      "Epoch:  20\n",
      "Accuracy: 0.69795\n",
      "Epoch:  20 Cost: 1.0638115037191007\n",
      "Epoch:  21 Cost: 1.0579162793171182\n",
      "Epoch:  22 Cost: 1.0524605086786216\n",
      "Epoch:  23 Cost: 1.0473991285682305\n",
      "Epoch:  24 Cost: 1.0426870455871249\n",
      "Epoch:  25 Cost: 1.038284836246938\n",
      "Epoch:  26 Cost: 1.0341568504154117\n",
      "Epoch:  27 Cost: 1.0302755441408957\n",
      "Epoch:  28 Cost: 1.026622936436477\n",
      "Epoch:  29 Cost: 1.0231773390981944\n",
      "Epoch:  30\n",
      "Accuracy: 0.7449666666666667\n",
      "Epoch:  30 Cost: 1.0199267455281047\n",
      "Epoch:  31 Cost: 1.016850535529604\n",
      "Epoch:  32 Cost: 1.013931451927748\n",
      "Epoch:  33 Cost: 1.0111577471530335\n",
      "Epoch:  34 Cost: 1.0085091090225506\n",
      "Epoch:  35 Cost: 1.0059802617953224\n",
      "Epoch:  36 Cost: 1.0035696866411092\n",
      "Epoch:  37 Cost: 1.001269377658539\n",
      "Epoch:  38 Cost: 0.9990702650318684\n",
      "Epoch:  39 Cost: 0.9969670090039195\n",
      "Epoch:  40\n",
      "Accuracy: 0.7724666666666666\n",
      "Epoch:  40 Cost: 0.9949513314532754\n",
      "Epoch:  41 Cost: 0.9930198400606062\n",
      "Epoch:  42 Cost: 0.9911653367555087\n",
      "Epoch:  43 Cost: 0.9893829462282059\n",
      "Epoch:  44 Cost: 0.9876673255060338\n",
      "Epoch:  45 Cost: 0.9860161400626113\n",
      "Epoch:  46 Cost: 0.9844250105798805\n",
      "Epoch:  47 Cost: 0.9828911134915\n",
      "Epoch:  48 Cost: 0.9814101505692722\n",
      "Epoch:  49 Cost: 0.979976030236189\n",
      "Epoch:  50\n",
      "Accuracy: 0.79155\n",
      "Epoch:  50 Cost: 0.978590976720451\n",
      "Epoch:  51 Cost: 0.9772542713643889\n",
      "Epoch:  52 Cost: 0.9759640673530312\n",
      "Epoch:  53 Cost: 0.9747173730650218\n",
      "Epoch:  54 Cost: 0.9735099332053003\n",
      "Epoch:  55 Cost: 0.9723410390642605\n",
      "Epoch:  56 Cost: 0.9712091364643709\n",
      "Epoch:  57 Cost: 0.9701120188204239\n",
      "Epoch:  58 Cost: 0.9690474302751878\n",
      "Epoch:  59 Cost: 0.968013530096415\n",
      "Epoch:  60\n",
      "Accuracy: 0.8058833333333333\n",
      "Epoch:  60 Cost: 0.9670080390870857\n",
      "Epoch:  61 Cost: 0.9660296112056325\n",
      "Epoch:  62 Cost: 0.9650777456347921\n",
      "Epoch:  63 Cost: 0.9641514579459567\n",
      "Epoch:  64 Cost: 0.9632496863470993\n",
      "Epoch:  65 Cost: 0.9623714752443501\n",
      "Epoch:  66 Cost: 0.9615158353198363\n",
      "Epoch:  67 Cost: 0.9606808196632151\n",
      "Epoch:  68 Cost: 0.9598650144318261\n",
      "Epoch:  69 Cost: 0.9590685401158179\n",
      "Epoch:  70\n",
      "Accuracy: 0.8166166666666667\n",
      "Epoch:  70 Cost: 0.9582909908073293\n",
      "Epoch:  71 Cost: 0.9575315957173584\n",
      "Epoch:  72 Cost: 0.9567895712666399\n",
      "Epoch:  73 Cost: 0.9560651152682639\n",
      "Epoch:  74 Cost: 0.9553575725477044\n",
      "Epoch:  75 Cost: 0.9546662782853622\n",
      "Epoch:  76 Cost: 0.9539898075971419\n",
      "Epoch:  77 Cost: 0.9533271279904474\n",
      "Epoch:  78 Cost: 0.9526786609577902\n",
      "Epoch:  79 Cost: 0.9520433828468555\n",
      "Epoch:  80\n",
      "Accuracy: 0.8249833333333333\n",
      "Epoch:  80 Cost: 0.9514205930111025\n",
      "Epoch:  81 Cost: 0.9508106015475157\n",
      "Epoch:  82 Cost: 0.95021343553671\n",
      "Epoch:  83 Cost: 0.949627846195996\n",
      "Epoch:  84 Cost: 0.949054104625694\n",
      "Epoch:  85 Cost: 0.9484920218942985\n",
      "Epoch:  86 Cost: 0.9479412979227639\n",
      "Epoch:  87 Cost: 0.9474017480691901\n",
      "Epoch:  88 Cost: 0.9468730073361874\n",
      "Epoch:  89 Cost: 0.9463550240391259\n",
      "Epoch:  90\n",
      "Accuracy: 0.8318833333333333\n",
      "Epoch:  90 Cost: 0.94584741633227\n",
      "Epoch:  91 Cost: 0.9453497956008632\n",
      "Epoch:  92 Cost: 0.9448619865971158\n",
      "Epoch:  93 Cost: 0.9443835299362765\n",
      "Epoch:  94 Cost: 0.9439139154007539\n",
      "Epoch:  95 Cost: 0.9434532924607856\n",
      "Epoch:  96 Cost: 0.9430013602584187\n",
      "Epoch:  97 Cost: 0.9425580820444609\n",
      "Epoch:  98 Cost: 0.9421230032774304\n",
      "Epoch:  99 Cost: 0.94169548017636\n",
      "Epoch:  100\n",
      "Accuracy: 0.8379\n",
      "Epoch:  100 Cost: 0.9412753380798052\n",
      "Epoch:  101 Cost: 0.9408626911792177\n",
      "Epoch:  102 Cost: 0.9404568083266104\n",
      "Epoch:  103 Cost: 0.9400577683815642\n",
      "Epoch:  104 Cost: 0.9396658746185134\n",
      "Epoch:  105 Cost: 0.9392808971829296\n",
      "Epoch:  106 Cost: 0.9389015803487363\n",
      "Epoch:  107 Cost: 0.9385282909883361\n",
      "Epoch:  108 Cost: 0.938161057148917\n",
      "Epoch:  109 Cost: 0.937799449890267\n",
      "Epoch:  110\n",
      "Accuracy: 0.8431833333333333\n",
      "Epoch:  110 Cost: 0.937443708362379\n",
      "Epoch:  111 Cost: 0.9370933292530813\n",
      "Epoch:  112 Cost: 0.9367487166399612\n",
      "Epoch:  113 Cost: 0.9364098447000583\n",
      "Epoch:  114 Cost: 0.936076477579934\n",
      "Epoch:  115 Cost: 0.9357484764015775\n",
      "Epoch:  116 Cost: 0.9354257700263149\n",
      "Epoch:  117 Cost: 0.9351073853497833\n",
      "Epoch:  118 Cost: 0.9347937815370088\n",
      "Epoch:  119 Cost: 0.9344848903435419\n",
      "Epoch:  120\n",
      "Accuracy: 0.8480166666666666\n",
      "Epoch:  120 Cost: 0.9341808363298824\n",
      "Epoch:  121 Cost: 0.9338811453600089\n",
      "Epoch:  122 Cost: 0.9335859005325697\n",
      "Epoch:  123 Cost: 0.9332951375397515\n",
      "Epoch:  124 Cost: 0.9330087668099951\n",
      "Epoch:  125 Cost: 0.9327261256108472\n",
      "Epoch:  126 Cost: 0.9324471892699334\n",
      "Epoch:  127 Cost: 0.9321724516381231\n",
      "Epoch:  128 Cost: 0.9319018582514856\n",
      "Epoch:  129 Cost: 0.9316352622905336\n",
      "Epoch:  130\n",
      "Accuracy: 0.8519333333333333\n",
      "Epoch:  130 Cost: 0.9313726433541527\n",
      "Epoch:  131 Cost: 0.9311136803864948\n",
      "Epoch:  132 Cost: 0.9308574543690474\n",
      "Epoch:  133 Cost: 0.9306047152070923\n",
      "Epoch:  134 Cost: 0.9303556280730043\n",
      "Epoch:  135 Cost: 0.9301101036922144\n",
      "Epoch:  136 Cost: 0.9298675971137489\n",
      "Epoch:  137 Cost: 0.9296285515816531\n",
      "Epoch:  138 Cost: 0.9293928627071474\n",
      "Epoch:  139 Cost: 0.9291604499038665\n",
      "Epoch:  140\n",
      "Accuracy: 0.8553666666666667\n",
      "Epoch:  140 Cost: 0.9289308957697178\n",
      "Epoch:  141 Cost: 0.928704334561527\n",
      "Epoch:  142 Cost: 0.928480611227435\n",
      "Epoch:  143 Cost: 0.9282599669593166\n",
      "Epoch:  144 Cost: 0.9280423214180683\n",
      "Epoch:  145 Cost: 0.9278276023595814\n",
      "Epoch:  146 Cost: 0.9276157720474634\n",
      "Epoch:  147 Cost: 0.9274067211853878\n",
      "Epoch:  148 Cost: 0.9272001986843342\n",
      "Epoch:  149 Cost: 0.9269963744927835\n",
      "Epoch:  150\n",
      "Accuracy: 0.8582333333333333\n",
      "Epoch:  150 Cost: 0.9267952137004132\n",
      "Epoch:  151 Cost: 0.9265966905601845\n",
      "Epoch:  152 Cost: 0.9264007658726612\n",
      "Epoch:  153 Cost: 0.9262074167899912\n",
      "Epoch:  154 Cost: 0.9260165577468159\n",
      "Epoch:  155 Cost: 0.9258281885242744\n",
      "Epoch:  156 Cost: 0.9256422651070836\n",
      "Epoch:  157 Cost: 0.9254587771871358\n",
      "Epoch:  158 Cost: 0.9252776562861106\n",
      "Epoch:  159 Cost: 0.9250988529841467\n",
      "Epoch:  160\n",
      "Accuracy: 0.8613333333333333\n",
      "Epoch:  160 Cost: 0.9249221060192753\n",
      "Epoch:  161 Cost: 0.9247471855068974\n",
      "Epoch:  162 Cost: 0.9245744350567703\n",
      "Epoch:  163 Cost: 0.9244038520592835\n",
      "Epoch:  164 Cost: 0.9242354256392438\n",
      "Epoch:  165 Cost: 0.9240691002522008\n",
      "Epoch:  166 Cost: 0.9239047855747808\n",
      "Epoch:  167 Cost: 0.923742404397379\n",
      "Epoch:  168 Cost: 0.923582001934682\n",
      "Epoch:  169 Cost: 0.9234235356889579\n",
      "Epoch:  170\n",
      "Accuracy: 0.8639\n",
      "Epoch:  170 Cost: 0.9232669669138717\n",
      "Epoch:  171 Cost: 0.9231122695972962\n",
      "Epoch:  172 Cost: 0.9229592164014344\n",
      "Epoch:  173 Cost: 0.9228076157693804\n",
      "Epoch:  174 Cost: 0.9226576526409997\n",
      "Epoch:  175 Cost: 0.922509423001303\n",
      "Epoch:  176 Cost: 0.9223629089638054\n",
      "Epoch:  177 Cost: 0.9222180905376477\n",
      "Epoch:  178 Cost: 0.9220749289624949\n",
      "Epoch:  179 Cost: 0.9219334072654679\n",
      "Epoch:  180\n",
      "Accuracy: 0.8665666666666667\n",
      "Epoch:  180 Cost: 0.9217934714074631\n",
      "Epoch:  181 Cost: 0.9216551279201175\n",
      "Epoch:  182 Cost: 0.9215180854453877\n",
      "Epoch:  183 Cost: 0.921382435706796\n",
      "Epoch:  184 Cost: 0.9212483425107462\n",
      "Epoch:  185 Cost: 0.921115734205709\n",
      "Epoch:  186 Cost: 0.9209842617474753\n",
      "Epoch:  187 Cost: 0.9208543506212927\n",
      "Epoch:  188 Cost: 0.9207258712041347\n",
      "Epoch:  189 Cost: 0.9205989170709948\n",
      "Epoch:  190\n",
      "Accuracy: 0.8688666666666667\n",
      "Epoch:  190 Cost: 0.9204733096340538\n",
      "Epoch:  191 Cost: 0.9203491371857998\n",
      "Epoch:  192 Cost: 0.9202262494817917\n",
      "Epoch:  193 Cost: 0.9201047158966595\n",
      "Epoch:  194 Cost: 0.919984407332509\n",
      "Epoch:  195 Cost: 0.9198655209475809\n",
      "Epoch:  196 Cost: 0.9197477580253581\n",
      "Epoch:  197 Cost: 0.9196313664668667\n",
      "Epoch:  198 Cost: 0.9195160327871872\n",
      "Epoch:  199 Cost: 0.9194020439718346\n",
      "Epoch:  200\n",
      "Accuracy: 0.87075\n",
      "Epoch:  200 Cost: 0.9192886622145939\n",
      "Epoch:  201 Cost: 0.9191766953607164\n",
      "Epoch:  202 Cost: 0.9190655994662185\n",
      "Epoch:  203 Cost: 0.918955600868982\n",
      "Epoch:  204 Cost: 0.91884606911619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  205 Cost: 0.9187379510375439\n",
      "Epoch:  206 Cost: 0.9186305033326757\n",
      "Epoch:  207 Cost: 0.9185245833073581\n",
      "Epoch:  208 Cost: 0.9184191903128457\n",
      "Epoch:  209 Cost: 0.9183150587597788\n",
      "Epoch:  210\n",
      "Accuracy: 0.8723833333333333\n",
      "Epoch:  210 Cost: 0.9182110227753916\n",
      "Epoch:  211 Cost: 0.9181086106864448\n",
      "Epoch:  212 Cost: 0.9180062814413948\n",
      "Epoch:  213 Cost: 0.9179059880577377\n",
      "Epoch:  214 Cost: 0.9178054812853248\n",
      "Epoch:  215 Cost: 0.9177073249592165\n",
      "Epoch:  216 Cost: 0.9176084922940849\n",
      "Epoch:  217 Cost: 0.917512510174746\n",
      "Epoch:  218 Cost: 0.9174152390706729\n",
      "Epoch:  219 Cost: 0.9173212840469797\n",
      "Epoch:  220\n",
      "Accuracy: 0.8742333333333333\n",
      "Epoch:  220 Cost: 0.9172246019755679\n",
      "Epoch:  221 Cost: 0.9171324824004146\n",
      "Epoch:  222 Cost: 0.9170367172713908\n",
      "Epoch:  223 Cost: 0.9169467502551671\n",
      "Epoch:  224 Cost: 0.9168519765113585\n",
      "Epoch:  225 Cost: 0.9167649119880135\n",
      "Epoch:  226 Cost: 0.9166708580694143\n",
      "Epoch:  227 Cost: 0.9165872782044935\n",
      "Epoch:  228 Cost: 0.9164936297833345\n",
      "Epoch:  229 Cost: 0.9164142905618872\n",
      "Epoch:  230\n",
      "Accuracy: 0.8761666666666666\n",
      "Epoch:  230 Cost: 0.9163209002070909\n",
      "Epoch:  231 Cost: 0.9162473308105908\n",
      "Epoch:  232 Cost: 0.9161537244603986\n",
      "Epoch:  233 Cost: 0.9160889490344077\n",
      "Epoch:  234 Cost: 0.9159949192720017\n",
      "Epoch:  235 Cost: 0.9159433439546549\n",
      "Epoch:  236 Cost: 0.9158516460344878\n",
      "Epoch:  237 Cost: 0.9158198639682043\n",
      "Epoch:  238 Cost: 0.9157357928090444\n",
      "Epoch:  239 Cost: 0.9157326860916516\n",
      "Epoch:  240\n",
      "Accuracy: 0.8767166666666667\n",
      "Epoch:  240 Cost: 0.9156669064593347\n",
      "Epoch:  241 Cost: 0.915703574599095\n",
      "Epoch:  242 Cost: 0.9156763257604328\n",
      "Epoch:  243 Cost: 0.9157607757890397\n",
      "Epoch:  244 Cost: 0.9158011160599646\n",
      "Epoch:  245 Cost: 0.9159363427953695\n",
      "Epoch:  246 Cost: 0.9160870387836517\n",
      "Epoch:  247 Cost: 0.9162456408520605\n",
      "Epoch:  248 Cost: 0.916532653446409\n",
      "Epoch:  249 Cost: 0.9166311718580908\n",
      "Epoch:  250\n",
      "Accuracy: 0.8647666666666667\n",
      "Epoch:  250 Cost: 0.9170224965518157\n",
      "Epoch:  251 Cost: 0.9169415692189166\n",
      "Epoch:  252 Cost: 0.9173383011711747\n",
      "Epoch:  253 Cost: 0.9169964160487021\n",
      "Epoch:  254 Cost: 0.917274963235591\n",
      "Epoch:  255 Cost: 0.9167109526138828\n",
      "Epoch:  256 Cost: 0.9168104821852422\n",
      "Epoch:  257 Cost: 0.9161755488023486\n",
      "Epoch:  258 Cost: 0.91612889779433\n",
      "Epoch:  259 Cost: 0.9155752276724168\n",
      "Epoch:  260\n",
      "Accuracy: 0.8730333333333333\n",
      "Epoch:  260 Cost: 0.9154420781275018\n",
      "Epoch:  261 Cost: 0.9150358438108444\n",
      "Epoch:  262 Cost: 0.914865078677573\n",
      "Epoch:  263 Cost: 0.9145834221827571\n",
      "Epoch:  264 Cost: 0.9144156031212447\n",
      "Epoch:  265 Cost: 0.9142225872804273\n",
      "Epoch:  266 Cost: 0.9140727143980897\n",
      "Epoch:  267 Cost: 0.9139410229331979\n",
      "Epoch:  268 Cost: 0.9138109946212146\n",
      "Epoch:  269 Cost: 0.9137163624127936\n",
      "Epoch:  270\n",
      "Accuracy: 0.8808833333333334\n",
      "Epoch:  270 Cost: 0.9136016551226642\n",
      "Epoch:  271 Cost: 0.9135285921627272\n",
      "Epoch:  272 Cost: 0.913424604962782\n",
      "Epoch:  273 Cost: 0.9133646277282303\n",
      "Epoch:  274 Cost: 0.9132680701543183\n",
      "Epoch:  275 Cost: 0.9132167165300982\n",
      "Epoch:  276 Cost: 0.9131254032602993\n",
      "Epoch:  277 Cost: 0.9130803259489042\n",
      "Epoch:  278 Cost: 0.9129928626343905\n",
      "Epoch:  279 Cost: 0.912952843257673\n",
      "Epoch:  280\n",
      "Accuracy: 0.8828833333333334\n",
      "Epoch:  280 Cost: 0.9128683253763173\n",
      "Epoch:  281 Cost: 0.9128327536355828\n",
      "Epoch:  282 Cost: 0.9127506689574272\n",
      "Epoch:  283 Cost: 0.9127193560337714\n",
      "Epoch:  284 Cost: 0.91263946826343\n",
      "Epoch:  285 Cost: 0.9126124543453428\n",
      "Epoch:  286 Cost: 0.9125347667289387\n",
      "Epoch:  287 Cost: 0.9125122169222244\n",
      "Epoch:  288 Cost: 0.9124369141344916\n",
      "Epoch:  289 Cost: 0.9124190788355455\n",
      "Epoch:  290\n",
      "Accuracy: 0.8837666666666667\n",
      "Epoch:  290 Cost: 0.912346625155728\n",
      "Epoch:  291 Cost: 0.912333794181449\n",
      "Epoch:  292 Cost: 0.9122649420942565\n",
      "Epoch:  293 Cost: 0.9122573473697373\n",
      "Epoch:  294 Cost: 0.9121932210832253\n",
      "Epoch:  295 Cost: 0.9121908947459542\n",
      "Epoch:  296 Cost: 0.9121330894826487\n",
      "Epoch:  297 Cost: 0.9121355488159879\n",
      "Epoch:  298 Cost: 0.9120860570529774\n",
      "Epoch:  299 Cost: 0.9120919082417118\n",
      "Epoch:  300\n",
      "Accuracy: 0.88345\n",
      "Epoch:  300 Cost: 0.9120528156441229\n",
      "Epoch:  301 Cost: 0.9120594225544912\n",
      "Epoch:  302 Cost: 0.9120322858559518\n",
      "Epoch:  303 Cost: 0.9120357865955516\n",
      "Epoch:  304 Cost: 0.912020773692284\n",
      "Epoch:  305 Cost: 0.9120161642054927\n",
      "Epoch:  306 Cost: 0.9120112873409805\n",
      "Epoch:  307 Cost: 0.911993387368818\n",
      "Epoch:  308 Cost: 0.9119948812847039\n",
      "Epoch:  309 Cost: 0.911958945374012\n",
      "Epoch:  310\n",
      "Accuracy: 0.8820333333333333\n",
      "Epoch:  310 Cost: 0.9119613819996455\n",
      "Epoch:  311 Cost: 0.9119045244242645\n",
      "Epoch:  312 Cost: 0.9119019117717697\n",
      "Epoch:  313 Cost: 0.9118243338676532\n",
      "Epoch:  314 Cost: 0.911811292084056\n",
      "Epoch:  315 Cost: 0.9117167742982047\n",
      "Epoch:  316 Cost: 0.9116895210338951\n",
      "Epoch:  317 Cost: 0.911584501203991\n",
      "Epoch:  318 Cost: 0.9115417632066376\n",
      "Epoch:  319 Cost: 0.9114336759787206\n",
      "Epoch:  320\n",
      "Accuracy: 0.8837666666666667\n",
      "Epoch:  320 Cost: 0.9113765474856789\n",
      "Epoch:  321 Cost: 0.9112721378644358\n",
      "Epoch:  322 Cost: 0.9112036462433643\n",
      "Epoch:  323 Cost: 0.9111077663478406\n",
      "Epoch:  324 Cost: 0.9110319058412955\n",
      "Epoch:  325 Cost: 0.9109464239463291\n",
      "Epoch:  326 Cost: 0.9108677449607178\n",
      "Epoch:  327 Cost: 0.9107929502676084\n",
      "Epoch:  328 Cost: 0.910714653648351\n",
      "Epoch:  329 Cost: 0.9106496083503168\n",
      "Epoch:  330\n",
      "Accuracy: 0.8876166666666667\n",
      "Epoch:  330 Cost: 0.9105733999753746\n",
      "Epoch:  331 Cost: 0.9105168367516424\n",
      "Epoch:  332 Cost: 0.9104429895146928\n",
      "Epoch:  333 Cost: 0.9103941695471656\n",
      "Epoch:  334 Cost: 0.9103228522456703\n",
      "Epoch:  335 Cost: 0.9102806279212792\n",
      "Epoch:  336 Cost: 0.9102119344296455\n",
      "Epoch:  337 Cost: 0.9101750906568673\n",
      "Epoch:  338 Cost: 0.91010913888064\n",
      "Epoch:  339 Cost: 0.9100768146880647\n",
      "Epoch:  340\n",
      "Accuracy: 0.8893166666666666\n",
      "Epoch:  340 Cost: 0.9100134807296839\n",
      "Epoch:  341 Cost: 0.9099850218411067\n",
      "Epoch:  342 Cost: 0.909924122939339\n",
      "Epoch:  343 Cost: 0.909899028025449\n",
      "Epoch:  344 Cost: 0.9098405058831307\n",
      "Epoch:  345 Cost: 0.9098183931340997\n",
      "Epoch:  346 Cost: 0.9097622477146066\n",
      "Epoch:  347 Cost: 0.9097427012271854\n",
      "Epoch:  348 Cost: 0.9096889222722208\n",
      "Epoch:  349 Cost: 0.9096715085599649\n",
      "Epoch:  350\n",
      "Accuracy: 0.8900833333333333\n",
      "Epoch:  350 Cost: 0.9096201629492716\n",
      "Epoch:  351 Cost: 0.9096044213328816\n",
      "Epoch:  352 Cost: 0.9095555740299683\n",
      "Epoch:  353 Cost: 0.9095409403612746\n",
      "Epoch:  354 Cost: 0.9094946473464345\n",
      "Epoch:  355 Cost: 0.9094804717664371\n",
      "Epoch:  356 Cost: 0.9094367317909631\n",
      "Epoch:  357 Cost: 0.9094220298672253\n",
      "Epoch:  358 Cost: 0.9093810964049531\n",
      "Epoch:  359 Cost: 0.909365067382075\n",
      "Epoch:  360\n",
      "Accuracy: 0.8902833333333333\n",
      "Epoch:  360 Cost: 0.9093268710714031\n",
      "Epoch:  361 Cost: 0.9093087787300429\n",
      "Epoch:  362 Cost: 0.9092730070608818\n",
      "Epoch:  363 Cost: 0.9092521824593462\n",
      "Epoch:  364 Cost: 0.909218398877308\n",
      "Epoch:  365 Cost: 0.9091942983610349\n",
      "Epoch:  366 Cost: 0.9091619531230092\n",
      "Epoch:  367 Cost: 0.9091342403158242\n",
      "Epoch:  368 Cost: 0.9091027400788813\n",
      "Epoch:  369 Cost: 0.9090712956367208\n",
      "Epoch:  370\n",
      "Accuracy: 0.8905333333333333\n",
      "Epoch:  370 Cost: 0.909040002511346\n",
      "Epoch:  371 Cost: 0.9090049411091307\n",
      "Epoch:  372 Cost: 0.9089732976006585\n",
      "Epoch:  373 Cost: 0.9089349781118589\n",
      "Epoch:  374 Cost: 0.9089024984111078\n",
      "Epoch:  375 Cost: 0.9088614947245595\n",
      "Epoch:  376 Cost: 0.908827842637987\n",
      "Epoch:  377 Cost: 0.9087848612684292\n",
      "Epoch:  378 Cost: 0.9087498528272528\n",
      "Epoch:  379 Cost: 0.9087056586426842\n",
      "Epoch:  380\n",
      "Accuracy: 0.8916333333333334\n",
      "Epoch:  380 Cost: 0.9086692518946373\n",
      "Epoch:  381 Cost: 0.908624591080443\n",
      "Epoch:  382 Cost: 0.9085868951543213\n",
      "Epoch:  383 Cost: 0.9085424507354083\n",
      "Epoch:  384 Cost: 0.908503633335367\n",
      "Epoch:  385 Cost: 0.9084599742440777\n",
      "Epoch:  386 Cost: 0.9084202995223223\n",
      "Epoch:  387 Cost: 0.908377843655797\n",
      "Epoch:  388 Cost: 0.9083375651044558\n",
      "Epoch:  389 Cost: 0.9082965947965473\n",
      "Epoch:  390\n",
      "Accuracy: 0.8925833333333333\n",
      "Epoch:  390 Cost: 0.9082559789081791\n",
      "Epoch:  391 Cost: 0.9082166342351852\n",
      "Epoch:  392 Cost: 0.908175954523779\n",
      "Epoch:  393 Cost: 0.9081383031859813\n",
      "Epoch:  394 Cost: 0.9080977676019532\n",
      "Epoch:  395 Cost: 0.9080617773476006\n",
      "Epoch:  396 Cost: 0.9080215547413781\n",
      "Epoch:  397 Cost: 0.9079871303217255\n",
      "Epoch:  398 Cost: 0.9079473615292389\n",
      "Epoch:  399 Cost: 0.9079143865002879\n",
      "Epoch:  400\n",
      "Accuracy: 0.8938333333333334\n",
      "Epoch:  400 Cost: 0.907875186081245\n",
      "Epoch:  401 Cost: 0.9078435018919871\n",
      "Epoch:  402 Cost: 0.9078049496997006\n",
      "Epoch:  403 Cost: 0.9077743979595897\n",
      "Epoch:  404 Cost: 0.9077365694474345\n",
      "Epoch:  405 Cost: 0.9077070119995384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  406 Cost: 0.9076699378654803\n",
      "Epoch:  407 Cost: 0.9076411995689107\n",
      "Epoch:  408 Cost: 0.9076049063853635\n",
      "Epoch:  409 Cost: 0.9075768259652677\n",
      "Epoch:  410\n",
      "Accuracy: 0.89465\n",
      "Epoch:  410 Cost: 0.9075412970106344\n",
      "Epoch:  411 Cost: 0.9075137221580191\n",
      "Epoch:  412 Cost: 0.9074789713294892\n",
      "Epoch:  413 Cost: 0.90745175169394\n",
      "Epoch:  414 Cost: 0.9074177505830393\n",
      "Epoch:  415 Cost: 0.907390779585168\n",
      "Epoch:  416 Cost: 0.9073575208861717\n",
      "Epoch:  417 Cost: 0.9073306813804543\n",
      "Epoch:  418 Cost: 0.90729813363069\n",
      "Epoch:  419 Cost: 0.9072713256224197\n",
      "Epoch:  420\n",
      "Accuracy: 0.8953666666666666\n",
      "Epoch:  420 Cost: 0.9072394667240011\n",
      "Epoch:  421 Cost: 0.907212597527076\n",
      "Epoch:  422 Cost: 0.9071813993259786\n",
      "Epoch:  423 Cost: 0.9071544016759656\n",
      "Epoch:  424 Cost: 0.9071238057292356\n",
      "Epoch:  425 Cost: 0.9070962240093692\n",
      "Epoch:  426 Cost: 0.9070665467513788\n",
      "Epoch:  427 Cost: 0.9070383228983931\n",
      "Epoch:  428 Cost: 0.9070094569895177\n",
      "Epoch:  429 Cost: 0.9069806710958284\n",
      "Epoch:  430\n",
      "Accuracy: 0.896\n",
      "Epoch:  430 Cost: 0.9069522630841148\n",
      "Epoch:  431 Cost: 0.9069231791498918\n",
      "Epoch:  432 Cost: 0.9068951914775281\n",
      "Epoch:  433 Cost: 0.9068658242003371\n",
      "Epoch:  434 Cost: 0.9068382221867494\n",
      "Epoch:  435 Cost: 0.9068085747550361\n",
      "Epoch:  436 Cost: 0.9067813287428284\n",
      "Epoch:  437 Cost: 0.9067513996390357\n",
      "Epoch:  438 Cost: 0.9067244859083051\n",
      "Epoch:  439 Cost: 0.9066943263953967\n",
      "Epoch:  440\n",
      "Accuracy: 0.8965833333333333\n",
      "Epoch:  440 Cost: 0.9066677070716063\n",
      "Epoch:  441 Cost: 0.9066373387535914\n",
      "Epoch:  442 Cost: 0.9066109954365997\n",
      "Epoch:  443 Cost: 0.9065804430259715\n",
      "Epoch:  444 Cost: 0.9065542140824552\n",
      "Epoch:  445 Cost: 0.9065236779191621\n",
      "Epoch:  446 Cost: 0.9064975231210564\n",
      "Epoch:  447 Cost: 0.9064670598399771\n",
      "Epoch:  448 Cost: 0.9064409712503936\n",
      "Epoch:  449 Cost: 0.9064106349322559\n",
      "Epoch:  450\n",
      "Accuracy: 0.8974166666666666\n",
      "Epoch:  450 Cost: 0.906384622265353\n",
      "Epoch:  451 Cost: 0.9063544407567468\n",
      "Epoch:  452 Cost: 0.9063285061210303\n",
      "Epoch:  453 Cost: 0.9062985139026892\n",
      "Epoch:  454 Cost: 0.9062726562596206\n",
      "Epoch:  455 Cost: 0.9062428544491254\n",
      "Epoch:  456 Cost: 0.9062170682474755\n",
      "Epoch:  457 Cost: 0.9061874962548376\n",
      "Epoch:  458 Cost: 0.9061617927474213\n",
      "Epoch:  459 Cost: 0.9061324627643671\n",
      "Epoch:  460\n",
      "Accuracy: 0.89815\n",
      "Epoch:  460 Cost: 0.9061068482601967\n",
      "Epoch:  461 Cost: 0.9060777715904343\n",
      "Epoch:  462 Cost: 0.90605202904041\n",
      "Epoch:  463 Cost: 0.906023435440647\n",
      "Epoch:  464 Cost: 0.9059975430900943\n",
      "Epoch:  465 Cost: 0.9059694983966946\n",
      "Epoch:  466 Cost: 0.9059434640773087\n",
      "Epoch:  467 Cost: 0.9059159466310573\n",
      "Epoch:  468 Cost: 0.9058897767480018\n",
      "Epoch:  469 Cost: 0.9058628065783385\n",
      "Epoch:  470\n",
      "Accuracy: 0.8987333333333334\n",
      "Epoch:  470 Cost: 0.9058365083271234\n",
      "Epoch:  471 Cost: 0.9058100796598757\n",
      "Epoch:  472 Cost: 0.9057836596301588\n",
      "Epoch:  473 Cost: 0.9057577691191946\n",
      "Epoch:  474 Cost: 0.9057312295569053\n",
      "Epoch:  475 Cost: 0.9057058704520355\n",
      "Epoch:  476 Cost: 0.9056792110650659\n",
      "Epoch:  477 Cost: 0.9056543735793109\n",
      "Epoch:  478 Cost: 0.905627586632671\n",
      "Epoch:  479 Cost: 0.9056032512542911\n",
      "Epoch:  480\n",
      "Accuracy: 0.8993166666666667\n",
      "Epoch:  480 Cost: 0.9055763010616884\n",
      "Epoch:  481 Cost: 0.9055524613573158\n",
      "Epoch:  482 Cost: 0.9055253669215587\n",
      "Epoch:  483 Cost: 0.9055017985799295\n",
      "Epoch:  484 Cost: 0.9054747972357133\n",
      "Epoch:  485 Cost: 0.9054514677266884\n",
      "Epoch:  486 Cost: 0.9054245462586539\n",
      "Epoch:  487 Cost: 0.9054014436686729\n",
      "Epoch:  488 Cost: 0.9053745890465893\n",
      "Epoch:  489 Cost: 0.9053517174661381\n",
      "Epoch:  490\n",
      "Accuracy: 0.9\n",
      "Epoch:  490 Cost: 0.9053249204877295\n",
      "Epoch:  491 Cost: 0.9053022918588927\n",
      "Epoch:  492 Cost: 0.9052755591125868\n",
      "Epoch:  493 Cost: 0.9052531832285339\n",
      "Epoch:  494 Cost: 0.9052265066096783\n",
      "Epoch:  495 Cost: 0.9052043646688059\n",
      "Epoch:  496 Cost: 0.9051777139595513\n",
      "Epoch:  497 Cost: 0.9051558162875895\n",
      "Epoch:  498 Cost: 0.9051292032864147\n",
      "Epoch:  499 Cost: 0.9051075582618147\n",
      "Epoch:  500\n",
      "Accuracy: 0.9007333333333334\n",
      "Epoch:  500 Cost: 0.905080975790816\n",
      "Epoch:  501 Cost: 0.9050595822368344\n",
      "Epoch:  502 Cost: 0.9050330139918212\n",
      "Epoch:  503 Cost: 0.9050118751670341\n",
      "Epoch:  504 Cost: 0.9049853182909953\n",
      "Epoch:  505 Cost: 0.904964435737289\n",
      "Epoch:  506 Cost: 0.9049378813295044\n",
      "Epoch:  507 Cost: 0.9049172497252946\n",
      "Epoch:  508 Cost: 0.9048906877066122\n",
      "Epoch:  509 Cost: 0.9048702929850704\n",
      "Epoch:  510\n",
      "Accuracy: 0.9012666666666667\n",
      "Epoch:  510 Cost: 0.9048437059749322\n",
      "Epoch:  511 Cost: 0.9048235502852392\n",
      "Epoch:  512 Cost: 0.90479695035717\n",
      "Epoch:  513 Cost: 0.9047770435746539\n",
      "Epoch:  514 Cost: 0.9047504332498244\n",
      "Epoch:  515 Cost: 0.9047307658987367\n",
      "Epoch:  516 Cost: 0.9047041403359377\n",
      "Epoch:  517 Cost: 0.9046847073193894\n",
      "Epoch:  518 Cost: 0.9046580807789601\n",
      "Epoch:  519 Cost: 0.904638808758705\n",
      "Epoch:  520\n",
      "Accuracy: 0.902\n",
      "Epoch:  520 Cost: 0.9046122726107174\n",
      "Epoch:  521 Cost: 0.90459302457109\n",
      "Epoch:  522 Cost: 0.9045666686832785\n",
      "Epoch:  523 Cost: 0.9045474434000438\n",
      "Epoch:  524 Cost: 0.9045212887339233\n",
      "Epoch:  525 Cost: 0.9045020932983163\n",
      "Epoch:  526 Cost: 0.9044761170174246\n",
      "Epoch:  527 Cost: 0.9044569305850995\n",
      "Epoch:  528 Cost: 0.9044311219446488\n",
      "Epoch:  529 Cost: 0.9044119959390691\n",
      "Epoch:  530\n",
      "Accuracy: 0.9025666666666666\n",
      "Epoch:  530 Cost: 0.9043861872610358\n",
      "Epoch:  531 Cost: 0.9043672776472697\n",
      "Epoch:  532 Cost: 0.904341471962015\n",
      "Epoch:  533 Cost: 0.9043227877736691\n",
      "Epoch:  534 Cost: 0.9042970001868402\n",
      "Epoch:  535 Cost: 0.9042785308505658\n",
      "Epoch:  536 Cost: 0.9042527592158448\n",
      "Epoch:  537 Cost: 0.9042344957343005\n",
      "Epoch:  538 Cost: 0.9042087396235043\n",
      "Epoch:  539 Cost: 0.9041906772441404\n",
      "Epoch:  540\n",
      "Accuracy: 0.90325\n",
      "Epoch:  540 Cost: 0.904164949636147\n",
      "Epoch:  541 Cost: 0.9041470757816729\n",
      "Epoch:  542 Cost: 0.9041213738978702\n",
      "Epoch:  543 Cost: 0.9041036947941715\n",
      "Epoch:  544 Cost: 0.9040780378254891\n",
      "Epoch:  545 Cost: 0.9040605342387769\n",
      "Epoch:  546 Cost: 0.9040349074554178\n",
      "Epoch:  547 Cost: 0.904017571254341\n",
      "Epoch:  548 Cost: 0.9039919873028163\n",
      "Epoch:  549 Cost: 0.9039748163135904\n",
      "Epoch:  550\n",
      "Accuracy: 0.9038333333333334\n",
      "Epoch:  550 Cost: 0.9039492765424977\n",
      "Epoch:  551 Cost: 0.9039322703135382\n",
      "Epoch:  552 Cost: 0.9039067868072236\n",
      "Epoch:  553 Cost: 0.9038899418535103\n",
      "Epoch:  554 Cost: 0.9038645174393676\n",
      "Epoch:  555 Cost: 0.9038478121170535\n",
      "Epoch:  556 Cost: 0.903822438728392\n",
      "Epoch:  557 Cost: 0.9038058833304629\n",
      "Epoch:  558 Cost: 0.9037805784809454\n",
      "Epoch:  559 Cost: 0.9037641559722217\n",
      "Epoch:  560\n",
      "Accuracy: 0.9045333333333333\n",
      "Epoch:  560 Cost: 0.90373890735356\n",
      "Epoch:  561 Cost: 0.9037226185506521\n",
      "Epoch:  562 Cost: 0.9036974434950441\n",
      "Epoch:  563 Cost: 0.9036812808580039\n",
      "Epoch:  564 Cost: 0.9036561792799099\n",
      "Epoch:  565 Cost: 0.9036401515014794\n",
      "Epoch:  566 Cost: 0.9036151237433635\n",
      "Epoch:  567 Cost: 0.9035992032786427\n",
      "Epoch:  568 Cost: 0.9035742477420607\n",
      "Epoch:  569 Cost: 0.9035584507416523\n",
      "Epoch:  570\n",
      "Accuracy: 0.9052166666666667\n",
      "Epoch:  570 Cost: 0.9035335723863547\n",
      "Epoch:  571 Cost: 0.9035178744860763\n",
      "Epoch:  572 Cost: 0.9034930598787094\n",
      "Epoch:  573 Cost: 0.9034774731243239\n",
      "Epoch:  574 Cost: 0.9034527406816705\n",
      "Epoch:  575 Cost: 0.9034372522992277\n",
      "Epoch:  576 Cost: 0.9034125914042797\n",
      "Epoch:  577 Cost: 0.9033972120831925\n",
      "Epoch:  578 Cost: 0.9033726392315491\n",
      "Epoch:  579 Cost: 0.9033573556042755\n",
      "Epoch:  580\n",
      "Accuracy: 0.9057\n",
      "Epoch:  580 Cost: 0.9033328557304502\n",
      "Epoch:  581 Cost: 0.9033176724544109\n",
      "Epoch:  582 Cost: 0.9032932596546819\n",
      "Epoch:  583 Cost: 0.9032781648237236\n",
      "Epoch:  584 Cost: 0.9032538216446274\n",
      "Epoch:  585 Cost: 0.9032388230196401\n",
      "Epoch:  586 Cost: 0.9032145783095641\n",
      "Epoch:  587 Cost: 0.9031996657369368\n",
      "Epoch:  588 Cost: 0.9031755014722699\n",
      "Epoch:  589 Cost: 0.9031606855725378\n",
      "Epoch:  590\n",
      "Accuracy: 0.9060833333333334\n",
      "Epoch:  590 Cost: 0.9031366130019357\n",
      "Epoch:  591 Cost: 0.9031218764311676\n",
      "Epoch:  592 Cost: 0.9030978928750115\n",
      "Epoch:  593 Cost: 0.9030832480523648\n",
      "Epoch:  594 Cost: 0.9030593427849581\n",
      "Epoch:  595 Cost: 0.9030447755111382\n",
      "Epoch:  596 Cost: 0.9030209613620261\n",
      "Epoch:  597 Cost: 0.9030064686162158\n",
      "Epoch:  598 Cost: 0.9029827366364435\n",
      "Epoch:  599 Cost: 0.902968324958687\n",
      "Epoch:  600\n",
      "Accuracy: 0.9066166666666666\n",
      "Epoch:  600 Cost: 0.9029446872313623\n",
      "Epoch:  601 Cost: 0.9029303542168431\n",
      "Epoch:  602 Cost: 0.9029068022726521\n",
      "Epoch:  603 Cost: 0.9028925418015333\n",
      "Epoch:  604 Cost: 0.9028690771143092\n",
      "Epoch:  605 Cost: 0.9028548897971774\n",
      "Epoch:  606 Cost: 0.9028315121490373\n",
      "Epoch:  607 Cost: 0.9028174083494103\n",
      "Epoch:  608 Cost: 0.9027941187108331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  609 Cost: 0.9027800802501262\n",
      "Epoch:  610\n",
      "Accuracy: 0.9072\n",
      "Epoch:  610 Cost: 0.9027568779374859\n",
      "Epoch:  611 Cost: 0.9027429097059688\n",
      "Epoch:  612 Cost: 0.902719790685361\n",
      "Epoch:  613 Cost: 0.9027059056042503\n",
      "Epoch:  614 Cost: 0.9026828881282726\n",
      "Epoch:  615 Cost: 0.9026690781740495\n",
      "Epoch:  616 Cost: 0.9026461209951228\n",
      "Epoch:  617 Cost: 0.9026324149119965\n",
      "Epoch:  618 Cost: 0.9026094484333353\n",
      "Epoch:  619 Cost: 0.9025959121696849\n",
      "Epoch:  620\n",
      "Accuracy: 0.9077166666666666\n",
      "Epoch:  620 Cost: 0.9025729235049311\n",
      "Epoch:  621 Cost: 0.9025595594622668\n",
      "Epoch:  622 Cost: 0.902536561852521\n",
      "Epoch:  623 Cost: 0.9025233877789938\n",
      "Epoch:  624 Cost: 0.9025003845979722\n",
      "Epoch:  625 Cost: 0.9024873800790978\n",
      "Epoch:  626 Cost: 0.9024643620290008\n",
      "Epoch:  627 Cost: 0.9024514976254993\n",
      "Epoch:  628 Cost: 0.9024284959207258\n",
      "Epoch:  629 Cost: 0.9024157076598114\n",
      "Epoch:  630\n",
      "Accuracy: 0.9081666666666667\n",
      "Epoch:  630 Cost: 0.9023928033134874\n",
      "Epoch:  631 Cost: 0.9023800696610219\n",
      "Epoch:  632 Cost: 0.902357260434432\n",
      "Epoch:  633 Cost: 0.9023445870560901\n",
      "Epoch:  634 Cost: 0.9023218717681907\n",
      "Epoch:  635 Cost: 0.9023092560931404\n",
      "Epoch:  636 Cost: 0.9022866307802903\n",
      "Epoch:  637 Cost: 0.9022740688350204\n",
      "Epoch:  638 Cost: 0.9022515378592388\n",
      "Epoch:  639 Cost: 0.9022390283872365\n",
      "Epoch:  640\n",
      "Accuracy: 0.90875\n",
      "Epoch:  640 Cost: 0.9022165920004427\n",
      "Epoch:  641 Cost: 0.9022041416165455\n",
      "Epoch:  642 Cost: 0.9021817931643573\n",
      "Epoch:  643 Cost: 0.9021693819002152\n",
      "Epoch:  644 Cost: 0.9021471245791934\n",
      "Epoch:  645 Cost: 0.902134759084269\n",
      "Epoch:  646 Cost: 0.9021125987671307\n",
      "Epoch:  647 Cost: 0.9021002827154632\n",
      "Epoch:  648 Cost: 0.902078217569034\n",
      "Epoch:  649 Cost: 0.9020659593577105\n",
      "Epoch:  650\n",
      "Accuracy: 0.9090833333333334\n",
      "Epoch:  650 Cost: 0.9020439956204136\n",
      "Epoch:  651 Cost: 0.9020317867677099\n",
      "Epoch:  652 Cost: 0.9020099230246273\n",
      "Epoch:  653 Cost: 0.9019977569092267\n",
      "Epoch:  654 Cost: 0.9019759912437598\n",
      "Epoch:  655 Cost: 0.9019638669532625\n",
      "Epoch:  656 Cost: 0.9019421921124695\n",
      "Epoch:  657 Cost: 0.901930109502349\n",
      "Epoch:  658 Cost: 0.9019085346844921\n",
      "Epoch:  659 Cost: 0.901896497591898\n",
      "Epoch:  660\n",
      "Accuracy: 0.9094833333333333\n",
      "Epoch:  660 Cost: 0.9018750248960278\n",
      "Epoch:  661 Cost: 0.9018630277916638\n",
      "Epoch:  662 Cost: 0.9018416545015424\n",
      "Epoch:  663 Cost: 0.9018296998123811\n",
      "Epoch:  664 Cost: 0.9018084251690145\n",
      "Epoch:  665 Cost: 0.9017965134294998\n",
      "Epoch:  666 Cost: 0.9017753449366549\n",
      "Epoch:  667 Cost: 0.9017634541266489\n",
      "Epoch:  668 Cost: 0.9017423695794957\n",
      "Epoch:  669 Cost: 0.9017305109330469\n",
      "Epoch:  670\n",
      "Accuracy: 0.90995\n",
      "Epoch:  670 Cost: 0.9017095241748165\n",
      "Epoch:  671 Cost: 0.9016977016564752\n",
      "Epoch:  672 Cost: 0.9016768176846898\n",
      "Epoch:  673 Cost: 0.9016650214182013\n",
      "Epoch:  674 Cost: 0.9016442411910213\n",
      "Epoch:  675 Cost: 0.9016324804782787\n",
      "Epoch:  676 Cost: 0.9016117996017817\n",
      "Epoch:  677 Cost: 0.9016000711004157\n",
      "Epoch:  678 Cost: 0.9015794940814025\n",
      "Epoch:  679 Cost: 0.9015677887858607\n",
      "Epoch:  680\n",
      "Accuracy: 0.91055\n",
      "Epoch:  680 Cost: 0.9015473132634659\n",
      "Epoch:  681 Cost: 0.9015356345546258\n",
      "Epoch:  682 Cost: 0.9015152703285486\n",
      "Epoch:  683 Cost: 0.9015036176149349\n",
      "Epoch:  684 Cost: 0.9014833544818145\n",
      "Epoch:  685 Cost: 0.9014717304283586\n",
      "Epoch:  686 Cost: 0.9014515734493225\n",
      "Epoch:  687 Cost: 0.9014399619662269\n",
      "Epoch:  688 Cost: 0.9014199027286108\n",
      "Epoch:  689 Cost: 0.9014083050151971\n",
      "Epoch:  690\n",
      "Accuracy: 0.9109166666666667\n",
      "Epoch:  690 Cost: 0.9013883577756613\n",
      "Epoch:  691 Cost: 0.9013767792036612\n",
      "Epoch:  692 Cost: 0.901356941055685\n",
      "Epoch:  693 Cost: 0.9013453811305937\n",
      "Epoch:  694 Cost: 0.9013256518340519\n",
      "Epoch:  695 Cost: 0.9013141065543602\n",
      "Epoch:  696 Cost: 0.9012944959741278\n",
      "Epoch:  697 Cost: 0.9012829743292253\n",
      "Epoch:  698 Cost: 0.9012634847367622\n",
      "Epoch:  699 Cost: 0.9012519868157497\n",
      "Epoch:  700\n",
      "Accuracy: 0.9112666666666667\n",
      "Epoch:  700 Cost: 0.9012326189754275\n",
      "Epoch:  701 Cost: 0.9012211313591869\n",
      "Epoch:  702 Cost: 0.9012018747966889\n",
      "Epoch:  703 Cost: 0.9011903958975432\n",
      "Epoch:  704 Cost: 0.9011712515459018\n",
      "Epoch:  705 Cost: 0.9011597769417002\n",
      "Epoch:  706 Cost: 0.9011407500268576\n",
      "Epoch:  707 Cost: 0.9011292775835652\n",
      "Epoch:  708 Cost: 0.9011103604246834\n",
      "Epoch:  709 Cost: 0.9010988923018666\n",
      "Epoch:  710\n",
      "Accuracy: 0.9117666666666666\n",
      "Epoch:  710 Cost: 0.9010800945676334\n",
      "Epoch:  711 Cost: 0.9010686427603826\n",
      "Epoch:  712 Cost: 0.9010499739220531\n",
      "Epoch:  713 Cost: 0.9010385319080262\n",
      "Epoch:  714 Cost: 0.9010199931623686\n",
      "Epoch:  715 Cost: 0.9010085549254206\n",
      "Epoch:  716 Cost: 0.9009901420743508\n",
      "Epoch:  717 Cost: 0.9009787002750186\n",
      "Epoch:  718 Cost: 0.9009604013716611\n",
      "Epoch:  719 Cost: 0.9009489663793517\n",
      "Epoch:  720\n",
      "Accuracy: 0.9121\n",
      "Epoch:  720 Cost: 0.9009307958393975\n",
      "Epoch:  721 Cost: 0.9009193620089633\n",
      "Epoch:  722 Cost: 0.9009013187898098\n",
      "Epoch:  723 Cost: 0.9008898868244813\n",
      "Epoch:  724 Cost: 0.9008719716781389\n",
      "Epoch:  725 Cost: 0.9008605385772215\n",
      "Epoch:  726 Cost: 0.9008427540532137\n",
      "Epoch:  727 Cost: 0.9008313177768782\n",
      "Epoch:  728 Cost: 0.9008136543774563\n",
      "Epoch:  729 Cost: 0.9008022098751775\n",
      "Epoch:  730\n",
      "Accuracy: 0.9124833333333333\n",
      "Epoch:  730 Cost: 0.9007846646168208\n",
      "Epoch:  731 Cost: 0.9007732135156806\n",
      "Epoch:  732 Cost: 0.9007557884805599\n",
      "Epoch:  733 Cost: 0.9007443217092868\n",
      "Epoch:  734 Cost: 0.9007270229360674\n",
      "Epoch:  735 Cost: 0.9007155455764452\n",
      "Epoch:  736 Cost: 0.9006983741355035\n",
      "Epoch:  737 Cost: 0.9006868857209506\n",
      "Epoch:  738 Cost: 0.9006698373235633\n",
      "Epoch:  739 Cost: 0.9006583417525518\n",
      "Epoch:  740\n",
      "Accuracy: 0.9127166666666666\n",
      "Epoch:  740 Cost: 0.9006414203863637\n",
      "Epoch:  741 Cost: 0.9006299180706406\n",
      "Epoch:  742 Cost: 0.9006131231556916\n",
      "Epoch:  743 Cost: 0.9006016104761919\n",
      "Epoch:  744 Cost: 0.9005849464429113\n",
      "Epoch:  745 Cost: 0.9005734278263082\n",
      "Epoch:  746 Cost: 0.9005568947801631\n",
      "Epoch:  747 Cost: 0.9005453615517218\n",
      "Epoch:  748 Cost: 0.9005289548942852\n",
      "Epoch:  749 Cost: 0.9005174102706955\n",
      "Epoch:  750\n",
      "Accuracy: 0.9130333333333334\n",
      "Epoch:  750 Cost: 0.9005011275718593\n",
      "Epoch:  751 Cost: 0.9004895738548422\n",
      "Epoch:  752 Cost: 0.9004734176003549\n",
      "Epoch:  753 Cost: 0.900461848548133\n",
      "Epoch:  754 Cost: 0.900445812474808\n",
      "Epoch:  755 Cost: 0.900434226599583\n",
      "Epoch:  756 Cost: 0.900418315483602\n",
      "Epoch:  757 Cost: 0.9004067123037797\n",
      "Epoch:  758 Cost: 0.9003909249806936\n",
      "Epoch:  759 Cost: 0.9003793124081352\n",
      "Epoch:  760\n",
      "Accuracy: 0.9134833333333333\n",
      "Epoch:  760 Cost: 0.900363649536758\n",
      "Epoch:  761 Cost: 0.9003520269365536\n",
      "Epoch:  762 Cost: 0.9003364851025083\n",
      "Epoch:  763 Cost: 0.9003248523704865\n",
      "Epoch:  764 Cost: 0.9003094329497867\n",
      "Epoch:  765 Cost: 0.9002977784034936\n",
      "Epoch:  766 Cost: 0.9002824724148878\n",
      "Epoch:  767 Cost: 0.9002707964136163\n",
      "Epoch:  768 Cost: 0.9002556099471436\n",
      "Epoch:  769 Cost: 0.9002439212320259\n",
      "Epoch:  770\n",
      "Accuracy: 0.9137666666666666\n",
      "Epoch:  770 Cost: 0.9002288533002915\n",
      "Epoch:  771 Cost: 0.9002171536182012\n",
      "Epoch:  772 Cost: 0.900202199937314\n",
      "Epoch:  773 Cost: 0.9001904933070946\n",
      "Epoch:  774 Cost: 0.9001756600810064\n",
      "Epoch:  775 Cost: 0.900163940707847\n",
      "Epoch:  776 Cost: 0.9001492226618358\n",
      "Epoch:  777 Cost: 0.9001374847591843\n",
      "Epoch:  778 Cost: 0.9001228805709282\n",
      "Epoch:  779 Cost: 0.9001111322144982\n",
      "Epoch:  780\n",
      "Accuracy: 0.9139666666666667\n",
      "Epoch:  780 Cost: 0.9000966376263315\n",
      "Epoch:  781 Cost: 0.9000848787473358\n",
      "Epoch:  782 Cost: 0.9000704955359446\n",
      "Epoch:  783 Cost: 0.9000587298557796\n",
      "Epoch:  784 Cost: 0.9000444519411174\n",
      "Epoch:  785 Cost: 0.9000326717169852\n",
      "Epoch:  786 Cost: 0.9000185012303956\n",
      "Epoch:  787 Cost: 0.9000067118249119\n",
      "Epoch:  788 Cost: 0.899992649108827\n",
      "Epoch:  789 Cost: 0.8999808508289995\n",
      "Epoch:  790\n",
      "Accuracy: 0.91425\n",
      "Epoch:  790 Cost: 0.8999668929817061\n",
      "Epoch:  791 Cost: 0.8999550926859922\n",
      "Epoch:  792 Cost: 0.8999412293175495\n",
      "Epoch:  793 Cost: 0.8999294168946554\n",
      "Epoch:  794 Cost: 0.8999156500184812\n",
      "Epoch:  795 Cost: 0.8999038295431608\n",
      "Epoch:  796 Cost: 0.8998901606527946\n",
      "Epoch:  797 Cost: 0.8998783304520204\n",
      "Epoch:  798 Cost: 0.8998647557456664\n",
      "Epoch:  799 Cost: 0.8998529243229054\n",
      "Epoch:  800\n",
      "Accuracy: 0.91465\n",
      "Epoch:  800 Cost: 0.8998394530723118\n",
      "Epoch:  801 Cost: 0.8998276282203969\n",
      "Epoch:  802 Cost: 0.8998142473571323\n",
      "Epoch:  803 Cost: 0.8998024218461906\n",
      "Epoch:  804 Cost: 0.8997891313318548\n",
      "Epoch:  805 Cost: 0.8997773070748273\n",
      "Epoch:  806 Cost: 0.8997641039479751\n",
      "Epoch:  807 Cost: 0.8997522806915704\n",
      "Epoch:  808 Cost: 0.8997391668722289\n",
      "Epoch:  809 Cost: 0.8997273391589001\n",
      "Epoch:  810\n",
      "Accuracy: 0.9149666666666667\n",
      "Epoch:  810 Cost: 0.8997142998351104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  811 Cost: 0.8997024664177228\n",
      "Epoch:  812 Cost: 0.8996895016315236\n",
      "Epoch:  813 Cost: 0.8996776707714038\n",
      "Epoch:  814 Cost: 0.8996647815877694\n",
      "Epoch:  815 Cost: 0.8996529580721104\n",
      "Epoch:  816 Cost: 0.8996401465692736\n",
      "Epoch:  817 Cost: 0.8996283296768519\n",
      "Epoch:  818 Cost: 0.8996155884842053\n",
      "Epoch:  819 Cost: 0.8996037674455832\n",
      "Epoch:  820\n",
      "Accuracy: 0.9152833333333333\n",
      "Epoch:  820 Cost: 0.899591092231894\n",
      "Epoch:  821 Cost: 0.899579282376094\n",
      "Epoch:  822 Cost: 0.8995666801161896\n",
      "Epoch:  823 Cost: 0.8995548751928667\n",
      "Epoch:  824 Cost: 0.899542342877745\n",
      "Epoch:  825 Cost: 0.89953055139065\n",
      "Epoch:  826 Cost: 0.899518082253049\n",
      "Epoch:  827 Cost: 0.8995063010238519\n",
      "Epoch:  828 Cost: 0.8994938938139447\n",
      "Epoch:  829 Cost: 0.8994821275700415\n",
      "Epoch:  830\n",
      "Accuracy: 0.91555\n",
      "Epoch:  830 Cost: 0.8994697804415158\n",
      "Epoch:  831 Cost: 0.8994580260746369\n",
      "Epoch:  832 Cost: 0.8994457389730143\n",
      "Epoch:  833 Cost: 0.8994339974597081\n",
      "Epoch:  834 Cost: 0.8994217663773639\n",
      "Epoch:  835 Cost: 0.8994100357059325\n",
      "Epoch:  836 Cost: 0.899397863175678\n",
      "Epoch:  837 Cost: 0.8993861476572284\n",
      "Epoch:  838 Cost: 0.8993740134927897\n",
      "Epoch:  839 Cost: 0.8993622975549269\n",
      "Epoch:  840\n",
      "Accuracy: 0.91595\n",
      "Epoch:  840 Cost: 0.8993502177756325\n",
      "Epoch:  841 Cost: 0.8993385264823371\n",
      "Epoch:  842 Cost: 0.8993264956458286\n",
      "Epoch:  843 Cost: 0.8993148175156663\n",
      "Epoch:  844 Cost: 0.8993028356925116\n",
      "Epoch:  845 Cost: 0.8992911802887216\n",
      "Epoch:  846 Cost: 0.899279248285532\n",
      "Epoch:  847 Cost: 0.8992676132909945\n",
      "Epoch:  848 Cost: 0.8992557247369877\n",
      "Epoch:  849 Cost: 0.8992441082737509\n",
      "Epoch:  850\n",
      "Accuracy: 0.91625\n",
      "Epoch:  850 Cost: 0.8992322674943837\n",
      "Epoch:  851 Cost: 0.8992206774304204\n",
      "Epoch:  852 Cost: 0.8992088869727916\n",
      "Epoch:  853 Cost: 0.8991973243712221\n",
      "Epoch:  854 Cost: 0.8991855832318569\n",
      "Epoch:  855 Cost: 0.8991740479944763\n",
      "Epoch:  856 Cost: 0.8991623519174367\n",
      "Epoch:  857 Cost: 0.89915083955762\n",
      "Epoch:  858 Cost: 0.8991391840551525\n",
      "Epoch:  859 Cost: 0.8991276941138385\n",
      "Epoch:  860\n",
      "Accuracy: 0.9165666666666666\n",
      "Epoch:  860 Cost: 0.8991160783591865\n",
      "Epoch:  861 Cost: 0.8991046155974819\n",
      "Epoch:  862 Cost: 0.899093037399981\n",
      "Epoch:  863 Cost: 0.8990815957356523\n",
      "Epoch:  864 Cost: 0.8990700534003121\n",
      "Epoch:  865 Cost: 0.89905863842838\n",
      "Epoch:  866 Cost: 0.8990471321247329\n",
      "Epoch:  867 Cost: 0.8990357459119134\n",
      "Epoch:  868 Cost: 0.8990242857684335\n",
      "Epoch:  869 Cost: 0.8990129388086213\n",
      "Epoch:  870\n",
      "Accuracy: 0.9168333333333333\n",
      "Epoch:  870 Cost: 0.8990015233359432\n",
      "Epoch:  871 Cost: 0.8989902045469411\n",
      "Epoch:  872 Cost: 0.8989788302477951\n",
      "Epoch:  873 Cost: 0.8989675402687889\n",
      "Epoch:  874 Cost: 0.8989561959949419\n",
      "Epoch:  875 Cost: 0.8989449280062619\n",
      "Epoch:  876 Cost: 0.8989336217386564\n",
      "Epoch:  877 Cost: 0.8989223833078769\n",
      "Epoch:  878 Cost: 0.8989111037589422\n",
      "Epoch:  879 Cost: 0.8988997984243015\n",
      "Epoch:  880\n",
      "Accuracy: 0.91715\n",
      "Epoch:  880 Cost: 0.8988884606862464\n",
      "Epoch:  881 Cost: 0.8988771744008518\n",
      "Epoch:  882 Cost: 0.8988658614459543\n",
      "Epoch:  883 Cost: 0.8988545984343908\n",
      "Epoch:  884 Cost: 0.8988433178429568\n",
      "Epoch:  885 Cost: 0.8988320811624492\n",
      "Epoch:  886 Cost: 0.898820831195744\n",
      "Epoch:  887 Cost: 0.8988096270086704\n",
      "Epoch:  888 Cost: 0.898798408856105\n",
      "Epoch:  889 Cost: 0.8987872311490194\n",
      "Epoch:  890\n",
      "Accuracy: 0.9174\n",
      "Epoch:  890 Cost: 0.8987760453167627\n",
      "Epoch:  891 Cost: 0.8987648999175748\n",
      "Epoch:  892 Cost: 0.8987537480852918\n",
      "Epoch:  893 Cost: 0.8987426307072544\n",
      "Epoch:  894 Cost: 0.8987315085946482\n",
      "Epoch:  895 Cost: 0.8987204249997992\n",
      "Epoch:  896 Cost: 0.8987093381935806\n",
      "Epoch:  897 Cost: 0.898698283809958\n",
      "Epoch:  898 Cost: 0.8986872264980472\n",
      "Epoch:  899 Cost: 0.8986762001834276\n",
      "Epoch:  900\n",
      "Accuracy: 0.9177\n",
      "Epoch:  900 Cost: 0.89866517020013\n",
      "Epoch:  901 Cost: 0.8986541687656755\n",
      "Epoch:  902 Cost: 0.8986431708054136\n",
      "Epoch:  903 Cost: 0.8986321994348523\n",
      "Epoch:  904 Cost: 0.8986212305027316\n",
      "Epoch:  905 Cost: 0.8986102885368902\n",
      "Epoch:  906 Cost: 0.8985993525301381\n",
      "Epoch:  907 Cost: 0.8985884408485831\n",
      "Epoch:  908 Cost: 0.8985775345421302\n",
      "Epoch:  909 Cost: 0.8985666492345888\n",
      "Epoch:  910\n",
      "Accuracy: 0.9179833333333334\n",
      "Epoch:  910 Cost: 0.8985557696262123\n",
      "Epoch:  911 Cost: 0.898544908936153\n",
      "Epoch:  912 Cost: 0.8985340573702806\n",
      "Epoch:  913 Cost: 0.8985232195080504\n",
      "Epoch:  914 Cost: 0.8985123952590063\n",
      "Epoch:  915 Cost: 0.8985015842049484\n",
      "Epoch:  916 Cost: 0.8984907841633365\n",
      "Epoch:  917 Cost: 0.8984800014339946\n",
      "Epoch:  918 Cost: 0.8984692283304566\n",
      "Epoch:  919 Cost: 0.8984584674730309\n",
      "Epoch:  920\n",
      "Accuracy: 0.9182833333333333\n",
      "Epoch:  920 Cost: 0.8984477208869451\n",
      "Epoch:  921 Cost: 0.8984369890477298\n",
      "Epoch:  922 Cost: 0.8984262694006362\n",
      "Epoch:  923 Cost: 0.8984155629357898\n",
      "Epoch:  924 Cost: 0.898404874209878\n",
      "Epoch:  925 Cost: 0.8983941997742306\n",
      "Epoch:  926 Cost: 0.8983835394549682\n",
      "Epoch:  927 Cost: 0.8983728925684965\n",
      "Epoch:  928 Cost: 0.8983622631521049\n",
      "Epoch:  929 Cost: 0.8983516464745928\n",
      "Epoch:  930\n",
      "Accuracy: 0.9186833333333333\n",
      "Epoch:  930 Cost: 0.8983410443424779\n",
      "Epoch:  931 Cost: 0.8983304525931949\n",
      "Epoch:  932 Cost: 0.8983198795666204\n",
      "Epoch:  933 Cost: 0.8983093176252115\n",
      "Epoch:  934 Cost: 0.8982987708015492\n",
      "Epoch:  935 Cost: 0.8982882341896056\n",
      "Epoch:  936 Cost: 0.8982777121035245\n",
      "Epoch:  937 Cost: 0.8982672015606985\n",
      "Epoch:  938 Cost: 0.8982567079759938\n",
      "Epoch:  939 Cost: 0.8982462264684672\n",
      "Epoch:  940\n",
      "Accuracy: 0.9188333333333333\n",
      "Epoch:  940 Cost: 0.8982357634855815\n",
      "Epoch:  941 Cost: 0.8982253139161227\n",
      "Epoch:  942 Cost: 0.8982148804395101\n",
      "Epoch:  943 Cost: 0.8982044585533906\n",
      "Epoch:  944 Cost: 0.8981940528430348\n",
      "Epoch:  945 Cost: 0.8981836537103227\n",
      "Epoch:  946 Cost: 0.8981732700505148\n",
      "Epoch:  947 Cost: 0.898162897964435\n",
      "Epoch:  948 Cost: 0.8981525411898923\n",
      "Epoch:  949 Cost: 0.8981421937873347\n",
      "Epoch:  950\n",
      "Accuracy: 0.9191\n",
      "Epoch:  950 Cost: 0.8981318636438421\n",
      "Epoch:  951 Cost: 0.8981215464670351\n",
      "Epoch:  952 Cost: 0.8981112450355839\n",
      "Epoch:  953 Cost: 0.8981009545710421\n",
      "Epoch:  954 Cost: 0.8980906789356181\n",
      "Epoch:  955 Cost: 0.8980804125561871\n",
      "Epoch:  956 Cost: 0.8980701604769176\n",
      "Epoch:  957 Cost: 0.8980599182566619\n",
      "Epoch:  958 Cost: 0.8980496885947328\n",
      "Epoch:  959 Cost: 0.8980394683291505\n",
      "Epoch:  960\n",
      "Accuracy: 0.9194\n",
      "Epoch:  960 Cost: 0.8980292620725557\n",
      "Epoch:  961 Cost: 0.8980190672716755\n",
      "Epoch:  962 Cost: 0.8980088853619735\n",
      "Epoch:  963 Cost: 0.8979987129966766\n",
      "Epoch:  964 Cost: 0.8979885537827056\n",
      "Epoch:  965 Cost: 0.8979784016354352\n",
      "Epoch:  966 Cost: 0.897968262893446\n",
      "Epoch:  967 Cost: 0.897958133384287\n",
      "Epoch:  968 Cost: 0.897948017063788\n",
      "Epoch:  969 Cost: 0.8979379082781547\n",
      "Epoch:  970\n",
      "Accuracy: 0.9196666666666666\n",
      "Epoch:  970 Cost: 0.8979278158182313\n",
      "Epoch:  971 Cost: 0.8979177320863307\n",
      "Epoch:  972 Cost: 0.8979076613987309\n",
      "Epoch:  973 Cost: 0.8978975940817499\n",
      "Epoch:  974 Cost: 0.8978875403466666\n",
      "Epoch:  975 Cost: 0.8978774906457518\n",
      "Epoch:  976 Cost: 0.8978674576040949\n",
      "Epoch:  977 Cost: 0.8978574334351078\n",
      "Epoch:  978 Cost: 0.8978474273861992\n",
      "Epoch:  979 Cost: 0.8978374298082193\n",
      "Epoch:  980\n",
      "Accuracy: 0.9198833333333334\n",
      "Epoch:  980 Cost: 0.8978274446717471\n",
      "Epoch:  981 Cost: 0.8978174666397086\n",
      "Epoch:  982 Cost: 0.8978075035515996\n",
      "Epoch:  983 Cost: 0.8977975490958288\n",
      "Epoch:  984 Cost: 0.8977876081491537\n",
      "Epoch:  985 Cost: 0.8977776754531366\n",
      "Epoch:  986 Cost: 0.8977677602696256\n",
      "Epoch:  987 Cost: 0.8977578558747548\n",
      "Epoch:  988 Cost: 0.8977479702278827\n",
      "Epoch:  989 Cost: 0.8977380901894152\n",
      "Epoch:  990\n",
      "Accuracy: 0.9201833333333334\n",
      "Epoch:  990 Cost: 0.8977282238695279\n",
      "Epoch:  991 Cost: 0.8977183301597731\n",
      "Epoch:  992 Cost: 0.8977084393442755\n",
      "Epoch:  993 Cost: 0.8976985497328415\n",
      "Epoch:  994 Cost: 0.897688677796452\n",
      "Epoch:  995 Cost: 0.8976788124510734\n",
      "Epoch:  996 Cost: 0.8976689638151693\n",
      "Epoch:  997 Cost: 0.8976591179334229\n",
      "Epoch:  998 Cost: 0.8976492867919407\n",
      "Epoch:  999 Cost: 0.8976394596920272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f98bf4c0160>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist_net = NeuralNet('multi_logloss')\n",
    "mnist_net.addLayer(64, 'reLu')\n",
    "mnist_net.addLayer(10, 'softmax')\n",
    "costs = mnist_net.fit(X_train, y_train, 1000, 60000)\n",
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.96190400e-07],\n",
       "       [2.86105747e-09],\n",
       "       [4.35264544e-10],\n",
       "       [8.11213963e-01],\n",
       "       [1.38347722e-11],\n",
       "       [1.54519132e-01],\n",
       "       [1.26920023e-11],\n",
       "       [1.10273170e-03],\n",
       "       [1.54051086e-04],\n",
       "       [3.30096224e-02]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mnist_net.predict(X_test)\n",
    "pred_acc[i] = 100*np.sum(y_t == y_pred, axis=0) / X_t.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e3ba3ae7e706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m array([ 0.        ,  0.        ,  0.        , 12.34557301,  0.        ,\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0;36m1.44547623\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m \u001b[0;36m14.42673622\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11.38719467\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;36m5.75124653\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m27.30509425\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14.13349396\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16.0829925\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m \u001b[0;36m12.09050004\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.24301809\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m  \u001b[0;36m0.81315813\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'array' is not defined"
     ]
    }
   ],
   "source": [
    "array([ 0.        ,  0.        ,  0.        , 12.34557301,  0.        ,\n",
    "        1.44547623,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "        0.        ,  0.        , 14.42673622, 11.38719467,  0.        ,\n",
    "        5.75124653, 27.30509425, 14.13349396, 16.0829925 ,  0.        ,\n",
    "        0.        , 12.09050004,  0.24301809,  0.        ,  0.81315813,\n",
    "       36.25892928,  3.02591285,  0.        ,  7.69811406,  0.        ,\n",
    "        0.        ,  0.        , 23.42250571,  6.57030016,  2.73966946,\n",
    "        0.        ,  0.        ,  0.        , 13.62185096,  7.91879562,\n",
    "       22.05083927,  0.        , 29.15894322, 15.4927778 ,  0.        ,\n",
    "        0.        , 16.55078204, 13.3898415 ,  0.        ,  0.        ,\n",
    "        0.        , 19.41925836,  0.        ,  0.        ,  0.        ,\n",
    "       15.12313668,  8.86435797, 14.11965046,  0.        ,  0.        ,\n",
    "        0.        ,  0.37141094,  3.19106955, 27.12674025])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[:, 99].reshape(28, 28))\n",
    "X_test[:, 90].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
