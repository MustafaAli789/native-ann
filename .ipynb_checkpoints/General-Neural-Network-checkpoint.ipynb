{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipdb\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    #Activation Functions\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    def d_tanh(self, x):\n",
    "        return 1 - np.square(np.tanh(x))\n",
    "    def sigmoid(self, x):\n",
    "#         return 1/(1+ np.exp(-x))\n",
    "        return expit(x)\n",
    "    def d_sigmoid(self, x):\n",
    "        return (1 - self.sigmoid(x)) * self.sigmoid(x)\n",
    "    def ReLu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    def d_ReLu(self, Z):\n",
    "        return Z > 0\n",
    "    \n",
    "    #For output layer, useful for multiclass classification\n",
    "    def softmax(self, Z):\n",
    "#         expZ = np.exp(Z - np.max(Z))\n",
    "#         return expZ / expZ.sum(axis=0, keepdims=True)\n",
    "         return np.exp(Z) / sum(np.exp(Z))\n",
    "    def d_softmax(self, Z):\n",
    "        pass\n",
    "    \n",
    "    activationFunctions = {\n",
    "        'tanh': (tanh, d_tanh),\n",
    "        'sigmoid': (sigmoid, d_sigmoid),\n",
    "        'reLu': (ReLu, d_ReLu),\n",
    "        'softmax': (softmax, d_softmax)\n",
    "    }\n",
    "    \n",
    "    #Input -> num of neurons in prev layer, Neurons --> num neurons in cur layer, Activation -> activation fxn to use\n",
    "    def __init__(self, inputs, neurons, activation):\n",
    "        self.neurons = neurons\n",
    "        self.W = np.random.rand(neurons, inputs) - 0.5\n",
    "        self.b = np.zeros((neurons, 1))\n",
    "        self.Z = None\n",
    "        self.A_prev = None\n",
    "        self.act, self.d_act = self.activationFunctions.get(activation)\n",
    "        \n",
    "    def initializeWeights(self, inputs, neurons):\n",
    "        self.W = np.random.randn(neurons, inputs)\n",
    "        \n",
    "    def getNeuronCount(self):\n",
    "        return self.neurons\n",
    "    \n",
    "    def feedForward(self, A_prev):\n",
    "        #ipdb.set_trace()\n",
    "        self.A_prev = A_prev\n",
    "        self.Z = np.dot(self.W, self.A_prev) + self.b\n",
    "        self.A = self.act(self, self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    #All derivatives are wrt to cost\n",
    "    #Expects dA of cur layer\n",
    "    #Special case where doing multi class classification with mutli class logloss, you can get the dZ wrt dC directly without having to first get dA\n",
    "    def backprop(self, dA, learning_rate, dZ_Special):\n",
    "        \n",
    "        #ipdb.set_trace()\n",
    "        \n",
    "        #elementt by element matrix multip, not a normal dot prod since both matrices have same shape (essentialyl scalar)\n",
    "        dZ = np.multiply(self.d_act(self, self.Z), dA) if dZ_Special.any() == None else dZ_Special\n",
    "        \n",
    "         # need to normalize weights and divide by number of samples\n",
    "        # because it is actually a sum of weights\n",
    "        dW = 1*dZ.shape[1] * np.dot(dZ, self.A_prev.T)\n",
    "        \n",
    "        # this is to match shape since biases is supposed to be a col vector with 1 col but dZ has m cols\n",
    "        # w/ m being num of samples, we want to take avg of all samples in dZ (i.e on a row by row basis, sum of cols\n",
    "        # and divide by total num of smamples)\n",
    "        db = 1 / dZ.shape[1] * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        \n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "        \n",
    "        self.W = self.W - learning_rate * dW\n",
    "        self.b = self.b - learning_rate * db\n",
    "        return dA_prev\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    #Loss Functions, mse for regression, logloss for classification\n",
    "    def mse(self, a, target):\n",
    "        return np.square(a-target)\n",
    "    \n",
    "    def d_mse(self, a, target):\n",
    "        return 2*(a-target)\n",
    "    \n",
    "    def binary_logloss(self, a, target):\n",
    "        return -(target*np.log(a) + (1-target)*np.log(1-a))\n",
    "    \n",
    "    def d_binary_logloss(self, a, target):\n",
    "        return (a - target)/(a*(1 - a))\n",
    "    \n",
    "    def multi_logloss(self, a, target, eps=1e-15):\n",
    "        predictions = np.clip(a, eps, 1 - eps)\n",
    "\n",
    "        # normalize row sums to 1\n",
    "        predictions /= predictions.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        return -np.sum(target * np.log(predictions))/predictions.shape[0]\n",
    "    \n",
    "    def d_multi_logloss(self, a, target):\n",
    "        return np.zeros(a.shape) # kinda just a placeholder\n",
    "    \n",
    "    lossFunctions = {\n",
    "        'mse': (mse, d_mse),\n",
    "        'binary_logloss': (binary_logloss, d_binary_logloss),\n",
    "        'multi_logloss': (multi_logloss, d_multi_logloss)\n",
    "    }\n",
    "        \n",
    "    #LossFunction is either mse of logloss\n",
    "    def __init__(self, lossFunction):\n",
    "        self.layers = []\n",
    "        self.learning_rate = 0.1\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 10\n",
    "        self.classification = False if lossFunction == 'mse' else True\n",
    "        self.lossFunction = lossFunction\n",
    "        self.loss, self.d_loss = self.lossFunctions.get(lossFunction)\n",
    "    \n",
    "    #Units is 1-n and activationFunction is 'ReLu', 'sigmoid', 'tanh', or 'softmax'\n",
    "    def addLayer(self, units, activationFunction):\n",
    "        prevLayerNeuronCount = self.layers[-1].getNeuronCount() if len(self.layers) > 0 else 0\n",
    "        self.layers.append(Layer(prevLayerNeuronCount, units, activationFunction))\n",
    "        \n",
    "    def getNumBatches(self, num_samples, batch_size):\n",
    "        if (num_samples == batch_size):\n",
    "            return 1\n",
    "        elif (num_samples > batch_size):\n",
    "            if (num_samples % batch_size == 0):\n",
    "                return num_samples // batch_size\n",
    "            else:\n",
    "                return (num_samples // batch_size) + 1\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def oneHot(self, x):\n",
    "        one_hot_X = np.zeros((x.max() + 1, x.size)) #making a matrix of 10 x m\n",
    "        one_hot_X[x, np.arange(x.size)] = 1 #going through all cols and setting the row w/ index corresponding to the y to 1, its very easy to iterate over numpy arays like this apparently\n",
    "        return one_hot_X\n",
    "    \n",
    "    def fit(self, X, y, epochs = None, batch_size = None, learning_rate = None):\n",
    "        self.learning_rate = learning_rate if learning_rate != None else self.learning_rate\n",
    "        self.epochs = epochs if epochs != None else self.epochs\n",
    "        self.batch_size = batch_size if batch_size != None else self.batch_size\n",
    "        \n",
    "        #need at min one layer\n",
    "        if (len(self.layers) == 0):\n",
    "            raise ValueError('No layers have been added. Need at least one layer. Please add a layer') \n",
    "        \n",
    "        #multi class classificaiton problem need y to be one hot encoded and must use multi log loss\n",
    "        multiClassProblem = self.classification and (y.max() - y.min() > 1)\n",
    "        if (multiClassProblem):\n",
    "            y = self.oneHot(y)\n",
    "            if (self.lossFunction != 'multi_logloss'):\n",
    "                raise ValueError('Loss Function Must be multi_logloss for multi class classification')\n",
    "        \n",
    "        epoch_costs = []\n",
    "        batches_cost_sum = 0\n",
    "        num_batches = self.getNumBatches(X.shape[1], self.batch_size)\n",
    "        \n",
    "        #Initializing weights of the first layer \n",
    "        #need to do it right now because shape of input isnt known until now\n",
    "        self.layers[0].initializeWeights(X.shape[0], self.layers[0].getNeuronCount())\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            batches_cost_sum = 0\n",
    "            for batch in range(num_batches):\n",
    "                #ipdb.set_trace()\n",
    "                A = X[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
    "                \n",
    "                if (multiClassProblem): \n",
    "                    y_curBatch = y[:, batch*self.batch_size:(batch+1)*self.batch_size]\n",
    "                else:\n",
    "                    y_CurBatch = y[batch*self.batch_size:(batch+1)*self.batch_size]\n",
    "            \n",
    "                ipdb.set_trace()\n",
    "                for layer in self.layers:\n",
    "                    A = layer.feedForward(A)\n",
    "                batches_cost_sum += 1/self.batch_size * np.sum(self.loss(self, A, y_curBatch))\n",
    "                dZ_Special = A - y_curBatch if multiClassProblem else np.array([None])\n",
    "                dA = self.d_loss(self, A, y_curBatch) # after the final output layer dA is found like this since A is just the output\n",
    "                for layer in reversed(self.layers):\n",
    "                    if (layer == self.layers[-1]):\n",
    "                        dA = layer.backprop(dA, self.learning_rate, dZ_Special)\n",
    "                    else:\n",
    "                        dA = layer.backprop(dA, self.learning_rate, np.array([None]))\n",
    "            epoch_costs.append(batches_cost_sum) \n",
    "            print(\"Epoch: \", epoch, \"Cost:\", batches_cost_sum)\n",
    "        return epoch_costs\n",
    "        \n",
    "    def predict(self, X):\n",
    "        #ipdb.set_trace()\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.feedForward(A)\n",
    "        return A\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.array([[0, 0, 1, 1], \n",
    "#                     [0, 1, 0, 1]]) # 2 inputs and 4 samples, i.e 2x4\n",
    "# y_train = np.array([0, 1, 1, 0]) #1 x num of samples\n",
    "# net = NeuralNet('logloss')\n",
    "# net.addLayer(3, 'tanh')\n",
    "# net.addLayer(1, 'sigmoid')\n",
    "# costs = net.fit(x_train, y_train, 10000)\n",
    "# plt.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.array([[1], [1]])\n",
    "# a = net.predict(test)\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 1 ... 7 1 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation].astype(np.int)\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "print(y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=60000, test_size=10000)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).T\n",
    "X_test = scaler.transform(X_test).T\n",
    "\n",
    "# X_train = X_train.T\n",
    "# X_test = X_test.T\n",
    "\n",
    "plt.gray()\n",
    "plt.imshow(X_test[:, 50].reshape(28, 28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-62-de3595b3c639>\u001b[0m(100)\u001b[0;36mfit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     99 \u001b[0;31m                \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 100 \u001b[0;31m                \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    101 \u001b[0;31m                    \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-62-de3595b3c639>\u001b[0m(101)\u001b[0;36mfit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    100 \u001b[0;31m                \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 101 \u001b[0;31m                    \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    102 \u001b[0;31m                \u001b[0mbatches_cost_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_curBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-62-de3595b3c639>\u001b[0m(100)\u001b[0;36mfit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     99 \u001b[0;31m                \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 100 \u001b[0;31m                \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    101 \u001b[0;31m                    \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-62-de3595b3c639>\u001b[0m(101)\u001b[0;36mfit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    100 \u001b[0;31m                \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 101 \u001b[0;31m                    \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    102 \u001b[0;31m                \u001b[0mbatches_cost_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_curBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> A\n",
      "array([[ 0.        , 21.36762256,  0.        , ...,  5.88215693,\n",
      "         0.        ,  0.        ],\n",
      "       [ 6.14789962, 13.4520871 ,  0.86629988, ..., 10.69960299,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  5.31242484,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       ...,\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  7.86131027,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.32551483, ...,  4.08439632,\n",
      "        13.34895029,  7.89856312],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ]])\n",
      "ipdb> n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-61-1de2ce783999>:21: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return expZ / expZ.sum(axis=0, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-62-de3595b3c639>\u001b[0m(100)\u001b[0;36mfit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     99 \u001b[0;31m                \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 100 \u001b[0;31m                \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    101 \u001b[0;31m                    \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> A\n",
      "array([[nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       ...,\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan],\n",
      "       [nan, nan, nan, ..., nan, nan, nan]])\n",
      "ipdb> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-230f2e114cf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmnist_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reLu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmnist_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-de3595b3c639>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mbatches_cost_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_curBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-de3595b3c639>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mbatches_cost_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_curBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mnist_net = NeuralNet('multi_logloss')\n",
    "mnist_net.addLayer(64, 'reLu')\n",
    "mnist_net.addLayer(10, 'softmax')\n",
    "costs = mnist_net.fit(X_train, y_train, 100, 60000)\n",
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e3ba3ae7e706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m array([ 0.        ,  0.        ,  0.        , 12.34557301,  0.        ,\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0;36m1.44547623\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m \u001b[0;36m14.42673622\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11.38719467\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;36m5.75124653\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m27.30509425\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14.13349396\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16.0829925\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m \u001b[0;36m12.09050004\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.24301809\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.\u001b[0m        \u001b[0;34m,\u001b[0m  \u001b[0;36m0.81315813\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'array' is not defined"
     ]
    }
   ],
   "source": [
    "array([ 0.        ,  0.        ,  0.        , 12.34557301,  0.        ,\n",
    "        1.44547623,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "        0.        ,  0.        , 14.42673622, 11.38719467,  0.        ,\n",
    "        5.75124653, 27.30509425, 14.13349396, 16.0829925 ,  0.        ,\n",
    "        0.        , 12.09050004,  0.24301809,  0.        ,  0.81315813,\n",
    "       36.25892928,  3.02591285,  0.        ,  7.69811406,  0.        ,\n",
    "        0.        ,  0.        , 23.42250571,  6.57030016,  2.73966946,\n",
    "        0.        ,  0.        ,  0.        , 13.62185096,  7.91879562,\n",
    "       22.05083927,  0.        , 29.15894322, 15.4927778 ,  0.        ,\n",
    "        0.        , 16.55078204, 13.3898415 ,  0.        ,  0.        ,\n",
    "        0.        , 19.41925836,  0.        ,  0.        ,  0.        ,\n",
    "       15.12313668,  8.86435797, 14.11965046,  0.        ,  0.        ,\n",
    "        0.        ,  0.37141094,  3.19106955, 27.12674025])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
