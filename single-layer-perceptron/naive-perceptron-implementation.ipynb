{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Single layer, feedfoward neural net, no hidden layers, for binary classification\n",
    "- Using sigmoid activation function\n",
    "- No biases\n",
    "- Binary classifier\n",
    "<img src=\"perceptronArchitecture.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random weights:\n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n"
     ]
    }
   ],
   "source": [
    "training_inputs = np.array([[0, 0, 1], \n",
    "                            [1, 1, 1], \n",
    "                            [1, 0, 1], \n",
    "                            [0, 1, 1],\n",
    "                            [0, 0, 0]])\n",
    "training_outputs = np.array([[0, 1, 1, 0, 0]]).T # transponse to become 4x1 matrix\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "synaptic_weights = 2 * np.random.random((3, 1)) - 1\n",
    "\n",
    "print('Random weights:')\n",
    "print(synaptic_weights)\n",
    "\n",
    "# train --> 4x3\n",
    "# weights --> 3x1\n",
    "# matrix multip --> 4x1\n",
    "\n",
    "# 3x4 w/ a 4x1\n",
    "# [0, 1, 1, 0]\n",
    "# [0, 1, 0, 1]\n",
    "# [1, 1, 1, 1]\n",
    "\n",
    "# w/ a\n",
    "\n",
    "# [a]\n",
    "# [b]\n",
    "# [c]\n",
    "# [d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synaptic weights after training:\n",
      "[[12.00870061]\n",
      " [-0.2044116 ]\n",
      " [-5.8002822 ]]\n",
      "Outputs after training:\n",
      "[[0.00301758]\n",
      " [0.99753723]\n",
      " [0.99799161]\n",
      " [0.00246109]\n",
      " [0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(100000): #one epoch essentially, below we are not using a batched model or stochastic grad descent\n",
    "    input_layer = training_inputs\n",
    "    \n",
    "    outputs = sigmoid(np.dot(input_layer, synaptic_weights))\n",
    "    \n",
    "    error = training_outputs - outputs\n",
    "    \n",
    "    adjustments = error * sigmoid_derivative(outputs)\n",
    "    \n",
    "    synaptic_weights += np.dot(input_layer.T, adjustments) # num of cols of first must equals num of rows of secod for matrix multip\n",
    "\n",
    "print('Synaptic weights after training:')\n",
    "print(synaptic_weights)\n",
    "    \n",
    "print('Outputs after training:')\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Will adjust weights by error x input (either 0 or 1) x sigmoid'(output)\n",
    "- Take error into account so change is proportional to error\n",
    "- Error is expected - actual\n",
    "- Derivative of sigmoid taken into account because if the output of the neuron was large, it means the neuron was confident in its value so we dont want to change it too much and vice versa for a small output (derivative of sigmoid is small at extremes and large in the middle w/ small values)\n",
    "<img src=\"perceptronBackProp.png\" width=\"400\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This neural net wont produce exactly 1 and 0s as output because of the nature of the sigmoid function which requires an infinite amount of iterations to get to 0 and 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
